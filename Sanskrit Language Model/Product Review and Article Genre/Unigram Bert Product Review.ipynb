{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abd764b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "review_dataset = load_dataset(\"csv\", data_files=\"..\\datasets\\Hindi Product Review.csv\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1c1826",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6965d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_dataset.set_format(\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecadf5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get label counts for both classes\n",
    "label_counts = review_dataset[\"Label\"].value_counts()\n",
    "num_labels = (len(label_counts.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822e429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed49cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_token_length = max(review_dataset['Text'].str.len())\n",
    "max_token_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf07240",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_dataset.reset_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fd7e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "\n",
    "set_seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f829941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"../Hindi Pretraining/models/unigram/bert-base-pretrained-hindi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe21642",
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  AutoModelForSequenceClassification.from_pretrained(\"../Hindi Pretraining/models/unigram/bert-base-pretrained-hindi\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d06bad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_added_tokens = tokenizer.add_tokens([\"5\",\"7\",\"8\",\"9\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186836e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\n",
    "# model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab199977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"Text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e188b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "tokenized_dataset = review_dataset.map(tokenize_function, batched=True, remove_columns=[\"Text\"])\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8451fd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b93b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.filter(lambda x:x if x[\"Label\"] != 'Label' else None)\n",
    "tokenized_dataset = tokenized_dataset.filter(lambda x:x if (x[\"Label\"] != 'Label' and x[\"Label\"] != 'neutral') else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b3c677",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f85001f",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = tokenized_dataset.filter(lambda x:x if 0 in x[\"input_ids\"] else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cc32d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00db1906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_label(example):\n",
    "    #mapping = {\"neutral\":0, \"positive\":1, \"negative\":2}\n",
    "    mapping = {\"positive\":0, \"negative\":1}\n",
    "    example['Label'] = mapping[example['Label']]\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46142a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.map(assign_label)\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"Label\", \"labels\")\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "tokenized_dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbdeae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [tokenized_dataset[i] for i in range(20)]\n",
    "samples\n",
    "\n",
    "for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acefe412",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_dataset[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93559af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsampled_dataset = tokenized_dataset.train_test_split(\n",
    "#     train_size=0.8, seed=42\n",
    "# )\n",
    "# downsampled_dataset\n",
    "\n",
    "stratify_column_name = \"labels\"\n",
    "\n",
    "# create class label column and stratify\n",
    "downsampled_dataset = tokenized_dataset.class_encode_column(\n",
    "    stratify_column_name\n",
    ").train_test_split(\n",
    "    test_size=0.2, \n",
    "    seed = 42,\n",
    "    #stratify_by_column=stratify_column_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abab80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a34189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 16\n",
    "# batch_size = 4\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    downsampled_dataset[\"train\"], shuffle=True, batch_size=batch_size, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    downsampled_dataset[\"test\"], batch_size=batch_size, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccc01a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72da652a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "with torch.no_grad():\n",
    "    outputs = model(**batch)\n",
    "    print(outputs.loss, outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15803ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "# optimizer = AdamW(model.parameters(), lr=4e-5)\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d7af02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342eaf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric_fun = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    metric_result = metric_fun.compute(predictions=predictions, references=labels, average='macro')\n",
    "    return {\n",
    "        \"f1\": metric_result[\"f1\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4867ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  disable weights and biases logging\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adf566d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 16\n",
    "# batch_size = 4\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(downsampled_dataset[\"train\"]) // batch_size\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    report_to = None,\n",
    "    output_dir=\"models/bert-unigram-bengali-classifier\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    #learning_rate=4e-5,\n",
    "    learning_rate=3e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    #num_train_epochs=6,\n",
    "    num_train_epochs=4,\n",
    "    #push_to_hub=True,\n",
    "    fp16=True,\n",
    "    logging_steps=logging_steps,\n",
    "    #metric_for_best_model = 'f1',\n",
    "    #load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901aaefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=downsampled_dataset[\"train\"],\n",
    "    eval_dataset=downsampled_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3456882c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6598241a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c63766c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050bb9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric['eval_f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d4e1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds, y_true, _ = trainer.predict(downsampled_dataset[\"test\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdf8cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = np.argmax(y_preds, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba85f83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8dff58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = [\"positive\", \"negative\"]\n",
    "print(classification_report(y_true, y_preds,target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a4d888",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = review_dataset.filter(lambda x:x if x[\"Label\"]=='negative' else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fde6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee2a70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faad1b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = list()\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# First make the kfold object\n",
    "folds = StratifiedKFold(n_splits=5)\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.shuffle(seed=42)\n",
    "# tokenized_dataset = tokenized_dataset.shuffle(seed=30)\n",
    "\n",
    "# Now make our splits based off of the labels. \n",
    "# We can use `np.zeros()` here since it only works off of indices, we really care about the labels\n",
    "splits = folds.split(np.zeros(tokenized_dataset.num_rows), tokenized_dataset[\"labels\"])\n",
    "\n",
    "# In this case I'm overriding the train/val/test\n",
    "for train_idxs, val_idxs in splits:\n",
    "    fold_dataset = DatasetDict({\n",
    "    \"train\":tokenized_dataset.select(train_idxs),\n",
    "    \"validation\":tokenized_dataset.select(val_idxs),\n",
    "    })\n",
    "    \n",
    "    \n",
    "    model =  AutoModelForSequenceClassification.from_pretrained(\"../Hindi Pretraining/models/unigram/bert-base-pretrained-hindi\", num_labels=2)\n",
    "    model.to(device)\n",
    "\n",
    "    batch_size = 16\n",
    "    # Show the training loss with every epoch\n",
    "    logging_steps = len(fold_dataset[\"train\"]) // batch_size\n",
    "\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        report_to = None,\n",
    "        output_dir=\"models/cross-validation\",\n",
    "        overwrite_output_dir=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=3e-5,\n",
    "        weight_decay=0.01,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=4,\n",
    "        fp16=True,\n",
    "        logging_steps=logging_steps,\n",
    "        metric_for_best_model = 'f1',\n",
    "        load_best_model_at_end=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=fold_dataset[\"train\"],\n",
    "        eval_dataset=fold_dataset[\"validation\"],\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    metric = trainer.evaluate()\n",
    "    scores.append(metric['eval_f1'])\n",
    "    print(metric['eval_f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3012263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712187aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(scores)  / len(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
