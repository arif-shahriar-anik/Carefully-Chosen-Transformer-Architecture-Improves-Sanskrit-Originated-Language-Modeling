{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cd998685",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "set_seed(80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "25fa122b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "import torch.nn as nn\n",
    "\n",
    "class CharacterBERTForMultipleChoice(nn.Module):\n",
    "    def __init__(self):\n",
    "          super(CharacterBERTForMultipleChoice, self).__init__()\n",
    "          self.bert = BertModel.from_pretrained(\"../Hindi Pretraining/models/unigram/bert-base-pretrained-hindi\")\n",
    "          ### New layers:\n",
    "          self.linear1 = nn.Linear(768, 1) ## 1 is the number of classes in this example\n",
    "\n",
    "    def forward(self, input_ids,attention_mask,token_type_ids,position_ids,head_mask,\\\n",
    "                inputs_embeds,output_attentions,output_hidden_states,return_dict):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask,\n",
    "            token_type_ids,\n",
    "            position_ids,\n",
    "            head_mask,\n",
    "            inputs_embeds,\n",
    "            output_attentions,\n",
    "            output_hidden_states,\n",
    "            return_dict)\n",
    "        \n",
    "        pooled_output = outputs[1]\n",
    "        logits = self.linear1(pooled_output)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "209e3f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-6ecfa560884c9a31\n",
      "Found cached dataset json (C:/Users/arifa/.cache/huggingface/datasets/json/default-6ecfa560884c9a31/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"json\", data_files=\"..\\datasets\\copa-translated\\\\hi\\\\train.jsonl\", \\\n",
    "                             split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f1cb0550",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-d361c8987e918d36\n",
      "Found cached dataset json (C:/Users/arifa/.cache/huggingface/datasets/json/default-d361c8987e918d36/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    }
   ],
   "source": [
    "val_dataset = load_dataset(\"json\", data_files=\"..\\datasets\\copa-translated\\\\hi\\\\val.jsonl\", \\\n",
    "                            split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f25a14c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-07180908d3559f11\n",
      "Found cached dataset json (C:/Users/arifa/.cache/huggingface/datasets/json/default-07180908d3559f11/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    }
   ],
   "source": [
    "test_dataset = load_dataset(\"json\", data_files=\"..\\datasets\\copa-translated\\\\hi\\\\test.jsonl\", \\\n",
    "                             split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "99241519",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "datasets = DatasetDict()\n",
    "datasets['train'] = train_dataset\n",
    "datasets['validation'] = val_dataset\n",
    "datasets['test'] = test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "89b417a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'choice1', 'choice2', 'question', 'idx', 'label'],\n",
       "        num_rows: 362\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['premise', 'choice1', 'choice2', 'question', 'idx', 'label'],\n",
       "        num_rows: 88\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['premise', 'choice1', 'choice2', 'question', 'idx', 'label'],\n",
       "        num_rows: 449\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bac25dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': ['मेरे शरीर ने घास पर छाया डाली।',\n",
       "  'महिला ने अपने दोस्त के कठिन व्यवहार को सहन किया।',\n",
       "  'महिलाएं कॉफी के लिए मिलीं।',\n",
       "  'धावक ने शॉर्ट्स पहनी थी।',\n",
       "  'पार्टी के मेहमान सोफे के पीछे छिप गए।'],\n",
       " 'choice1': ['सूरज उग रहा था।',\n",
       "  'महिला को पता था कि उसका दोस्त कठिन समय से गुजर रहा है।',\n",
       "  'एक नए स्थान में कैफे फिर से खुल गया।',\n",
       "  'पूर्वानुमान में उच्च तापमान की भविष्यवाणी की गई थी।',\n",
       "  'यह एक सरप्राइज पार्टी थी।'],\n",
       " 'choice2': ['घास काटी गई।',\n",
       "  'महिला को लगा कि उसके दोस्त ने उसकी दया का फायदा उठाया।',\n",
       "  'वे एक-दूसरे को पकड़ना चाहते थे।',\n",
       "  'उसने समुद्र तट के साथ दौड़ने की योजना बनाई।',\n",
       "  'यह जन्मदिन की पार्टी थी।'],\n",
       " 'question': ['cause', 'cause', 'cause', 'cause', 'cause'],\n",
       " 'idx': [0, 1, 2, 3, 4],\n",
       " 'label': [0, 0, 1, 0, 0]}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a882b123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': ['आइटम को बबल रैप में पैक किया गया था।',\n",
       "  'मैंने अपनी जेबें खाली कर दीं।'],\n",
       " 'choice1': ['यह नाजुक था।', 'मैंने एक टिकट स्टब को पुनः प्राप्त किया।'],\n",
       " 'choice2': ['छोटा था।', 'मुझे एक हथियार मिला।'],\n",
       " 'question': ['cause', 'effect'],\n",
       " 'idx': [0, 1],\n",
       " 'label': [0, 0]}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"test\"][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "40831c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.set_format(\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1ce2da35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get label counts for both classes\n",
    "label_counts = datasets[\"train\"][\"label\"].value_counts()\n",
    "num_labels = (len(label_counts.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1afd418e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    186\n",
       "0    176\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "33520fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.reset_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0161e6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast, AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"../Hindi Pretraining/models/unigram/bert-base-pretrained-hindi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b8fc03b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "choice_names = ['choice1', 'choice2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c6010dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    premise = [[context] * 2 for context in examples[\"premise\"]]\n",
    "    cause = [[f\"{examples[choice][i]}\" for choice in choice_names] for i,_ in enumerate(premise)]\n",
    "\n",
    "    premise = sum(premise, [])\n",
    "    cause = sum(cause, [])\n",
    "    \n",
    "#     print(premise)\n",
    "#     print(cause)\n",
    "    \n",
    "\n",
    "    tokenized_examples = tokenizer(premise, cause, truncation=True, max_length=128, padding='max_length')\n",
    "#     print(len(tokenized_examples))\n",
    "    return {k: [v[i : i + 2] for i in range(0, len(v), 2)] for k, v in tokenized_examples.items()}\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "29d4330d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = preprocess_function(datasets[\"train\"][:1])\n",
    "len(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3957f337",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\arifa\\.cache\\huggingface\\datasets\\json\\default-6ecfa560884c9a31\\0.0.0\\0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\\cache-8939e5bd4b02951d.arrow\n",
      "Loading cached processed dataset at C:\\Users\\arifa\\.cache\\huggingface\\datasets\\json\\default-d361c8987e918d36\\0.0.0\\0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\\cache-1efcbb1b6b9b27eb.arrow\n",
      "Loading cached processed dataset at C:\\Users\\arifa\\.cache\\huggingface\\datasets\\json\\default-07180908d3559f11\\0.0.0\\0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\\cache-dc7c9d0a99be6f58.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b3a51ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../Hindi Pretraining/models/unigram/bert-base-pretrained-hindi were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ../Hindi Pretraining/models/unigram/bert-base-pretrained-hindi and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "model = CharacterBERTForMultipleChoice()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8cde03ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'choice1', 'choice2', 'question', 'idx', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 362\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['premise', 'choice1', 'choice2', 'question', 'idx', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 88\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['premise', 'choice1', 'choice2', 'question', 'idx', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 449\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e0c411e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       " 'validation': ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       " 'test': ['label', 'input_ids', 'token_type_ids', 'attention_mask']}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns(['premise', 'choice1', 'choice2', 'question', 'idx'])\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "146876d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets[\"train\"][0]['input_ids'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f52e209d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets[\"train\"][0]['attention_mask'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6840fbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets[\"train\"][0]['token_type_ids'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c2e12f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c1aecc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arifa\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "33d352b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "724\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 2\n",
    "num_training_steps = num_epochs * len(tokenized_datasets[\"train\"])\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    #num_warmup_steps=0,\n",
    "    num_warmup_steps=0.1 * num_training_steps,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "18a6239f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a2cc17e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# classifier = nn.Linear(768, 1).to(device)\n",
    "loss_fct = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7252f332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2238, -0.2424]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor(0.6839, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for item in tokenized_datasets[\"train\"]:\n",
    "    item = {k: v.to(device) for k, v in item.items()}\n",
    "    position_ids=None\n",
    "    head_mask=None\n",
    "    inputs_embeds=None\n",
    "    output_attentions=None\n",
    "    output_hidden_states=None\n",
    "    return_dict=None\n",
    "    logits = model(\n",
    "            input_ids=item[\"input_ids\"],\n",
    "            attention_mask=item[\"attention_mask\"],\n",
    "            token_type_ids=item[\"token_type_ids\"],\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "    reshaped_logits = logits.view(-1, 2)\n",
    "    print(reshaped_logits)\n",
    "    print(item[\"label\"].unsqueeze(0))\n",
    "    loss = loss_fct(reshaped_logits, item[\"label\"].unsqueeze(0))\n",
    "    print(loss)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ee067651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "748b1f69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb9f7c5e77d424b965509b2f1ad65df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/724 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 0: Training Loss: 0.6967911720275879\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5faa42aa215453ea2205503b13aff68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy {'accuracy': 0.5}\n",
      ">>> Epoch 1: Training Loss: 0.5518900752067566\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c665c2a078404836a5a7e52b337bf360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy {'accuracy': 0.5909090909090909}\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    training_losses = []\n",
    "    for item in tokenized_datasets[\"train\"]:\n",
    "        item = {k: v.to(device) for k, v in item.items()}\n",
    "        logits = model(\n",
    "                input_ids=item[\"input_ids\"],\n",
    "                attention_mask=item[\"attention_mask\"],\n",
    "                token_type_ids=item[\"token_type_ids\"],\n",
    "                position_ids=None,\n",
    "                head_mask=None,\n",
    "                inputs_embeds=None,\n",
    "                output_attentions=None,\n",
    "                output_hidden_states=None,\n",
    "                return_dict=None\n",
    "            )\n",
    "        reshaped_logits = logits.view(-1, 2)\n",
    "        loss = loss_fct(reshaped_logits, item[\"label\"].unsqueeze(0))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "        training_losses.append(loss.repeat(1))\n",
    "    \n",
    "    training_losses = torch.cat(training_losses)\n",
    "    training_losses = training_losses[: len(tokenized_datasets[\"train\"])]\n",
    "    \n",
    "    print(f\">>> Epoch {epoch}: Training Loss: {torch.mean(training_losses)}\")\n",
    "    \n",
    "    progress_bar2 = tqdm(range(len(tokenized_datasets[\"validation\"])))\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    training_losses = []\n",
    "    for item in tokenized_datasets[\"validation\"]:\n",
    "        item = {k: v.to(device) for k, v in item.items()}\n",
    "        logits = model(\n",
    "                input_ids=item[\"input_ids\"],\n",
    "                attention_mask=item[\"attention_mask\"],\n",
    "                token_type_ids=item[\"token_type_ids\"],\n",
    "                position_ids=None,\n",
    "                head_mask=None,\n",
    "                inputs_embeds=None,\n",
    "                output_attentions=None,\n",
    "                output_hidden_states=None,\n",
    "                return_dict=None\n",
    "            )\n",
    "        reshaped_logits = logits.view(-1, 2)\n",
    "        pred = torch.argmax(reshaped_logits)\n",
    "        metric.add_batch(predictions=pred.unsqueeze(0), references=item[\"label\"].unsqueeze(0))\n",
    "        progress_bar2.update(1)\n",
    "\n",
    "    acc = metric.compute()\n",
    "    print(\"accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4c2d2cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bee03b260623408dbe6e99bdb075fdc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(len(tokenized_datasets[\"test\"])))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "training_losses = []\n",
    "for item in tokenized_datasets[\"test\"]:\n",
    "    item = {k: v.to(device) for k, v in item.items()}\n",
    "    logits = model(\n",
    "            input_ids=item[\"input_ids\"],\n",
    "            attention_mask=item[\"attention_mask\"],\n",
    "            token_type_ids=item[\"token_type_ids\"],\n",
    "            position_ids=None,\n",
    "            head_mask=None,\n",
    "            inputs_embeds=None,\n",
    "            output_attentions=None,\n",
    "            output_hidden_states=None,\n",
    "            return_dict=None\n",
    "        )\n",
    "    reshaped_logits = logits.view(-1, 2)\n",
    "    pred = torch.argmax(reshaped_logits)\n",
    "    metric.add_batch(predictions=pred.unsqueeze(0), references=item[\"label\"].unsqueeze(0))\n",
    "    progress_bar.update(1)\n",
    "        \n",
    "acc = metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9af7a96b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5746102449888641}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
