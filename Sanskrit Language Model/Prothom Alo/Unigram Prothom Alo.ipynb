{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4672c606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b5ac6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28b2394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "\n",
    "# # start a new wandb run to track this script\n",
    "# wandb.init(\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project=\"unigram-bert-prothom-alo-epoch6\",\n",
    "    \n",
    "#     # track hyperparameters and run metadata\n",
    "#     config={\n",
    "#     \"learning_rate\": 2e-5,\n",
    "#     \"weight_decay\" : 0.01,\n",
    "#     \"architecture\": \"unigram-bert-base\",\n",
    "#     \"dataset\": \"prothom_alo\",\n",
    "#     \"epochs\": 6,\n",
    "#     \"batch size\": 64,\n",
    "#     }\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a4ec8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "prothom_alo_dataset = load_dataset(\"text\", data_files=\"../datasets/Bangla Prothom Alo.txt\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852527e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prothom_alo_dataset.set_format(\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34400f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "prothom_alo_dataset.set_format(\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3559f13-4ee7-4f91-9e34-818fb699ed42",
   "metadata": {},
   "outputs": [],
   "source": [
    "prothom_alo_df = prothom_alo_dataset[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e6bff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prothom_alo_df_lens = prothom_alo_df['text'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97695c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(prothom_alo_df_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5212943",
   "metadata": {},
   "outputs": [],
   "source": [
    "(prothom_alo_df=='').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ae6765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretraining_df_lens = pretraining_df['text'].str.len()\n",
    "count = prothom_alo_df['text'].str.split().apply(len).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37ff75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "count.index = count.index.astype(str) + ' words:'\n",
    "count.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55a29a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e4a388",
   "metadata": {},
   "outputs": [],
   "source": [
    "prothom_alo_dataset.reset_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c108a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prothom_alo_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181ba3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prothom_alo_dataset = prothom_alo_dataset.filter(lambda x: x[\"text\"]!=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9b025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prothom_alo_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b31d378-f4c8-44da-9c11-cbcc78f8ee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "prothom_alo_dataset[100:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7cf45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    for i in range(0, len(prothom_alo_dataset), 1000):\n",
    "        yield prothom_alo_dataset[i : i+1000][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa85920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dac1de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.Unigram())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b453a38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Regex\n",
    "\n",
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [\n",
    "        normalizers.Replace(\"``\", '\"'),\n",
    "        normalizers.Replace(\"''\", '\"'),\n",
    "        normalizers.NFKC(),\n",
    "#         normalizers.StripAccents(),\n",
    "        normalizers.Replace(Regex(\" {2,}\"), \" \"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a3ebfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.normalizer.normalize_str('রিয়াল ভাবতে পারে তারা আসলে 1111111111 পয়েন্টে এগিয়ে ।'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d9f5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_tokenizer = pre_tokenizers.Sequence(\n",
    "    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae668a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce83017",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer.pre_tokenize_str('রিয়াল ভাবতে পারে তারা আসলে 1111111111 পয়েন্টে এগিয়ে ।')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0190dbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.UnigramTrainer(\n",
    "    vocab_size=30522, special_tokens=special_tokens, unk_token=\"[UNK]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d057ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec6c262",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode('রিয়াল ভাবতে পারে তারা আসলে 1111111111 পয়েন্টে এগিয়ে ।')\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb64fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
    "print(cls_token_id, sep_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301d56bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f5aa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode('রিয়াল ভাবতে পারে তারা আসলে 1111111111 পয়েন্টে এগিয়ে ।')\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b375a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode(\"এ ছাড়া শিক্ষাপ্রতিষ্ঠানেও চলবে প্রচারণা ।\",\"সহযোগিতা করছে তথ্য ও যোগাযোগপ্রযুক্তি আইসিটি বিভাগ ।\")\n",
    "print(encoding.tokens)\n",
    "print(encoding.type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54484d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.Metaspace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d5a2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e271e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.enable_truncation(max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24baa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"unigram_tokenizer_prothom_alo.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfadb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer.from_file(\"unigram_tokenizer_prothom_alo.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae2fab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    #tokenizer_object=tokenizer,\n",
    "    tokenizer_file=\"unigram_tokenizer_prothom_alo.json\", # You can load from the tokenizer file, alternatively\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    "    return_special_tokens_mask = True,\n",
    "    model_max_length = 512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77046651",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"unigram_tokenizers_prothom_alo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20953781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# tokenizer = PreTrainedTokenizerFast.from_pretrained(\"unigram_tokenizers_prothom_alo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2d937a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertForMaskedLM\n",
    "\n",
    "# Building the config\n",
    "config = BertConfig()\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad4d269-eef9-4a14-9fa2-63617c8528be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test tokenizer\n",
    "# tokenizer.tokenize(\"শিক্ষাপ্রতিষ্ঠানেও\")\n",
    "# tokenizer.tokenize(\"পাচজনকে\")\n",
    "# tokenizer.tokenize(\"অংশগুলি\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aca711",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_bert_config = BertConfig(pad_token_id=tokenizer.pad_token_id)\n",
    "print(unigram_bert_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfa9d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_bert_config.save_pretrained(save_directory=\"configs/unigram_bert_config/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1391b06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_bert_config = BertConfig.from_pretrained(\"configs/unigram_bert_config/config.json\")\n",
    "# Building the model from the config\n",
    "# Model is randomly initialized\n",
    "model = BertForMaskedLM(unigram_bert_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f400c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_bert_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0d1af6-8a77-422d-a2ef-410e59e2d1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = 0\n",
    "for p in model.parameters():\n",
    "    if len(p.shape) == 2:\n",
    "        total_params += p.shape[0] * p.shape[1]\n",
    "        \n",
    "print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ba7cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = 'পরে সেখানে সংক্ষিপ্ত সমাবেশ অনুষ্ঠিত হয় ।'\n",
    "text = 'পরে সেখানে সংক্ষিপ্ত সমাবেশ [MASK] হয় ।'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d96610",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fcc364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "# inputs.to(\"cuda\")\n",
    "\n",
    "token_logits = model(**inputs).logits\n",
    "# Find the location of [MASK] and extract its logits\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc4fe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"text\"], padding=\"max_length\", max_length=80, truncation=True)\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "\n",
    "# Use batched=True to activate fast multithreading!\n",
    "tokenized_datasets = prothom_alo_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"text\"]\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38acd53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets.remove_columns(\"token_type_ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526f7924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = tokenized_datasets.filter(lambda x:x if 1 in x[\"input_ids\"] else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e59b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e939480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Create a new labels column\n",
    "    examples[\"labels\"] = examples[\"input_ids\"].copy()\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084e9842",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0231e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(lm_datasets[1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522f2ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(lm_datasets[1][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a2f73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f7dcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "np.random\n",
    "from transformers import default_data_collator\n",
    "\n",
    "wwm_probability = 0.15\n",
    "\n",
    "\n",
    "def bangla_data_collator(features):\n",
    "    for feature in features:\n",
    "#         word_ids = feature.pop(\"word_ids\")\n",
    "\n",
    "#         # Create a map between words and corresponding token indices\n",
    "#         mapping = collections.defaultdict(list)\n",
    "#         current_word_index = -1\n",
    "#         current_word = None\n",
    "#         for idx, word_id in enumerate(word_ids):\n",
    "#             if word_id is not None:\n",
    "#                 if word_id != current_word:\n",
    "#                     current_word = word_id\n",
    "#                     current_word_index += 1\n",
    "#                 mapping[current_word_index].append(idx)\n",
    "\n",
    "        # Randomly mask words\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        labels = feature[\"labels\"]\n",
    "        mask = np.random.binomial(1, wwm_probability, (len(input_ids),))\n",
    "        special_tokens =  [tokenizer.unk_token_id, tokenizer.pad_token_id, tokenizer.cls_token_id, \\\n",
    "                           tokenizer.sep_token_id, tokenizer.mask_token_id]\n",
    "        \n",
    "        new_labels = [-100] * len(labels)\n",
    "        for idx in np.where(mask)[0]:\n",
    "#             word_id = word_id.item()\n",
    "#             print(word_id)\n",
    "#             for idx in mapping[word_id]:\n",
    "#             if word_ids[idx] is not None:\n",
    "            if input_ids[idx] not in special_tokens:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id\n",
    "            feature[\"labels\"] = new_labels\n",
    "        \n",
    "    return default_data_collator(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21576526",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "lm_datasets = lm_datasets.remove_columns([\"word_ids\"])\n",
    "data_collator = bangla_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17aec7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [lm_datasets[i] for i in range(3)]\n",
    "# for sample in samples:\n",
    "#     _ = sample.pop(\"word_ids\")\n",
    "\n",
    "for chunk in bangla_data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ff8ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prothom_alo_dataset[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92973520",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [lm_datasets[i] for i in range(1)]\n",
    "\n",
    "chunk = data_collator(samples)\n",
    "print(chunk[\"input_ids\"])\n",
    "print(chunk[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d1f636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_size = 10_000\n",
    "# test_size = int(0.1 * train_size)\n",
    "\n",
    "downsampled_dataset = lm_datasets.train_test_split(\n",
    "    train_size=0.8, seed=42\n",
    ")\n",
    "downsampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053ed7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, sample in enumerate(downsampled_dataset[\"train\"][\"input_ids\"][:3]):\n",
    "    print(f\"'>>> Article {idx} length: {len(sample)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596d299d-dcf4-44dc-aacb-c3e811a69b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(downsampled_dataset[\"test\"][\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00cd613",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  disable weights and biases logging\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c32c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce9a906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 64\n",
    "eval_batch_size = 64\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(downsampled_dataset[\"train\"]) // batch_size\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs = 3,\n",
    "    report_to = None,\n",
    "    output_dir=\"models/unigram/bert-base-pretrained-prothom-alo\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=eval_batch_size,\n",
    "    #push_to_hub=True,\n",
    "    fp16=True,\n",
    "    logging_steps=logging_steps,\n",
    "    load_best_model_at_end=True,\n",
    "    save_strategy = \"epoch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a65f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=downsampled_dataset[\"train\"],\n",
    "    eval_dataset=downsampled_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adb2b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2e6ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0182776e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f815a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd924e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [optional] finish the wandb run, necessary in notebooks\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af3e354",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMaskedLM.from_pretrained(\"models/unigram/bert-base-pretrained-prothom-alo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b380fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"models/unigram/bert-base-pretrained-prothom-alo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10aab00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
