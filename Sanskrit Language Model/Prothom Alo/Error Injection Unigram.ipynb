{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6273017f-dd71-4be4-9dd6-7dcee760ad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "167b240a-45f9-4342-bad0-0f31c501e6de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 4090'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29eb897a-3624-4a85-8b42-98e7cae3640d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset text (/home/ashahri1/.cache/huggingface/datasets/text/default-66e892579a4abe66/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "prothom_alo_dataset = load_dataset(\"text\", data_files=\"../datasets/Bangla Prothom Alo.txt\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2cb52d0-e9c7-431c-854c-53ece16fea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prothom_alo_dataset.set_format(\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "299c6547-190a-4da7-88d8-ab81e0fa5d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prothom_alo_df = prothom_alo_dataset[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f9d99f1-0a2d-427a-b68b-31ec0c9b1ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text    1350000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(prothom_alo_df=='').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b962c0b9-fe54-4f0d-ae6b-50e79e740034",
   "metadata": {},
   "outputs": [],
   "source": [
    "prothom_alo_dataset.reset_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f5d7a65-a87b-4479-ac07-425d23377d96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 2700000\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prothom_alo_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f6c7e8e-54a1-4426-98c7-df4f8ea4828b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ashahri1/.cache/huggingface/datasets/text/default-66e892579a4abe66/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-37bc92e3ecf68258.arrow\n"
     ]
    }
   ],
   "source": [
    "prothom_alo_dataset = prothom_alo_dataset.filter(lambda x: x[\"text\"]!=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10504ea5-991d-477a-9f26-c4875a269f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 1350000\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prothom_alo_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "939cd8f0-bf7c-4c6a-a1df-076c8b85169d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1080000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 270000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_size = 10_000\n",
    "# test_size = int(0.1 * train_size)\n",
    "\n",
    "downsampled_dataset = prothom_alo_dataset.train_test_split(\n",
    "    train_size=0.8, seed=42\n",
    ")\n",
    "downsampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "733497b3-0f33-46a2-aeff-3cd4d58426c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datasets/Bangla Error Words.txt', encoding='utf-8') as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7e342e3-0453-471b-8da2-754983bea8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_words = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5e4682c-2693-4a37-b0e8-1a11135d926b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in lines:\n",
    "    combination = line.split()\n",
    "    original_word = combination[0]\n",
    "    modified_words = combination[1:]\n",
    "    error_words[original_word] = modified_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a90066f-cf22-4e8c-bb52-98757b7c5b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['গইত']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_words['গত']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e7fe636-5010-4242-89e6-2afafbc64c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "def replace_error_word(sentence, error_words):\n",
    "    for error_word in error_words.keys():\n",
    "        if error_word in sentence:\n",
    "            #print(error_word)\n",
    "            index = np.random.randint(len(error_words[error_word]))\n",
    "            sentence = sentence.replace(error_word, error_words[error_word][index])\n",
    "            break\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60f12f10-44c4-4ec9-b656-8cafa1294630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.randint(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b25f81e4-7b95-40b6-95b5-26eff60aa24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace_error_word(\"তখন আমাদের দেখা হবে।\", error_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bca195f-3893-4646-b668-55ed6a1c3d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# index = 0\n",
    "# indices = list()\n",
    "# for sample in downsampled_dataset[\"test\"].select(range(10)):\n",
    "#     #replace 15% of time\n",
    "#     if np.random.random() < 0.15:\n",
    "#         replaced_sample = replace_error_word(sample['text'], error_words)\n",
    "#         sample['text'] = replaced_sample\n",
    "#         print(sample['text'])\n",
    "#         indices.append(index)\n",
    "#         count += 1\n",
    "#     print(index)\n",
    "#     index += 1\n",
    "        \n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa019a84-5304-49b9-9c03-c1cc8032e7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_text(examples):\n",
    "    # Create a corrupt example\n",
    "    #replace 15% of time\n",
    "    if np.random.random() < 0.10:\n",
    "        examples[\"text\"] = replace_error_word(examples['text'], error_words)\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a1f015c-cf6d-4dad-8709-0711424bfc13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/270000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "downsampled_dataset[\"test\"] = downsampled_dataset[\"test\"].map(corrupt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f88e563f-a6db-4558-ad5c-729ada478b31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['যত দ্রুত সম্ভব এই ঘুষ দুর্নীতি দমন করতে হবে ।',\n",
       "  'ভর্তি পরীক্ষা স্থগিতের পেছনে এটিও একটি বড় কারণ ।',\n",
       "  'বাংলা একাডেমির উদ্যোগে তাঁর 1111111111 গান ইংরেজিতে অনূদিত হয়েছে ।',\n",
       "  'একই সময়ের মধ্যে বৌদ্ধ সম্প্রদায়ের জনসংখ্যার হেরফের হবে না ।',\n",
       "  'ইউপি সদস্যরা ভয়ে আছেন ।',\n",
       "  'তারা এই উদ্যোগকে দেশবিরোধিতার সঙ্গে তুলনা করেছে ।',\n",
       "  'এর মাধ্যমে দেশের জকোনো প্রান্ত থেকে জকোনো ব্যক্তি অভিযোগ বা পরামর্শ দিতে পারবেন ।',\n",
       "  'মৃত্যুদণ্ডের রায় শোনার পর নূর হোসেন ও র\\u200d্যাবের সাবেক তিন কর্মকর্তা স্বাভাবিক ছিলেন ।',\n",
       "  'এর জন্য জনমত গঠন করতে হবে ।',\n",
       "  'খুব কমসংখ্যক বলা যায় নামমাত্র দু একজন সুযোগ পেয়ে যেতেন ।']}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downsampled_dataset[\"test\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57836ffd-45de-4536-b9ed-a546933abdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    #tokenizer_object=tokenizer,\n",
    "    tokenizer_file=\"unigram_tokenizer_prothom_alo.json\", # You can load from the tokenizer file, alternatively\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    "    return_special_tokens_mask = True,\n",
    "    model_max_length = 512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50698ce1-5fd7-422f-befd-710d3004155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertForMaskedLM\n",
    "\n",
    "unigram_bert_config = BertConfig.from_pretrained(\"configs/unigram_bert_config/config.json\")\n",
    "# Building the model from the config\n",
    "# Model is randomly initialized\n",
    "model = BertForMaskedLM(unigram_bert_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1db65678-c9f9-4ed3-802f-a77610b13b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.16.0.dev0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_bert_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cfdd2d01-3b9d-4709-b4ed-c10e8dc2ee31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"text\"], padding=\"max_length\", max_length=80, truncation=True)\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a86ec28-9d49-42b1-b015-68f41b537d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1080000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/270000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 1080000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 270000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use batched=True to activate fast multithreading!\n",
    "tokenized_datasets = downsampled_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"text\"]\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6e9fbad5-e5f2-4ce4-9ab8-d2d232f72032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 1080000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 270000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets.remove_columns(\"token_type_ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e15856c-ca0a-4221-b8b8-eb07d4b96939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = tokenized_datasets.filter(lambda x:x if 1 in x[\"input_ids\"] else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b845fe6b-35c8-4ece-bcc6-bd7bc3737c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "567d7be9-09f3-4aec-892a-ae66da2a0e82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2,\n",
       "  2528,\n",
       "  826,\n",
       "  12,\n",
       "  1860,\n",
       "  16,\n",
       "  5,\n",
       "  6,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'word_ids': [None,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56703f4d-1ff9-48da-967f-a55f306b539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Create a new labels column\n",
    "    examples[\"labels\"] = examples[\"input_ids\"].copy()\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8003c22-d179-4638-81bf-cca0f1115e06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1080000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/270000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 1080000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 270000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4cffe97c-7437-409e-9f31-38249c7ad4b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['যত দ্রুত সম্ভব এই ঘুষ দুর্নীতি দমন করতে হবে ।',\n",
       "  'ভর্তি পরীক্ষা স্থগিতের পেছনে এটিও একটি বড় কারণ ।',\n",
       "  'বাংলা একাডেমির উদ্যোগে তাঁর 1111111111 গান ইংরেজিতে অনূদিত হয়েছে ।',\n",
       "  'একই সময়ের মধ্যে বৌদ্ধ সম্প্রদায়ের জনসংখ্যার হেরফের হবে না ।',\n",
       "  'ইউপি সদস্যরা ভয়ে আছেন ।',\n",
       "  'তারা এই উদ্যোগকে দেশবিরোধিতার সঙ্গে তুলনা করেছে ।',\n",
       "  'এর মাধ্যমে দেশের জকোনো প্রান্ত থেকে জকোনো ব্যক্তি অভিযোগ বা পরামর্শ দিতে পারবেন ।',\n",
       "  'মৃত্যুদণ্ডের রায় শোনার পর নূর হোসেন ও র\\u200d্যাবের সাবেক তিন কর্মকর্তা স্বাভাবিক ছিলেন ।',\n",
       "  'এর জন্য জনমত গঠন করতে হবে ।',\n",
       "  'খুব কমসংখ্যক বলা যায় নামমাত্র দু একজন সুযোগ পেয়ে যেতেন ।',\n",
       "  'আজ কারও কাছ থাকে প্রেমের প্রস্তাব পেতে পারেন ।',\n",
       "  'তাই এই কালেকশনের জন্য এ দুজন ছাড়া আর কোনো বিকল্প ছিল না ।',\n",
       "  'গতকাল সোমবার সন্ধ্যায় নগরীর চাষাঢ়া কেন্দ্রীয় শহীদ মিনারে এই কর্মসূচি পালন করা হয় ।',\n",
       "  'প্রতিটি দল এ সময়ে ছয়টি সিরিজ খেলবে তিনটি ঘরের মাঠে ও তিনটি বাইরে ।',\n",
       "  'সাংঘাতিক ব্যক্তিত্বসম্পন্ন ও মেধাবী একটা মেয়ে ।',\n",
       "  'তখন দুই ধরনের খবর ছড়িয়ে পড়ায় দেখা দেয় বিভ্রান্তি ।',\n",
       "  'প্রশাসনের পক্ষ থেকে চিকিৎসার বিষয়টি দেখা হচ্ছে ।',\n",
       "  'একটি ভবন নির্মাণের ক্ষেত্রে রড ও সিমেন্টের গুরুত্ব সবচেয়ে বেশি ।',\n",
       "  'অগত্যা কিনে দিলেন উড়োজাহাজ আকৃতির বেলুন ।',\n",
       "  'এসব এলাকার মধ্যে কুইন্সের জ্যাকসন হাইটস ও এস্টোরিয়া এবং ম্যানহাটনের সেলসা অন্যতম ।',\n",
       "  'পরে শহীদ রেজাউল চত্বরে শোকসভা অনুষ্ঠিত হয় ।',\n",
       "  'কাতারে বাংলাদেশ দূতাবাস এ বিষয়ে বেশ কয়েকবার সতর্কতামূলক প্রচারণার উদ্যোগ নিয়েছিল ।',\n",
       "  'তিনি বলেছেন মাকে তো বলবই ।',\n",
       "  'এ পরিপ্রেক্ষিতে কেন্দ্রীয় ব্যাংক গত বছরের 1111111111 মের মধ্যে অর্থ ফেরত আনার নির্দেশ দেয় ।',\n",
       "  'আদেশটি ঢাকা থেকে পাঠানো হয় ফ্যাক্সযোগে ।',\n",
       "  'কারাগার থেকে ছাড়া পাওয়ার তিন বছরের মধ্যে তাঁকে জরিমানার অর্থ পরিশোধ করতে বলেছেন আদালত ।',\n",
       "  'এতে ঘরের নিচে চাপা পড়ে রইছ উদ্দিন গুরুতর আহত হন ।',\n",
       "  'তিনি হত্যার কারণ জানাননি ।',\n",
       "  'এটা সংগ্রহ ক্রেন ।',\n",
       "  'এটি চালুর দায়িত্বও তাদের ।',\n",
       "  'প্রতি প্যাকেট বিড়ির সর্বনিম্ন দাম 1111111111 টাকা ।',\n",
       "  'রাষ্ট্রীয়ভাবে আজকের দিনটি যথাযোগ্য মর্যাদায় পালন অরা হচ্ছে ।',\n",
       "  'জামাল আহমেদের বাড়ি বালিগঞ্জের ব্রডস্ট্রিটে ।',\n",
       "  'এতে সড়ক দিয়ে সরাসরি যান চলাচল বন্ধ ।',\n",
       "  'অমনোযোগ আর অসচেতনতা আছে বলেই অনেক কিছু থেকেও নেই ।',\n",
       "  'এ ঘটসনায় চার পুলিশ সদস্য আহত হয়েছেন ।',\n",
       "  'গ্রুপ কোথায় গিয়ে দাঁড়াবে ?',\n",
       "  'সাফল্য না পেলেও দলের উন্নতি নিয়ে সন্তুষ্ট মাশরাফি টানা আট টি টোয়েন্টি হেরেছি ।',\n",
       "  'সন্ধ্যার পর গভীর রাতে এবং ভোরে ওই এলাকাগুলোতে ছিনতাই হচ্ছে ।',\n",
       "  'তবে দুই দেশের রাজনৈতিক ইতিহাসই এই দ্বৈরথে গত কয়েক বছরে পানি ঢেলে দিচ্ছে ।',\n",
       "  'আসলে মায়েদের হৃদয় এমনই ।',\n",
       "  'ঘরটি নিয়ে শাহ আবদুল করিমের অনেক স্বপ্ন ছিল ।',\n",
       "  'আমারও ইচ্ছে আছে আমাদের বাড়ির একটি অংশ মানবিক কাজে ব্যয় করার ।',\n",
       "  'আমি এ বিষয়টিও মিটমাটের চেষ্টা করব ।',\n",
       "  'রসনা ভর্তি খাবার খেয়ে অতিথিরা তৃপ্তির ঢেকুর তোলেন ।',\n",
       "  'জোরে শিস বাজাতেও শোনা যায় অনেককে ।',\n",
       "  'শিগগিরই সেতু মেরামতের ব্যবস্থা করা হবে ।',\n",
       "  'সভায় শিক্ষাপ্রতিষ্ঠানের 1111111111 জন অধ্যক্ষ প্রধান শিক্ষক ও সুপার উপস্থিত ছলেন ।',\n",
       "  'খেলোয়াড় হিসেবেই ওদের সঙ্গে রয়েছি ।',\n",
       "  'আমি যখন বয়সী চরিত্রে অভিনয় করেছি তখন এখনকার ফরিদাকে মনে করে শট দিয়েছি ।',\n",
       "  'তখন পুলিশ বাধা দেয় ।',\n",
       "  'দর্শকেরা একটু বৈচিত্র্য পাবেন ।',\n",
       "  'তাঁর দাবি প্রথম ভারতীয় হিসেবে এ কাজ করেছেন তিনি ।',\n",
       "  'কমিটির আহ্বায়ক শাহানুর হোসেন লিখিত বক্তব্যে বিলেম ওজোপাডিকোর বর্তমান গ্রাহকসংখ্যা এখন সাড়ে 1111111111 লাখ ।',\n",
       "  'কেউ আবার ফোন কী হ্যাক না ম্যাক কী জানি কয় করে ফেলল কিনা ।',\n",
       "  'বরং সজলকেই কপট মনে হয়েছে আর দর্শকের সহানুভূতি জেগেছে তার প্রেমিকার প্রতি ।',\n",
       "  'সেদিন দরিদ্র বেবিট্যাক্সিচালকের সন্তান নূর হোসেন হয়ে গেলেন বাংলাদেশের সন্তান ।',\n",
       "  'সেই সঙ্গে রয়েছে ধুলাবালু ।',\n",
       "  'গত বছরের 1111111111 সেপ্টেম্বর এই পরীক্ষা অনুষ্ঠিত হয় ।',\n",
       "  'আমার জন্যও সেটা কঠিন ছিল ।',\n",
       "  'রায়টি প্রধান িবচারপতি লিখলেও অন্য িবচারপতিরাও তাঁর সঙ্গে সহমত প্রকাশ করেছেন ।',\n",
       "  'নিজের অর্থস্বপ্ন ও অর্থসংকটের কথা লিখেই স্ট্যাটাস দেন ফেসবুকে ।',\n",
       "  'কিন্তু সে কোনো কথা না বলে উল্টো পথে হাঁটতে থাকে ।',\n",
       "  'হর্ষবর্ধন শ্রিংলা বলেন বাংলাদেশের স্বাধীনতার জন্য মুক্তিযোদ্ধা ও ভারতীয় সেনারা একসঙ্গে রক্ত দিয়েছিলেন ।',\n",
       "  'এ ব্যাপারে আইনি ব্যবস্থা নেওয়া হবে ।',\n",
       "  'আজকাল বিভিন্ন রেস্তোরাঁয় শিশুদের জন্নন আলাদা মেন্যুর ব্যবস্থা থাকে ।',\n",
       "  'স্বাভাবিক পরিবেশেই শেষ হয়েছে রংপুর সিটি করপোরেশন নির্বাচনের ভোট গ্রহণ ।',\n",
       "  'বন্ধু দিবস চালুর উদ্দেশ্যটা কিন্তু ছিল ব্যবসা ।',\n",
       "  'বরাদ্দ এলে ভাঙন রোধে বালুর বস্তা ফেলা হইবে ।',\n",
       "  'কাজেই যে জায়গা থেকে এই নেতৃত্ব গড়ে উঠেছে তা মানুষের জন্য নয় ।',\n",
       "  'খুব বেশি বল না করলেও ভালো মিডিয়াম পেসার ছিলেন তিনি ।',\n",
       "  '1111111111 আগস্ট দেওয়া আদেশে শিন্ডলিনের পর্যবেক্ষণের কথা উল্লেখ করেছেন টোরেস ।',\n",
       "  'প্রতিশোধ নিতে পরে যে মেয়েটিকে ধর্ষণ করা হয় তার বয়স 1111111111 অথবা 1111111111 বছর ।',\n",
       "  'আমারও ভালো লাগত তাঁকে ।',\n",
       "  'গেম প্রতিযোগিতায় প্রথম হয়েছে মিলিটারি ইনস্টিটিউট অব সায়েন্স অ্যান্ড টেকনোলজি এবং দ্বিতীয় ইউএপি ।',\n",
       "  'আবার দুই দিন ঘুরতে না ঘুরতে সব আগের মতোই ।',\n",
       "  'মন্ত্রী বলেন এ দেশে গত 1111111111 বছরে অনেক অর্জন হলেও তা ধরে রাখা যায়নি ।',\n",
       "  'গানের শেষে চতুরং পরিবেশনায় ছিলেন সংগীত পরিষদের ছাত্রছাত্রীরা ।',\n",
       "  'উনি তামান্নে এন্টারপ্রাইজের মালিকের আত্মীয় ।',\n",
       "  'এতে মুরারীর ডান হাত ও বাম পা ভেঙে যায় ।',\n",
       "  'কলম্বিয়া ফুটবল দল অ্যাডিডাসের ভুল নিয়ে তেমন মাতামাতি না করলেও ভক্তরা ছেড়ে কথা বলেনি ।',\n",
       "  'আজ দিনের পরের খেলায় মুখোমুখি সানরাইজার্স হায়দরাবাদ ও কিংস ইলেভেন পাঞ্জাব ।',\n",
       "  'অনুমোদন পাওয়ার পরদিন থেকেই পাইলিংয়ের কাজ হবে ।',\n",
       "  'বিচারিক প্রক্রিয়া শেষে গতকাল আসামির উপস্থিতিতে আদালত দণ্ড ঘোষণা করেন ।',\n",
       "  'প্রশ্ন শব্দগুলোর অর্থ লেখো ।',\n",
       "  'টাঙ্গাইলের গমজানিতে মা বাবার দেওয়া তিন বিঘা ধানি জমি পেয়েছিলেন এক নারী ।',\n",
       "  'সম্প্রতি মালদহের আফরাজুল খান নামের এক ব্যক্তি রাজস্থানে কাজের খোঁজে গিয়েছিলেন ।',\n",
       "  'আমাকে নিয়ে গানটা লেখা হয়েছে এটা সত্যিই ভালো লেগেছে ।',\n",
       "  'তিনি বলেন ন্যাম ভবন বিশেষ গুরুত্বপূর্ণ স্থাপনা ।',\n",
       "  'স্কুলের বন্ধুরা মিলে ছোট পরিসরেই হয়তো গান করে একদল কিশোর ।',\n",
       "  'কোনো দিন পাব কি না তা ও জানি না ।',\n",
       "  'এখন শুধু রাষ্ট্রই টেলিভিশনের মালিক নয় অনেক ব্যক্তির হাতে চলে এসেছে টেলিভিশন ।',\n",
       "  'বাংলাদেশি মুদ্রায় যা প্রায় 1111111111 টাকা ।',\n",
       "  'তবে এ পদে বিজ্ঞান বিভাগের প্রার্থীদের অগ্রাধিকার দেওয়া হব্বে ।',\n",
       "  'প্রদর্শনীতে দেখানো হয় তুলা থেকে সুতা কাটার চরকাও ।',\n",
       "  'পদ থাকলেই পদায়ন করতে হবে এমন কোনো মানে নেই ।',\n",
       "  'ফ্লোটিং পয়েন্ট অ্যালবামে কাজ করেছেন জ্যাজ কিংবদন্তি জন ম্যাকলাফলিনের সঙ্গে ।',\n",
       "  'ব্যাগের ভেতরে অপশনের একটি মেনু আছে যার বিস্তারিত বিবরণ থাকে ব্ল্যাক বুকে ।',\n",
       "  'তাদের উত্ত্যক্ত করা দুরন্ত মাছির আনাগোনায় ।',\n",
       "  'প্রতিযোগিতায় অংশ নেওয়ার পর 1111111111 নভেম্বর দেশে ফিরবেন জেসিয়া ইসলাম ।']}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downsampled_dataset[\"test\"][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0230e7e2-e25a-455d-bf6f-a1e08574571c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] শীত মৌসুমে যেকোনো জায়গায় বসা যায় ।[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fc7c5825-2b69-4b0e-9e04-0ee7cea4ba73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] শীত মৌসুমে যেকোনো জায়গায় বসা যায় ।[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets[\"train\"][1][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7c62a289-44db-4e02-96c1-de078292a31c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d3f98f81-76dc-4701-82ee-3292256f48e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "np.random\n",
    "from transformers import default_data_collator\n",
    "\n",
    "wwm_probability = 0.15\n",
    "\n",
    "\n",
    "def bangla_data_collator(features):\n",
    "    for feature in features:\n",
    "#         word_ids = feature.pop(\"word_ids\")\n",
    "\n",
    "#         # Create a map between words and corresponding token indices\n",
    "#         mapping = collections.defaultdict(list)\n",
    "#         current_word_index = -1\n",
    "#         current_word = None\n",
    "#         for idx, word_id in enumerate(word_ids):\n",
    "#             if word_id is not None:\n",
    "#                 if word_id != current_word:\n",
    "#                     current_word = word_id\n",
    "#                     current_word_index += 1\n",
    "#                 mapping[current_word_index].append(idx)\n",
    "\n",
    "        # Randomly mask words\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        labels = feature[\"labels\"]\n",
    "        mask = np.random.binomial(1, wwm_probability, (len(input_ids),))\n",
    "        special_tokens =  [tokenizer.unk_token_id, tokenizer.pad_token_id, tokenizer.cls_token_id, \\\n",
    "                           tokenizer.sep_token_id, tokenizer.mask_token_id]\n",
    "        \n",
    "        new_labels = [-100] * len(labels)\n",
    "        for idx in np.where(mask)[0]:\n",
    "#             word_id = word_id.item()\n",
    "#             print(word_id)\n",
    "#             for idx in mapping[word_id]:\n",
    "#             if word_ids[idx] is not None:\n",
    "            if input_ids[idx] not in special_tokens:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id\n",
    "            feature[\"labels\"] = new_labels\n",
    "        \n",
    "    return default_data_collator(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "55a48d09-ac22-493c-8250-adb7c1bf7e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "lm_datasets = lm_datasets.remove_columns([\"word_ids\"])\n",
    "data_collator = bangla_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "87f09a4c-3c33-4817-9608-f13b8a6e294a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] হাওরে একবারই ফসল হয়[MASK]।[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]'\n",
      "\n",
      "'>>> [CLS][MASK] মৌসুমে যেকোনো জায়গায় বসা যায় ।[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]'\n",
      "\n",
      "'>>> [CLS] সৌম্যর ব্যাটিং সব সময়ই দৃষ্টিসুখকর হয় ।[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]'\n"
     ]
    }
   ],
   "source": [
    "samples = [lm_datasets[\"train\"][i] for i in range(3)]\n",
    "# for sample in samples:\n",
    "#     _ = sample.pop(\"word_ids\")\n",
    "\n",
    "for chunk in bangla_data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e96d8b5c-abb9-4397-98ab-3e7110762f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   2, 2528,    4,   12, 1860,    4,    5,    4,    3,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1]])\n",
      "tensor([[-100, -100,  826, -100, -100,   16, -100,    6, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100]])\n"
     ]
    }
   ],
   "source": [
    "samples = [lm_datasets[\"train\"][i] for i in range(1)]\n",
    "\n",
    "chunk = data_collator(samples)\n",
    "print(chunk[\"input_ids\"])\n",
    "print(chunk[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "38a0472c-60ed-4d49-9fe0-318a6b8ec4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMaskedLM.from_pretrained(\"models/unigram/bert-base-pretrained-prothom-alo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a402ded8-6864-4d52-90c2-56b9b7153c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"models/unigram/bert-base-pretrained-prothom-alo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "64c50fc2-dc18-4ffc-9169-ee49d4de9ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  disable weights and biases logging\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "68637099-2ce9-4a90-b90c-9b1cb2b2ec2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 64\n",
    "eval_batch_size = 64\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(downsampled_dataset[\"train\"]) // batch_size\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs = 3,\n",
    "    report_to = None,\n",
    "    output_dir=\"models/unigram/bert-base-pretrained-prothom-alo\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=eval_batch_size,\n",
    "    #push_to_hub=True,\n",
    "    fp16=True,\n",
    "    logging_steps=logging_steps,\n",
    "    load_best_model_at_end=True,\n",
    "    save_strategy = \"epoch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ca1c50e4-235f-414c-9999-524d09a6fecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9888d891-70c1-493f-a1c9-4d50a4529b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 270000\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4219' max='4219' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4219/4219 01:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 29.27\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b88a1ee4-909e-4270-ad5b-02963d03acc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = 'পরে সেখানে সংক্ষিপ্ত সমাবেশ অনুষ্ঠিত হয় ।'\n",
    "text =  '[MASK] সেখানে সংক্ষিপ্ত সমাবেশ অনুষ্ঠিত হয় ।'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "82a0bd9f-9606-4682-bcac-20eac66fe603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[MASK]', '▁সেখানে', '▁সংক্ষিপ্ত', '▁সমাবেশ', '▁অনুষ্ঠিত', '▁হয়', '▁', '।']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "532eb4f6-a499-4cf0-9427-cae5974034bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> পরে সেখানে সংক্ষিপ্ত সমাবেশ অনুষ্ঠিত হয় ।'\n",
      "'>>> এরপর সেখানে সংক্ষিপ্ত সমাবেশ অনুষ্ঠিত হয় ।'\n",
      "'>>> বিকেলে সেখানে সংক্ষিপ্ত সমাবেশ অনুষ্ঠিত হয় ।'\n",
      "'>>> সেখানে সেখানে সংক্ষিপ্ত সমাবেশ অনুষ্ঠিত হয় ।'\n",
      "'>>> দুপুরে সেখানে সংক্ষিপ্ত সমাবেশ অনুষ্ঠিত হয় ।'\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "inputs.to(\"cuda\")\n",
    "\n",
    "token_logits = model(**inputs).logits\n",
    "# Find the location of [MASK] and extract its logits\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5d3504-23df-41af-9a38-d50fa3fe966e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
