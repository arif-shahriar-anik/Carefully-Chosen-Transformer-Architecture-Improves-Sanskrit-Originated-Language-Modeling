{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6273017f-dd71-4be4-9dd6-7dcee760ad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167b240a-45f9-4342-bad0-0f31c501e6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29eb897a-3624-4a85-8b42-98e7cae3640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "prothom_alo_dataset = load_dataset(\"text\", data_files=\"../datasets/Bangla Prothom Alo.txt\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cb52d0-e9c7-431c-854c-53ece16fea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prothom_alo_dataset.set_format(\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299c6547-190a-4da7-88d8-ab81e0fa5d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prothom_alo_df = prothom_alo_dataset[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9d99f1-0a2d-427a-b68b-31ec0c9b1ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "(prothom_alo_df=='').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b962c0b9-fe54-4f0d-ae6b-50e79e740034",
   "metadata": {},
   "outputs": [],
   "source": [
    "prothom_alo_dataset.reset_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5d7a65-a87b-4479-ac07-425d23377d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "prothom_alo_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6c7e8e-54a1-4426-98c7-df4f8ea4828b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prothom_alo_dataset = prothom_alo_dataset.filter(lambda x: x[\"text\"]!=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10504ea5-991d-477a-9f26-c4875a269f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "prothom_alo_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939cd8f0-bf7c-4c6a-a1df-076c8b85169d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_size = 10_000\n",
    "# test_size = int(0.1 * train_size)\n",
    "\n",
    "downsampled_dataset = prothom_alo_dataset.train_test_split(\n",
    "    train_size=0.8, seed=42\n",
    ")\n",
    "downsampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733497b3-0f33-46a2-aeff-3cd4d58426c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datasets/Bangla Error Words.txt', encoding='utf-8') as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e342e3-0453-471b-8da2-754983bea8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_words = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e4682c-2693-4a37-b0e8-1a11135d926b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in lines:\n",
    "    combination = line.split()\n",
    "    original_word = combination[0]\n",
    "    modified_words = combination[1:]\n",
    "    error_words[original_word] = modified_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a90066f-cf22-4e8c-bb52-98757b7c5b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_words['গত']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7fe636-5010-4242-89e6-2afafbc64c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "def replace_error_word(sentence, error_words):\n",
    "    for error_word in error_words.keys():\n",
    "        if error_word in sentence:\n",
    "            #print(error_word)\n",
    "            index = np.random.randint(len(error_words[error_word]))\n",
    "            sentence = sentence.replace(error_word, error_words[error_word][index])\n",
    "            break\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f12f10-44c4-4ec9-b656-8cafa1294630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.randint(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25f81e4-7b95-40b6-95b5-26eff60aa24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace_error_word(\"তখন আমাদের দেখা হবে।\", error_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bca195f-3893-4646-b668-55ed6a1c3d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# index = 0\n",
    "# indices = list()\n",
    "# for sample in downsampled_dataset[\"test\"].select(range(10)):\n",
    "#     #replace 15% of time\n",
    "#     if np.random.random() < 0.15:\n",
    "#         replaced_sample = replace_error_word(sample['text'], error_words)\n",
    "#         sample['text'] = replaced_sample\n",
    "#         print(sample['text'])\n",
    "#         indices.append(index)\n",
    "#         count += 1\n",
    "#     print(index)\n",
    "#     index += 1\n",
    "        \n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa019a84-5304-49b9-9c03-c1cc8032e7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_text(examples):\n",
    "    # Create a corrupt example\n",
    "    #replace 15% of time\n",
    "    if np.random.random() < 0.20:\n",
    "        examples[\"text\"] = replace_error_word(examples['text'], error_words)\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1f015c-cf6d-4dad-8709-0711424bfc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_dataset[\"test\"] = downsampled_dataset[\"test\"].map(corrupt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88e563f-a6db-4558-ad5c-729ada478b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_dataset[\"test\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57836ffd-45de-4536-b9ed-a546933abdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    #tokenizer_object=tokenizer,\n",
    "    tokenizer_file=\"wordpiece_tokenizer_prothom_alo.json\", # You can load from the tokenizer file, alternatively\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    "    return_special_tokens_mask = True,\n",
    "    model_max_length = 512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50698ce1-5fd7-422f-befd-710d3004155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertForMaskedLM\n",
    "\n",
    "\n",
    "# Set a configuration for our RoBERTa model\n",
    "wordpiece_bert_config = BertConfig(pad_token_id=tokenizer.pad_token_id)\n",
    "\n",
    "# Building the model from the config\n",
    "# Model is randomly initialized\n",
    "model = BertForMaskedLM(wordpiece_bert_config)\n",
    "\n",
    "print(wordpiece_bert_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db65678-c9f9-4ed3-802f-a77610b13b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordpiece_bert_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdd2d01-3b9d-4709-b4ed-c10e8dc2ee31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"text\"], padding=\"max_length\", max_length=80, truncation=True)\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a86ec28-9d49-42b1-b015-68f41b537d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use batched=True to activate fast multithreading!\n",
    "tokenized_datasets = downsampled_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"text\"]\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9fbad5-e5f2-4ce4-9ab8-d2d232f72032",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets.remove_columns(\"token_type_ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e15856c-ca0a-4221-b8b8-eb07d4b96939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = tokenized_datasets.filter(lambda x:x if 1 in x[\"input_ids\"] else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b845fe6b-35c8-4ece-bcc6-bd7bc3737c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567d7be9-09f3-4aec-892a-ae66da2a0e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56703f4d-1ff9-48da-967f-a55f306b539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Create a new labels column\n",
    "    examples[\"labels\"] = examples[\"input_ids\"].copy()\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8003c22-d179-4638-81bf-cca0f1115e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cffe97c-7437-409e-9f31-38249c7ad4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_dataset[\"test\"][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0230e7e2-e25a-455d-bf6f-a1e08574571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7c5825-2b69-4b0e-9e04-0ee7cea4ba73",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(lm_datasets[\"train\"][1][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c62a289-44db-4e02-96c1-de078292a31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f98f81-76dc-4701-82ee-3292256f48e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "np.random\n",
    "from transformers import default_data_collator\n",
    "\n",
    "wwm_probability = 0.15\n",
    "\n",
    "\n",
    "def bangla_data_collator(features):\n",
    "    for feature in features:\n",
    "#         word_ids = feature.pop(\"word_ids\")\n",
    "\n",
    "#         # Create a map between words and corresponding token indices\n",
    "#         mapping = collections.defaultdict(list)\n",
    "#         current_word_index = -1\n",
    "#         current_word = None\n",
    "#         for idx, word_id in enumerate(word_ids):\n",
    "#             if word_id is not None:\n",
    "#                 if word_id != current_word:\n",
    "#                     current_word = word_id\n",
    "#                     current_word_index += 1\n",
    "#                 mapping[current_word_index].append(idx)\n",
    "\n",
    "        # Randomly mask words\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        labels = feature[\"labels\"]\n",
    "        mask = np.random.binomial(1, wwm_probability, (len(input_ids),))\n",
    "        special_tokens =  [tokenizer.unk_token_id, tokenizer.pad_token_id, tokenizer.cls_token_id, \\\n",
    "                           tokenizer.sep_token_id, tokenizer.mask_token_id]\n",
    "        \n",
    "        new_labels = [-100] * len(labels)\n",
    "        for idx in np.where(mask)[0]:\n",
    "#             word_id = word_id.item()\n",
    "#             print(word_id)\n",
    "#             for idx in mapping[word_id]:\n",
    "#             if word_ids[idx] is not None:\n",
    "            if input_ids[idx] not in special_tokens:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id\n",
    "            feature[\"labels\"] = new_labels\n",
    "        \n",
    "    return default_data_collator(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a48d09-ac22-493c-8250-adb7c1bf7e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "lm_datasets = lm_datasets.remove_columns([\"word_ids\"])\n",
    "data_collator = bangla_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f09a4c-3c33-4817-9608-f13b8a6e294a",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [lm_datasets[\"train\"][i] for i in range(3)]\n",
    "# for sample in samples:\n",
    "#     _ = sample.pop(\"word_ids\")\n",
    "\n",
    "for chunk in bangla_data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96d8b5c-abb9-4397-98ab-3e7110762f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [lm_datasets[\"train\"][i] for i in range(1)]\n",
    "\n",
    "chunk = data_collator(samples)\n",
    "print(chunk[\"input_ids\"])\n",
    "print(chunk[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a0472c-60ed-4d49-9fe0-318a6b8ec4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMaskedLM.from_pretrained(\"models/wordpiece/bert-base-pretrained-prothom-alo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a402ded8-6864-4d52-90c2-56b9b7153c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"models/wordpiece/bert-base-pretrained-prothom-alo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c50fc2-dc18-4ffc-9169-ee49d4de9ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  disable weights and biases logging\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68637099-2ce9-4a90-b90c-9b1cb2b2ec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 64\n",
    "# Show the training loss with every epoch|\n",
    "logging_steps = len(downsampled_dataset[\"train\"]) // batch_size\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs = 6,\n",
    "    report_to = None,\n",
    "    output_dir=\"models/wordpiece/bert-base-pretrained-prothom-alo\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    #push_to_hub=True,\n",
    "    fp16=True,\n",
    "    logging_steps=logging_steps,\n",
    "    load_best_model_at_end=True,\n",
    "    save_strategy = \"epoch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1c50e4-235f-414c-9999-524d09a6fecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9888d891-70c1-493f-a1c9-4d50a4529b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88a1ee4-909e-4270-ad5b-02963d03acc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = 'পরে সেখানে সংক্ষিপ্ত সমাবেশ অনুষ্ঠিত হয় ।'\n",
    "text =  'পরে সেখানে সংক্ষিপ্ত সমাবেশ অনুষ্ঠিত [MASK] ।'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb80d93-dc69-4259-8bf0-40d720102afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee746f1-1991-485e-a38b-097e58f78dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "inputs.to(\"cuda\")\n",
    "\n",
    "token_logits = model(**inputs).logits\n",
    "# Find the location of [MASK] and extract its logits\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7d0c8f-b246-4251-ae1e-c77933bbfad5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
