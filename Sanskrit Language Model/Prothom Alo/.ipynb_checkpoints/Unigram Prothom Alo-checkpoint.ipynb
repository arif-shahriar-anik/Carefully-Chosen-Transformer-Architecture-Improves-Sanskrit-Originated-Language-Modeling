{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4672c606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1b5ac6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 2080 Ti'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e28b2394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "\n",
    "# # start a new wandb run to track this script\n",
    "# wandb.init(\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project=\"unigram-bert-prothom-alo-epoch6\",\n",
    "    \n",
    "#     # track hyperparameters and run metadata\n",
    "#     config={\n",
    "#     \"learning_rate\": 2e-5,\n",
    "#     \"weight_decay\" : 0.01,\n",
    "#     \"architecture\": \"unigram-bert-base\",\n",
    "#     \"dataset\": \"prothom_alo\",\n",
    "#     \"epochs\": 6,\n",
    "#     \"batch size\": 64,\n",
    "#     }\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41a4ec8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset text (/home/ashahri1/.cache/huggingface/datasets/text/default-66e892579a4abe66/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "prothom_alo_dataset = load_dataset(\"text\", data_files=\"../datasets/Bangla Prothom Alo.txt\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "852527e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prothom_alo_dataset.set_format(\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34400f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "prothom_alo_dataset.set_format(\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3559f13-4ee7-4f91-9e34-818fb699ed42",
   "metadata": {},
   "outputs": [],
   "source": [
    "prothom_alo_df = prothom_alo_dataset[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4e6bff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prothom_alo_df_lens = prothom_alo_df['text'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97695c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "219"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(prothom_alo_df_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5212943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text    1350000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(prothom_alo_df=='').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62ae6765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretraining_df_lens = pretraining_df['text'].str.len()\n",
    "count = prothom_alo_df['text'].str.split().apply(len).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f37ff75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "count.index = count.index.astype(str) + ' words:'\n",
    "count.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a55a29a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text\n",
       "0 words:     1350000\n",
       "10 words:     144211\n",
       "11 words:     136289\n",
       "12 words:     125121\n",
       "13 words:     114017\n",
       "14 words:     100848\n",
       "15 words:      87522\n",
       "4 words:       39962\n",
       "5 words:       75678\n",
       "6 words:      106133\n",
       "7 words:      130050\n",
       "8 words:      143045\n",
       "9 words:      147124\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68e4a388",
   "metadata": {},
   "outputs": [],
   "source": [
    "prothom_alo_dataset.reset_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93c108a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 2700000\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prothom_alo_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "181ba3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ashahri1/.cache/huggingface/datasets/text/default-66e892579a4abe66/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-c308135649b65eaf.arrow\n"
     ]
    }
   ],
   "source": [
    "prothom_alo_dataset = prothom_alo_dataset.filter(lambda x: x[\"text\"]!=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec9b025b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 1350000\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prothom_alo_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b31d378-f4c8-44da-9c11-cbcc78f8ee53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['বাংলাদেশে রাজনৈতিক দলের সংখ্যা কত তা কেউ জানেন না ।',\n",
       "  'এ সময় আইনশৃঙ্খলাসহ সবকিছু নির্বাচন কমিশনের কাছে থাকবে ।',\n",
       "  'রাজনৈতিক দলের নিবন্ধনবর্তমান 1111111111 অনুচ্ছেদটি নিম্নরুপ 1111111111 ।',\n",
       "  'ছবির নাম দাগ ।',\n",
       "  'কেন তিনি এ বেনজির হত্যা মামলার তদন্ত করলেন না ?',\n",
       "  'আমরা শুধু অনুঘটকের দায়িত্ব পালন করি ।',\n",
       "  'এ কর্মসূচিতে তিনটি হলের প্রায় 1111111111 ছাত্রী অংশ নেন ।',\n",
       "  'প্রশ্ন আপনার সময়ে তো অনেকেই লিখেছেন ?',\n",
       "  'হেলথ মাস্টার হেলথ পাল স্বাস্থ্যকথন ও হ্যালোপ্যাথি রয়েছে স্বাস্থ্য ও জীবনযাপন বিভাগে ।',\n",
       "  'পুলিশ ঘটনার সঙ্গে জড়িত থাকার সন্দেহে দুজনকে আটক করেছে ।',\n",
       "  'উইকেট পেয়ে ইশান্তের গর্জনরত অভিব্যক্তিটা দেখে ভয় পেতেই পারত অস্ট্রেলিয়া ।',\n",
       "  'এ সময় তাঁরা ওই বাঁধগুলোর 1111111111 শতাংশ কাজ হয়েছে বলে দেখতে পান ।',\n",
       "  'ঠাঁই মেলে রোহিঙ্গা আশ্রয়শিবিরে ।',\n",
       "  'অস্কার অনুষ্ঠানের আর বেশি দিন বাকি নেই ।',\n",
       "  'এক প্রশ্নের জবাবে তিনি বলেন এখন আর ঝুঁকি নেই ।',\n",
       "  'মার্কিন মুলুকে বাঙালি অভিবাসীদের জীবনচিত্র অত্যন্ত হৃদয়গ্রাহীভাবে তুলে ধরেছেন নাসরিন চৌধুরী ।',\n",
       "  'সপ্তাহে তিন দিন এখানে এসে অফিস করলেও বাকি দুদিন না আসায় নানা সমস্যা হচ্ছে ।',\n",
       "  'দোতলা বাসে করে ছাত্রছাত্রীদের শিক্ষাপ্রতিষ্ঠান থেকে আনা নেওয়া করা হবে ।',\n",
       "  'হাতে তেমন ডলারও ছিল না ।',\n",
       "  'পুরো মাস পাবনাসভার বন্ধুরা কাটিয়ে দেয় নতুন বইয়ের সঙ্গে ।',\n",
       "  'বললাম আপনিও তো সরকারের অংশ ।',\n",
       "  'তঁারা সেটা করেনও ।',\n",
       "  'পরিচ্ছন্নতাকর্মীরা এসেছিলেন হাজতখানা পরিষ্কার করতে ।',\n",
       "  'বিকেন্দ্রীকরণে ব্যর্থতার দায় শুধু সরকারের ঘাড়ে চাপালে হবে না ।',\n",
       "  'আমার কৈশোরের ঈদ উপভোগের ওটাই ছিল সেরা সময় ।',\n",
       "  'আমি সামাজিক যোগাযোগমাধ্যমে আমার মেয়ের ছবিও তাই প্রকাশ করি না ।',\n",
       "  'আমি তখন মগ হাতে পানি খাচ্ছিলাম ।',\n",
       "  'এ সময় অপর একটি পক্ষ সেখানে ইয়াছিনের ওপর হামলা করে ।',\n",
       "  'কোনা সরকারি গাড়ি তাঁর আত্মীয়দের জন্য ব্যবহৃত হয়নি ।',\n",
       "  'কিন্তু ওই যে টেস্ট চলাকালে ড্রেসিংরুমের পথটা ওয়ান ওয়ে ।',\n",
       "  'এ রকম কত কত দশক শতক পেরিয়ে যায় টেম্পারার ছবি মলিন হয় না ।',\n",
       "  'তাই সব মিলিয়ে ভবিষ্যতে সম্প্রসারিত মার্কিন ব্রিটিশ তুর্কি ইসরায়েলি অক্ষশক্তির উত্থান দেখা যেতেও পারে ।',\n",
       "  'দরজায় কড়া নাড়ার শব্দে কেউ সাড়া দেননি ।',\n",
       "  'ভিভালদি ও অপেরা ব্রাউজারে ক্রোমের অনেক এক্সটেনশনই সমর্থন করে ।',\n",
       "  'বাকিরা অন্যান্য বিভাগের বা অনুষদের ।',\n",
       "  'সংঘর্ষের সময় ঘরবাড়িতে হামলা ভাঙচুর ও অগ্নিসংযোগের ঘটনাও ঘটে ।',\n",
       "  'সেখানে রবীন্দ্রনাথ ঠাকুরের গান গেয়ে শোনাবেন আনন্দময়ী মজুমদার ও বুলবুল ইসলাম ।',\n",
       "  'সৌদ বলেন আমি দর্শকের জন্য ছবি বানিয়েছি ।',\n",
       "  'তবে আমরা বুঝতে পারছিলাম বুশ প্রশাসন ইরাকে কিছু একটা করতে চায় ।',\n",
       "  'যাবতীয় কেনাকাটা শুভ ।',\n",
       "  'এবার বিশেষ সংগীত সম্মান পেয়েছেন জিৎ গঙ্গোপাধ্যায় ও অভিজিৎ ।',\n",
       "  'পরিবেশ আন্দোলনের কর্মীরা বলেছেন অবৈধ কসাইখানাগুলোর বর্জ্যে পরিবেশ নষ্ট হচ্ছে ।',\n",
       "  'চাঁপাইনবাবগঞ্জ শহরের বঙ্গবন্ধু মুক্তমঞ্চের সামনে থেকে দুপুরে বিক্ষোভ মিছিল শুরু হয় ।',\n",
       "  'এ ছাড়া হানিফের ক্রেডিট কার্ডও নিয়ে গেছে তারা ।',\n",
       "  'এনবিআর চেয়ারম্যান বলেন নতুন মূসক আইন নিয়ে ক্ষুদ্র ও খুচরা ব্যবসায়ীদের কিছু আপত্তি আছে ।',\n",
       "  'কীভাবে যেন দ্রুত তাদের কীর্তি এলাকায় ছড়িয়ে পড়ে ।',\n",
       "  'আশা করছি আগামী সপ্তাহের মধ্যে একটা সমাধান হবে ।',\n",
       "  'প্রেসিডেন্ট ট্রাম্প চেয়েছেন ব্যক্তিগত আনুগত্য ।',\n",
       "  'ইতিহাস 1111111111 বছরের টেস্ট ইতিহাস ম্যাচের প্রথম বলে ছক্কা দেখেছে ওই একবারই ।',\n",
       "  'ব্যাগপত্র আনার কথা বলে ফাতেমাকে স্বপ্নার কোলে রেখে যান তিনি ।',\n",
       "  'রাজধানী ঢাকাসহ দেশের বিভিন্ন এলাকা থেকে পুণ্যার্থীসহ সাধারণ মানুষও এ উৎসবে আসেন ।',\n",
       "  'পুলিশকেও কিছু জানাননি ।',\n",
       "  'তাঁর নাম রবীন্দ্র পাল ।',\n",
       "  'সরকারি কাঠামোতে বেতন দিয়ে গবেষণা হয় না ।',\n",
       "  'রাষ্ট্রপতি আবদুল হামিদের মধ্যে সব সময়ই হাস্যরসের মাধ্যমে সত্য কথা বলার প্রবণতা রয়েছে ।',\n",
       "  'অপরাধ প্রমাণিত না হওয়ায় জাহানুর বেগমকে বেকসুর খালাস দেওয়া হয় ।',\n",
       "  'এতে যেকোনো মুহূর্তে বড় ধরনের দুর্ঘটনার আশঙ্কা রয়েছে ।',\n",
       "  'নিউজিল্যান্ডের কাছে একটা ওয়ানডে সিরিজ হারেই তো আর সব এলোমেলো হয়ে যায়নি ।',\n",
       "  'কিন্তু বন্দর ও কাস্টমস কর্তৃপক্ষ আমদানিকারকের আবেদন নাকচ করে দেন ।',\n",
       "  'পাঁচ মিনিট পরই অবশ্য সেটি শোধ দিয়ে দেয় আর্জেন্টিনা ।',\n",
       "  'আমরা এভাবে আসলে মাকে নিয়ে মূল্যায়ন করি না ।',\n",
       "  'সঙ্গে মন্ত্রী আমলাদের এক বড়সড় প্রতিনিধিদল ।',\n",
       "  'সেখানে অনলাইনে আসা নিবন্ধনের আবেদন সংরক্ষণ করছেন কেউ ।',\n",
       "  'তাঁদের বেশির ভাগই অন্যের জমি টাকার বিনিময়ে পত্তন নিয়ে চাষাবাদ করেছিলেন ।',\n",
       "  'সংসদীয় ব্যবস্থায় যখন তখন অধ্যাদেশ জারির বিরোধিতা করেন এই প্রবীণ রাজনীতিক ।',\n",
       "  'ভারতের রাজধানী নয়াদিল্লির একটা শরণার্থী শিবিরে আমার জন্ম ।',\n",
       "  'পুরোনো বিপণিবিতানগুলো যেসব ভবনে তার বড় অংশেরই বৈদ্যুতিক তার নষ্ট হয়ে গেছে ।',\n",
       "  'নৌবাহিনী ও ফায়ার সার্ভিসের ডুবুরি দল ঘটনাস্থলে যাচ্ছে ।',\n",
       "  'কিন্তু একটা সময় পর আপনিও পারবেন ।',\n",
       "  'ভাষা হারিয়ে ফেলেছি ।',\n",
       "  'প্রেমিক বা প্রেমিকার মড়া জলে ভাসে না ।',\n",
       "  'সিদ্দিকুর যেখানে গল্পের নায়ক ।',\n",
       "  'এ রকম কিছু হলে তা সরকারের জন্য চরম বিব্রতকর হতো তাতে সন্দেহ নেই ।',\n",
       "  'প্রথমে তাঁকে চৌহালী স্বাস্থ্য কমপ্লেক্সে নিয়ে যাওয়া হয় ।',\n",
       "  'আজিজ সুপার মার্কেট থেকে বিভিন্ন ফ্যাশন হাউসসবখানেই খাদির পোশাক চোখে পড়ে কমবেশি ।',\n",
       "  'আমরা ওর বেডরুমে ঢুকলাম ।',\n",
       "  'গলের তরুণ ক্রিকেটারদের অবশ্য উদ্বেগ ধরা পড়ছে না ।',\n",
       "  'ঝড়ের তাণ্ডবে তখন প্রাণ হারায় তাঁর মেয়ে মারজান 1111111111 ।',\n",
       "  'তবে বাকিদের আরও অবনমন তাঁকে সেরা দশে তুলে আনল ।',\n",
       "  'পরে তাঁরা এখানে আসেন ।',\n",
       "  'সেখানে বেলা সাড়ে তিনটার দিকে তিনি মারা যান ।',\n",
       "  'চ্যাম্পিয়নস ট্রফিতে দুবার শিরোপাজয়ী একমাত্র দল অস্ট্রেলিয়ার দুটি ম্যাচেই যেমন আসেনি ।',\n",
       "  'সেখানে গিয়ে দেখা যায় সারিবদ্ধভাবে নিজ নিজ সন্তানের সামনে বসে আছেন মায়েরা ।',\n",
       "  'রোববার রেলওয়ে পুলিশ তার মৃতদেহ ময়নাতদন্তের জন্য ঢাকা মেডিকেল কলেজ মর্গে পাঠায় ।',\n",
       "  'গত সপ্তাহে বিআরটিএ থেকে সবগুলো বাসের অনুমতিপত্র নেওয়া হয় ।',\n",
       "  'অভিযানে যেসব সরঞ্জাম দরকার হতে পারে সেগুলো নিশ্চিত করতে অস্থায়ী কন্ট্রোল রুমকে নির্দেশ দিলাম ।',\n",
       "  'নিচতলায় কাচের ফ্রেমের ভেতরেও সাঠানো শাকিব খানের রাজনীতি ।',\n",
       "  'আরেক প্রান্ত তীরের ওপর ।',\n",
       "  'এ ছাড়া অগমেডিক্সে যোগ্যতার ভিত্তিতে অন্যান্য পদও রয়েছে ।',\n",
       "  'জিজ্ঞাসাবাদে তাঁর কাছ থেকে পাওয়া তথ্যগুলো যাচাই বাছাই করা হচ্ছে ।',\n",
       "  'তারপর চিকিৎসকের সহায়তা ।',\n",
       "  'সাত মাস আগে রাস্তায় গাড়িটি চলা শুরু করে ।',\n",
       "  'ওই মিশন ইউরোপের বাজারে বাংলাদেশের অগ্রাধিকারমূলক বাজারসুবিধা জিএসপি পাওয়ার বিষয়ে তদন্ত করবে ।',\n",
       "  'দাপট পুরো ক্যাম্পেজেনেভা ক্যাম্প এলাকায় 1111111111 হাজারের মতো আটকে পড়া পাকিস্তানি বসবাস করে ।',\n",
       "  'কিন্তু যদি বিভিন্ন দেশ পারমাণবিক অস্ত্রের প্রতিযোগিতায় নামে তাহলে আমরা সবার ওপরে থাকতে চাই ।',\n",
       "  'নয় দিনব্যাপী আগস্টের নাট্যসম্ভার শিরোনামের নাট্যোৎসবের তৃতীয় দিন ছিল গতকাল ।',\n",
       "  'সিলেটে নিহত নারীর নাম হেলেন শর্মা 1111111111 ।',\n",
       "  'তারপর তাঁরা করেন চেন্নাই এক্সপ্রেস ও হ্যাপি নিউইয়ার ছবি দুটি ।',\n",
       "  'এ নিয়ে গত 1111111111 দিনে বিমান ও সৌদি এয়ারলাইনসের 1111111111 হজ ফ্লাইট বাতিল হলো ।',\n",
       "  'পরে শেলী ও তাঁর মা জানতে পারেন অ্যাসিড ছুড়ে মারা হয়েছে তাঁদের ওপর ।']}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prothom_alo_dataset[100:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d7cf45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    for i in range(0, len(prothom_alo_dataset), 1000):\n",
    "        yield prothom_alo_dataset[i : i+1000][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa85920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8dac1de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.Unigram())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b453a38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Regex\n",
    "\n",
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [\n",
    "        normalizers.Replace(\"``\", '\"'),\n",
    "        normalizers.Replace(\"''\", '\"'),\n",
    "        normalizers.NFKC(),\n",
    "#         normalizers.StripAccents(),\n",
    "        normalizers.Replace(Regex(\" {2,}\"), \" \"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52a3ebfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "রিয়াল ভাবতে পারে তারা আসলে 1111111111 পয়েন্টে এগিয়ে ।\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.normalizer.normalize_str('রিয়াল ভাবতে পারে তারা আসলে 1111111111 পয়েন্টে এগিয়ে ।'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97d9f5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_tokenizer = pre_tokenizers.Sequence(\n",
    "    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ae668a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bce83017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁রিয়াল', (0, 5)),\n",
       " ('▁ভাবতে', (5, 11)),\n",
       " ('▁পারে', (11, 16)),\n",
       " ('▁তারা', (16, 21)),\n",
       " ('▁আসলে', (21, 26)),\n",
       " ('▁1111111111', (26, 37)),\n",
       " ('▁পয়েন্টে', (37, 45)),\n",
       " ('▁এগিয়ে', (45, 51)),\n",
       " ('▁।', (51, 53))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer.pre_tokenize_str('রিয়াল ভাবতে পারে তারা আসলে 1111111111 পয়েন্টে এগিয়ে ।')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0190dbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.UnigramTrainer(\n",
    "    vocab_size=30522, special_tokens=special_tokens, unk_token=\"[UNK]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d057ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ec6c262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁রিয়াল', '▁ভাবতে', '▁পারে', '▁তারা', '▁আসলে', '▁', '1', '111111111', '▁পয়েন্টে', '▁এগিয়ে', '▁', '।']\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode('রিয়াল ভাবতে পারে তারা আসলে 1111111111 পয়েন্টে এগিয়ে ।')\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4bb64fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3\n"
     ]
    }
   ],
   "source": [
    "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
    "print(cls_token_id, sep_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "301d56bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0f5aa63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '▁রিয়াল', '▁ভাবতে', '▁পারে', '▁তারা', '▁আসলে', '▁', '1', '111111111', '▁পয়েন্টে', '▁এগিয়ে', '▁', '।', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode('রিয়াল ভাবতে পারে তারা আসলে 1111111111 পয়েন্টে এগিয়ে ।')\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d4b375a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '▁এ', '▁ছাড়া', '▁', 'শিক্ষাপ্রতিষ্ঠান', 'ে', 'ও', '▁চলবে', '▁প্রচারণা', '▁', '।', '[SEP]', '▁সহযোগিতা', '▁করছে', '▁তথ্য', '▁', 'ও', '▁', 'যোগাযোগপ্রযুক্তি', '▁আইসিটি', '▁বিভাগ', '▁', '।', '[SEP]']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"এ ছাড়া শিক্ষাপ্রতিষ্ঠানেও চলবে প্রচারণা ।\",\"সহযোগিতা করছে তথ্য ও যোগাযোগপ্রযুক্তি আইসিটি বিভাগ ।\")\n",
    "print(encoding.tokens)\n",
    "print(encoding.type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "54484d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.Metaspace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65d5a2d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'এ ছাড়া শিক্ষাপ্রতিষ্ঠানেও চলবে প্রচারণা । সহযোগিতা করছে তথ্য ও যোগাযোগপ্রযুক্তি আইসিটি বিভাগ ।'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e271e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.enable_truncation(max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c24baa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"unigram_tokenizer_prothom_alo.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0dfadb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer.from_file(\"unigram_tokenizer_prothom_alo.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ae2fab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    #tokenizer_object=tokenizer,\n",
    "    tokenizer_file=\"unigram_tokenizer_prothom_alo.json\", # You can load from the tokenizer file, alternatively\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    "    return_special_tokens_mask = True,\n",
    "    model_max_length = 512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77046651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('unigram_tokenizers_prothom_alo/tokenizer_config.json',\n",
       " 'unigram_tokenizers_prothom_alo/special_tokens_map.json',\n",
       " 'unigram_tokenizers_prothom_alo/tokenizer.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"unigram_tokenizers_prothom_alo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20953781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# tokenizer = PreTrainedTokenizerFast.from_pretrained(\"unigram_tokenizers_prothom_alo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec2d937a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertForMaskedLM\n",
    "\n",
    "# Building the config\n",
    "config = BertConfig()\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ad4d269-eef9-4a14-9fa2-63617c8528be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test tokenizer\n",
    "# tokenizer.tokenize(\"শিক্ষাপ্রতিষ্ঠানেও\")\n",
    "# tokenizer.tokenize(\"পাচজনকে\")\n",
    "# tokenizer.tokenize(\"অংশগুলি\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05aca711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unigram_bert_config = BertConfig(pad_token_id=tokenizer.pad_token_id)\n",
    "print(unigram_bert_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7bfa9d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_bert_config.save_pretrained(save_directory=\"configs/unigram_bert_config/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1391b06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_bert_config = BertConfig.from_pretrained(\"configs/unigram_bert_config/config.json\")\n",
    "# Building the model from the config\n",
    "# Model is randomly initialized\n",
    "model = BertForMaskedLM(unigram_bert_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57f400c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.29.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_bert_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f0d1af6-8a77-422d-a2ef-410e59e2d1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 109,360,128\n"
     ]
    }
   ],
   "source": [
    "total_params = 0\n",
    "for p in model.parameters():\n",
    "    if len(p.shape) == 2:\n",
    "        total_params += p.shape[0] * p.shape[1]\n",
    "        \n",
    "print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26ba7cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = 'পরে সেখানে সংক্ষিপ্ত সমাবেশ অনুষ্ঠিত হয় ।'\n",
    "text = 'পরে সেখানে সংক্ষিপ্ত সমাবেশ [MASK] হয় ।'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1d96610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁পরে', '▁সেখানে', '▁সংক্ষিপ্ত', '▁সমাবেশ', '▁', '[MASK]', '▁হয়', '▁', '।']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37fcc364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> পরে সেখানে সংক্ষিপ্ত সমাবেশ মৃণ্ময় হয় ।'\n",
      "'>>> পরে সেখানে সংক্ষিপ্ত সমাবেশ পাথরবোঝা হয় ।'\n",
      "'>>> পরে সেখানে সংক্ষিপ্ত সমাবেশ বিদ্যালয়গুলো হয় ।'\n",
      "'>>> পরে সেখানে সংক্ষিপ্ত সমাবেশ সচেতন হয় ।'\n",
      "'>>> পরে সেখানে সংক্ষিপ্ত সমাবেশ লেখক হয় ।'\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "# inputs.to(\"cuda\")\n",
    "\n",
    "token_logits = model(**inputs).logits\n",
    "# Find the location of [MASK] and extract its logits\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "efc4fe00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ashahri1/.cache/huggingface/datasets/text/default-66e892579a4abe66/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-f15318c8addee905.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids'],\n",
       "    num_rows: 1350000\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"text\"], padding=\"max_length\", max_length=80, truncation=True)\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "\n",
    "# Use batched=True to activate fast multithreading!\n",
    "tokenized_datasets = prothom_alo_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"text\"]\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "38acd53a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "    num_rows: 1350000\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets.remove_columns(\"token_type_ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "526f7924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = tokenized_datasets.filter(lambda x:x if 1 in x[\"input_ids\"] else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91e59b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2,\n",
       "  1434,\n",
       "  1843,\n",
       "  80,\n",
       "  111,\n",
       "  574,\n",
       "  5,\n",
       "  7,\n",
       "  8,\n",
       "  4444,\n",
       "  341,\n",
       "  5,\n",
       "  6,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'word_ids': [None,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3e939480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Create a new labels column\n",
    "    examples[\"labels\"] = examples[\"input_ids\"].copy()\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "084e9842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1350000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "    num_rows: 1350000\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a0231e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] পরে সেখানে সংক্ষিপ্ত সমাবেশ অনুষ্ঠিত হয় ।[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets[1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "522f2ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] পরে সেখানে সংক্ষিপ্ত সমাবেশ অনুষ্ঠিত হয় ।[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets[1][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "04a2f73c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "94f7dcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "np.random\n",
    "from transformers import default_data_collator\n",
    "\n",
    "wwm_probability = 0.15\n",
    "\n",
    "\n",
    "def bangla_data_collator(features):\n",
    "    for feature in features:\n",
    "#         word_ids = feature.pop(\"word_ids\")\n",
    "\n",
    "#         # Create a map between words and corresponding token indices\n",
    "#         mapping = collections.defaultdict(list)\n",
    "#         current_word_index = -1\n",
    "#         current_word = None\n",
    "#         for idx, word_id in enumerate(word_ids):\n",
    "#             if word_id is not None:\n",
    "#                 if word_id != current_word:\n",
    "#                     current_word = word_id\n",
    "#                     current_word_index += 1\n",
    "#                 mapping[current_word_index].append(idx)\n",
    "\n",
    "        # Randomly mask words\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        labels = feature[\"labels\"]\n",
    "        mask = np.random.binomial(1, wwm_probability, (len(input_ids),))\n",
    "        special_tokens =  [tokenizer.unk_token_id, tokenizer.pad_token_id, tokenizer.cls_token_id, \\\n",
    "                           tokenizer.sep_token_id, tokenizer.mask_token_id]\n",
    "        \n",
    "        new_labels = [-100] * len(labels)\n",
    "        for idx in np.where(mask)[0]:\n",
    "#             word_id = word_id.item()\n",
    "#             print(word_id)\n",
    "#             for idx in mapping[word_id]:\n",
    "#             if word_ids[idx] is not None:\n",
    "            if input_ids[idx] not in special_tokens:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id\n",
    "            feature[\"labels\"] = new_labels\n",
    "        \n",
    "    return default_data_collator(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "21576526",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "lm_datasets = lm_datasets.remove_columns([\"word_ids\"])\n",
    "data_collator = bangla_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "17aec7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] রিয়াল ভাবতে পারে[MASK] আসলে 1111111111[MASK] এগিয়ে ।[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]'\n",
      "\n",
      "'>>> [CLS] পরে সেখানে সংক্ষিপ্ত সমাবেশ অনুষ্ঠিত[MASK] ।[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]'\n",
      "\n",
      "'>>> [CLS] বেঁচে থাকার[MASK] ভাবিনি বলছিলেন নূর মোহাম্মদ ।[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]'\n"
     ]
    }
   ],
   "source": [
    "samples = [lm_datasets[i] for i in range(3)]\n",
    "# for sample in samples:\n",
    "#     _ = sample.pop(\"word_ids\")\n",
    "\n",
    "for chunk in bangla_data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d6ff8ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['রিয়াল ভাবতে পারে তারা আসলে 1111111111 পয়েন্টে এগিয়ে ।',\n",
       "  'পরে সেখানে সংক্ষিপ্ত সমাবেশ অনুষ্ঠিত হয় ।',\n",
       "  'বেঁচে থাকার কথা ভাবিনি বলছিলেন নূর মোহাম্মদ ।']}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prothom_alo_dataset[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "92973520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   2, 1434, 1843,    4,  111,  574,    5,    7,    4, 4444,  341,    5,\n",
      "            6,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1]])\n",
      "tensor([[-100, -100, -100,   80, -100, -100, -100, -100,    8, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100]])\n"
     ]
    }
   ],
   "source": [
    "samples = [lm_datasets[i] for i in range(1)]\n",
    "\n",
    "chunk = data_collator(samples)\n",
    "print(chunk[\"input_ids\"])\n",
    "print(chunk[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "23d1f636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1080000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 270000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_size = 10_000\n",
    "# test_size = int(0.1 * train_size)\n",
    "\n",
    "downsampled_dataset = lm_datasets.train_test_split(\n",
    "    train_size=0.8, seed=42\n",
    ")\n",
    "downsampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "053ed7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Article 0 length: 80'\n",
      "'>>> Article 1 length: 80'\n",
      "'>>> Article 2 length: 80'\n"
     ]
    }
   ],
   "source": [
    "for idx, sample in enumerate(downsampled_dataset[\"train\"][\"input_ids\"][:3]):\n",
    "    print(f\"'>>> Article {idx} length: {len(sample)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "596d299d-dcf4-44dc-aacb-c3e811a69b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] যত দ্রুত সম্ভব এই ঘুষ দুর্নীতি দমন করতে হবে ।[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(downsampled_dataset[\"test\"][\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a00cd613",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  disable weights and biases logging\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "80c32c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bce9a906",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 64\n",
    "eval_batch_size = 64\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(downsampled_dataset[\"train\"]) // batch_size\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs = 3,\n",
    "    report_to = None,\n",
    "    output_dir=\"models/unigram/bert-base-pretrained-prothom-alo\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=eval_batch_size,\n",
    "    #push_to_hub=True,\n",
    "    fp16=True,\n",
    "    logging_steps=logging_steps,\n",
    "    load_best_model_at_end=True,\n",
    "    save_strategy = \"epoch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "09a65f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=downsampled_dataset[\"train\"],\n",
    "    eval_dataset=downsampled_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4adb2b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 270000\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4219' max='4219' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4219/4219 03:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 25.81\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9e2e6ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1080000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 50625\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50625' max='50625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50625/50625 2:43:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.075400</td>\n",
       "      <td>3.390895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.010000</td>\n",
       "      <td>3.308410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.006700</td>\n",
       "      <td>3.252617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 270000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to models/unigram/bert-base-pretrained-prothom-alo/checkpoint-16875\n",
      "Configuration saved in models/unigram/bert-base-pretrained-prothom-alo/checkpoint-16875/config.json\n",
      "Model weights saved in models/unigram/bert-base-pretrained-prothom-alo/checkpoint-16875/pytorch_model.bin\n",
      "tokenizer config file saved in models/unigram/bert-base-pretrained-prothom-alo/checkpoint-16875/tokenizer_config.json\n",
      "Special tokens file saved in models/unigram/bert-base-pretrained-prothom-alo/checkpoint-16875/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to models/unigram/bert-base-pretrained-prothom-alo/checkpoint-33750\n",
      "Configuration saved in models/unigram/bert-base-pretrained-prothom-alo/checkpoint-33750/config.json\n",
      "Model weights saved in models/unigram/bert-base-pretrained-prothom-alo/checkpoint-33750/pytorch_model.bin\n",
      "tokenizer config file saved in models/unigram/bert-base-pretrained-prothom-alo/checkpoint-33750/tokenizer_config.json\n",
      "Special tokens file saved in models/unigram/bert-base-pretrained-prothom-alo/checkpoint-33750/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to models/unigram/bert-base-pretrained-prothom-alo/checkpoint-50625\n",
      "Configuration saved in models/unigram/bert-base-pretrained-prothom-alo/checkpoint-50625/config.json\n",
      "Model weights saved in models/unigram/bert-base-pretrained-prothom-alo/checkpoint-50625/pytorch_model.bin\n",
      "tokenizer config file saved in models/unigram/bert-base-pretrained-prothom-alo/checkpoint-50625/tokenizer_config.json\n",
      "Special tokens file saved in models/unigram/bert-base-pretrained-prothom-alo/checkpoint-50625/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from models/unigram/bert-base-pretrained-prothom-alo/checkpoint-50625 (score: 3.252617120742798).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50625, training_loss=3.0306976080246915, metrics={'train_runtime': 9800.6714, 'train_samples_per_second': 330.59, 'train_steps_per_second': 5.165, 'total_flos': 1.3324743648e+17, 'train_loss': 3.0306976080246915, 'epoch': 3.0})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0182776e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 270000\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4219' max='4219' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4219/4219 03:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 26.05\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "48f815a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to models/unigram/bert-base-pretrained-prothom-alo\n",
      "Configuration saved in models/unigram/bert-base-pretrained-prothom-alo/config.json\n",
      "Model weights saved in models/unigram/bert-base-pretrained-prothom-alo/pytorch_model.bin\n",
      "tokenizer config file saved in models/unigram/bert-base-pretrained-prothom-alo/tokenizer_config.json\n",
      "Special tokens file saved in models/unigram/bert-base-pretrained-prothom-alo/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5cd924e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▇▅▂█▇█▅▁█▇█</td></tr><tr><td>eval/runtime</td><td>▇▄█▂▁▂▄▄▁▁▃</td></tr><tr><td>eval/samples_per_second</td><td>▂▄▁▆█▇▅▄█▇▆</td></tr><tr><td>eval/steps_per_second</td><td>▂▄▁▆█▇▅▄█▇▆</td></tr><tr><td>train/epoch</td><td>█▁▁▅▅████▁▁▅▅████</td></tr><tr><td>train/global_step</td><td>█▁▃▃▆▆████▁▃▃▆▆████</td></tr><tr><td>train/learning_rate</td><td>█▄▁█▄▁</td></tr><tr><td>train/loss</td><td>▅▇█▁▅▇</td></tr><tr><td>train/total_flos</td><td>▁▁</td></tr><tr><td>train/train_loss</td><td>█▁</td></tr><tr><td>train/train_runtime</td><td>█▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁█</td></tr><tr><td>train/train_steps_per_second</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>6.92882</td></tr><tr><td>eval/runtime</td><td>4.4849</td></tr><tr><td>eval/samples_per_second</td><td>222.968</td></tr><tr><td>eval/steps_per_second</td><td>55.742</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>7500</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>6.4857</td></tr><tr><td>train/total_flos</td><td>1974036096000000.0</td></tr><tr><td>train/train_loss</td><td>6.33972</td></tr><tr><td>train/train_runtime</td><td>729.7812</td></tr><tr><td>train/train_samples_per_second</td><td>41.108</td></tr><tr><td>train/train_steps_per_second</td><td>10.277</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">true-forest-8</strong>: <a href=\"https://wandb.ai/ashahri1/unigram-bert-project-demo/runs/64hq7non\" target=\"_blank\">https://wandb.ai/ashahri1/unigram-bert-project-demo/runs/64hq7non</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230526_150856-64hq7non\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [optional] finish the wandb run, necessary in notebooks\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6af3e354",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file models/unigram/bert-base-pretrained-prothom-alo/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"models/unigram/bert-base-pretrained-prothom-alo/checkpoint-101250\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file models/unigram/bert-base-pretrained-prothom-alo/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at models/unigram/bert-base-pretrained-prothom-alo.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = BertForMaskedLM.from_pretrained(\"models/unigram/bert-base-pretrained-prothom-alo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5b380fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file models/unigram/bert-base-pretrained-prothom-alo/added_tokens.json. We won't load it.\n",
      "loading file None\n",
      "loading file models/unigram/bert-base-pretrained-prothom-alo/special_tokens_map.json\n",
      "loading file models/unigram/bert-base-pretrained-prothom-alo/tokenizer_config.json\n",
      "loading file models/unigram/bert-base-pretrained-prothom-alo/tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"models/unigram/bert-base-pretrained-prothom-alo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "f10aab00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
