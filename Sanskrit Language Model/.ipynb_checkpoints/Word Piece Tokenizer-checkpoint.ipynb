{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e4ef1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f41f6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ffdb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"wordpiece-bert-project-epoch24\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"weight_decay\" : 0.01,\n",
    "    \"architecture\": \"wordpiece-bert-base\",\n",
    "    \"dataset\": \"bdnews24\",\n",
    "    \"epochs\": 24,\n",
    "    \"batch size\": 64,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a4ec8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "bdnews_dataset = load_dataset(\"text\", data_files=\"datasets/Bangla BDnews.txt\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852527e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdnews_dataset.set_format(\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34400f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdnews_df = bdnews_dataset[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e6bff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdnews_df_lens = bdnews_df['text'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97695c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(bdnews_df_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aa188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretraining_df_lens = pretraining_df['text'].str.len()\n",
    "count = bdnews_df['text'].str.split().apply(len).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b937b51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "count.index = count.index.astype(str) + ' words:'\n",
    "count.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb0d072",
   "metadata": {},
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e4a388",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdnews_dataset.reset_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c108a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdnews_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9b025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdnews_dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7cf45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    for i in range(0, len(bdnews_dataset), 1000):\n",
    "        yield bdnews_dataset[i : i+1000][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa85920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dac1de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846399a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [normalizers.NFKC()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a3ebfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.normalizer.normalize_str('গ্রীন ব্যাংকিং বা পরিবেশবান্ধব ব্যাংকিং ও ছাদ বাগান কার্যক্রম শুরু করেছে রাষ্ট্রমালিকানাধীন অগ্রণী ব্যাংক ।'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d9f5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce83017",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer.pre_tokenize_str('গ্রীন ব্যাংকিং বা পরিবেশবান্ধব ব্যাংকিং ও ছাদ বাগান কার্যক্রম শুরু করেছে রাষ্ট্রমালিকানাধীন অগ্রণী ব্যাংক ।')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0190dbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.WordPieceTrainer(\n",
    "    vocab_size=30522, special_tokens=special_tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d057ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb64fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
    "print(cls_token_id, sep_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301d56bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bb8faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode(\"এ ছাড়া শিক্ষাপ্রতিষ্ঠানেও চলবে প্রচারণা ।\",\"সহযোগিতা করছে তথ্য ও যোগাযোগপ্রযুক্তি আইসিটি বিভাগ ।\")\n",
    "print(encoding.tokens)\n",
    "print(encoding.type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54484d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.WordPiece(prefix=\"##\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d5a2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e271e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.enable_truncation(max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24baa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.save(\"wordpiece_tokenizer_bdnews.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfadb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer.from_file(\"unigram_tokenizer_bdnews.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48c22ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizerFast\n",
    "\n",
    "# tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c66e6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    #tokenizer_object=tokenizer,\n",
    "    tokenizer_file=\"wordpiece_tokenizer_bdnews.json\", # You can load from the tokenizer file, alternatively\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    "    return_special_tokens_mask = True,\n",
    "    model_max_length = 512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daf57d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.save_pretrained(\"wordpiece_tokenizer_bdnews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a806cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from transformers import BertTokenizerFast\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# tokenizer = PreTrainedTokenizerFast.from_pretrained(\"wordpiece_tokenizer_bdnews\")\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"models/wordpiece/bert-base-pretrained-bdnews24\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2d937a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertForMaskedLM\n",
    "\n",
    "\n",
    "# Set a configuration for our RoBERTa model\n",
    "unigram_bert_config = BertConfig(pad_token_id=tokenizer.pad_token_id)\n",
    "\n",
    "# Building the model from the config\n",
    "# Model is randomly initialized\n",
    "model = BertForMaskedLM(unigram_bert_config)\n",
    "\n",
    "print(unigram_bert_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ba7cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"এ ছাড়া শিক্ষাপ্রতিষ্ঠানেও চলবে [MASK] ।\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9e221d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "# inputs.to(\"cuda\")\n",
    "\n",
    "token_logits = model(**inputs).logits\n",
    "# Find the location of [MASK] and extract its logits\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "# # Pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc4fe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"text\"], padding=\"max_length\", max_length=128, truncation=True)\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "\n",
    "# Use batched=True to activate fast multithreading!\n",
    "tokenized_datasets = bdnews_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"text\"]\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38acd53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets.remove_columns(\"token_type_ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e59b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e939480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Create a new labels column\n",
    "    examples[\"labels\"] = examples[\"input_ids\"].copy()\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084e9842",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0231e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(lm_datasets[1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522f2ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(lm_datasets[1][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcc831d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95222bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "np.random\n",
    "from transformers import default_data_collator\n",
    "\n",
    "wwm_probability = 0.15\n",
    "\n",
    "\n",
    "def bangla_data_collator(features):\n",
    "    for feature in features:\n",
    "#         word_ids = feature.pop(\"word_ids\")\n",
    "\n",
    "#         # Create a map between words and corresponding token indices\n",
    "#         mapping = collections.defaultdict(list)\n",
    "#         current_word_index = -1\n",
    "#         current_word = None\n",
    "#         for idx, word_id in enumerate(word_ids):\n",
    "#             if word_id is not None:\n",
    "#                 if word_id != current_word:\n",
    "#                     current_word = word_id\n",
    "#                     current_word_index += 1\n",
    "#                 mapping[current_word_index].append(idx)\n",
    "\n",
    "        # Randomly mask words\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        labels = feature[\"labels\"]\n",
    "        mask = np.random.binomial(1, wwm_probability, (len(input_ids),))\n",
    "        special_tokens =  [tokenizer.unk_token_id, tokenizer.pad_token_id, tokenizer.cls_token_id, \\\n",
    "                           tokenizer.sep_token_id, tokenizer.mask_token_id]\n",
    "        \n",
    "        new_labels = [-100] * len(labels)\n",
    "        for idx in np.where(mask)[0]:\n",
    "#             word_id = word_id.item()\n",
    "#             print(word_id)\n",
    "#             for idx in mapping[word_id]:\n",
    "#             if word_ids[idx] is not None:\n",
    "            if input_ids[idx] not in special_tokens:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id\n",
    "            feature[\"labels\"] = new_labels\n",
    "        \n",
    "    return default_data_collator(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21576526",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "lm_datasets = lm_datasets.remove_columns([\"word_ids\"])\n",
    "data_collator = bangla_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f973e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [lm_datasets[i] for i in range(1)]\n",
    "# for sample in samples:\n",
    "#     _ = sample.pop(\"word_ids\")\n",
    "\n",
    "for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d9df54",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [lm_datasets[i] for i in range(1)]\n",
    "\n",
    "chunk = data_collator(samples)\n",
    "print(chunk[\"input_ids\"])\n",
    "print(chunk[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d1f636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_size = 10_000\n",
    "# test_size = int(0.1 * train_size)\n",
    "\n",
    "downsampled_dataset = lm_datasets.train_test_split(\n",
    "    train_size=0.8, seed=42\n",
    ")\n",
    "downsampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053ed7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, sample in enumerate(downsampled_dataset[\"train\"][\"input_ids\"][:3]):\n",
    "    print(f\"'>>> Article {idx} length: {len(sample)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00cd613",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  disable weights and biases logging\n",
    "# import os\n",
    "# os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce9a906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 64\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(downsampled_dataset[\"train\"]) // batch_size\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs = 6,\n",
    "    #report_to = None,\n",
    "    output_dir=\"models/wordpiece/bert-base-pretrained-bdnews24\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    #push_to_hub=True,\n",
    "    fp16=True,\n",
    "    logging_steps=logging_steps,\n",
    "    load_best_model_at_end=True,\n",
    "    save_strategy = \"epoch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a65f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=downsampled_dataset[\"train\"],\n",
    "    eval_dataset=downsampled_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adb2b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {eval_results['eval_loss']:.2f}\")\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2e6ec8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0182776e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f815a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5792fcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [optional] finish the wandb run, necessary in notebooks\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af3e354",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMaskedLM.from_pretrained(\"models/wordpiece/bert-base-pretrained-bdnews24\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b380fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"models/wordpiece/bert-base-pretrained-bdnews24\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e2a452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_random_mask(batch):\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    masked_inputs = data_collator(features)\n",
    "    # Create a new \"masked\" column for each column in the dataset\n",
    "    return {\"masked_\" + k: v.numpy() for k, v in masked_inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e539adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsampled_dataset = downsampled_dataset.remove_columns([\"word_ids\"])\n",
    "eval_dataset = downsampled_dataset[\"test\"].map(\n",
    "    insert_random_mask,\n",
    "    batched=True,\n",
    "    remove_columns=downsampled_dataset[\"test\"].column_names,\n",
    ")\n",
    "eval_dataset = eval_dataset.rename_columns(\n",
    "    {\n",
    "        \"masked_input_ids\": \"input_ids\",\n",
    "        \"masked_attention_mask\": \"attention_mask\",\n",
    "        \"masked_labels\": \"labels\",\n",
    "        'masked_token_type_ids' : 'token_type_ids'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a192df0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c95be4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "samples = [eval_dataset[i] for i in range(10)]\n",
    "\n",
    "for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c6c6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, _ in enumerate(eval_dataset['input_ids']):\n",
    "    indexes = [i for i, x in enumerate(eval_dataset[idx]['input_ids']) if x == 4]\n",
    "    references = [i for i, x in enumerate(eval_dataset[idx]['labels']) if x != -100]\n",
    "    if indexes != references:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c615f35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "batch_size = 4\n",
    "train_dataloader = DataLoader(\n",
    "    downsampled_dataset[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, batch_size=batch_size, collate_fn=default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a82f72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83763d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb6f672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertConfig, BertForMaskedLM\n",
    "\n",
    "# # Building the config\n",
    "# config = BertConfig()\n",
    "\n",
    "# # Building the model from the config\n",
    "# # Model is randomly initialized\n",
    "# model = BertForMaskedLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e623c11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"models/wordpiece/bert-base-pretrained-bdnews24\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6116419f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMaskedLM.from_pretrained(\"models/wordpiece/bert-base-pretrained-bdnews24\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb1f318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ab25a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from accelerate import Accelerator\n",
    "\n",
    "# accelerator = Accelerator()\n",
    "# model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "#     model, optimizer, train_dataloader, eval_dataloader\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccb64db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 1\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b4a2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"models/wordpiece/bert-base-pretrained-bdnews24-static\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504ad160",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import math\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        #accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        #losses.append(accelerator.gather(loss.repeat(batch_size)))\n",
    "        losses.append(loss.repeat(batch_size))\n",
    "\n",
    "    losses = torch.cat(losses)\n",
    "    losses = losses[: len(eval_dataset)]\n",
    "    try:\n",
    "        perplexity = math.exp(torch.mean(losses))\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "\n",
    "    print(f\">>> Epoch {epoch}: Loss: {torch.mean(losses)}\")\n",
    "    print(f\">>> Epoch {epoch}: Perplexity: {perplexity}\")\n",
    "\n",
    "    # Save and upload\n",
    "    #accelerator.wait_for_everyone()\n",
    "    #unwrapped_model = accelerator.unwrap_model(model)\n",
    "    #unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    #if accelerator.is_main_process:\n",
    "        #tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10aab00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
