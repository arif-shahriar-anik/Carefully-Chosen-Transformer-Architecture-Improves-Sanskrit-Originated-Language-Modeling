{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "705888d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/arifa/.cache/huggingface/datasets/csv/default-288983945b809be3/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "train_dataset = load_dataset(\"csv\", data_files=\"..\\datasets\\soham-articles\\\\bn\\\\bn-train.csv\", column_names=[\"labels\",\"text\"], split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7484c2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/arifa/.cache/huggingface/datasets/csv/default-934cec49a13b53ff/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    }
   ],
   "source": [
    "valid_dataset = load_dataset(\"csv\", data_files=\"..\\datasets\\soham-articles\\\\bn\\\\bn-valid.csv\", column_names=[\"labels\",\"text\"], split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b105db92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/arifa/.cache/huggingface/datasets/csv/default-04dc69dd625404f0/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    }
   ],
   "source": [
    "test_dataset = load_dataset(\"csv\", data_files=\"..\\datasets\\soham-articles\\\\bn\\\\bn-test.csv\", column_names=[\"labels\",\"text\"], split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "971c96bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "news_datasets = DatasetDict()\n",
    "news_datasets['train'] = train_dataset\n",
    "news_datasets['test'] = test_dataset\n",
    "news_datasets['validation'] = valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e6758cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'text'],\n",
       "        num_rows: 11284\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'text'],\n",
       "        num_rows: 1411\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['labels', 'text'],\n",
       "        num_rows: 1411\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62c44db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_format(\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7386598c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get label counts for both classes\n",
    "label_counts = train_dataset[\"labels\"].value_counts()\n",
    "num_labels = (len(label_counts.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91fd91ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels\n",
       "kolkata          4603\n",
       "state            2245\n",
       "national         1435\n",
       "sports           1289\n",
       "entertainment    1186\n",
       "international     526\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfcd1891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13776"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_token_length = max(train_dataset['text'].str.len())\n",
    "max_token_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa3c3367",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = train_dataset['text'].str.split().apply(len).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee37ab33",
   "metadata": {},
   "outputs": [],
   "source": [
    "count.index = count.index.astype(str) + ' words:'\n",
    "count.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6913c08e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text\n",
       "100 words:     28\n",
       "1002 words:     2\n",
       "101 words:     20\n",
       "1013 words:     1\n",
       "1017 words:     1\n",
       "               ..\n",
       "98 words:      25\n",
       "987 words:      1\n",
       "99 words:      19\n",
       "992 words:      1\n",
       "999 words:      1\n",
       "Name: count, Length: 771, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9848d245",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.reset_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "698c93f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "\n",
    "set_seed(30)\n",
    "# set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd3efaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertConfig, CharacterBertModel, CharacterBertTokenizer\n",
    "\n",
    "#### LOADING BERT FOR CLASSIFICATION ####\n",
    "\n",
    "config = BertConfig.from_pretrained('bert-base-uncased', num_labels=num_labels)  # binary classification\n",
    "model = BertForSequenceClassification(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a993c30f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30522, 768, padding_idx=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bert.embeddings.word_embeddings  # wordpiece embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4302d222",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at E:\\Documents\\Character Bert\\Question Classification\\character-bert were not used when initializing CharacterBertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing CharacterBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CharacterBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#### REPLACING BERT WITH CHARACTER_BERT ####\n",
    "\n",
    "character_bert_model = CharacterBertModel.from_pretrained(\n",
    "    \"E:\\Documents\\Character Bert\\Question Classification\\character-bert\")\n",
    "model.bert = character_bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67f6ac76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharacterCnn(\n",
       "  (char_conv_0): Conv1d(16, 32, kernel_size=(1,), stride=(1,))\n",
       "  (char_conv_1): Conv1d(16, 32, kernel_size=(2,), stride=(1,))\n",
       "  (char_conv_2): Conv1d(16, 64, kernel_size=(3,), stride=(1,))\n",
       "  (char_conv_3): Conv1d(16, 128, kernel_size=(4,), stride=(1,))\n",
       "  (char_conv_4): Conv1d(16, 256, kernel_size=(5,), stride=(1,))\n",
       "  (char_conv_5): Conv1d(16, 512, kernel_size=(6,), stride=(1,))\n",
       "  (char_conv_6): Conv1d(16, 1024, kernel_size=(7,), stride=(1,))\n",
       "  (_highways): Highway(\n",
       "    (_layers): ModuleList(\n",
       "      (0-1): 2 x Linear(in_features=2048, out_features=4096, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (_projection): Linear(in_features=2048, out_features=768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bert.embeddings.word_embeddings  # wordpieces are replaced with a CharacterCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a454b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# # Load the actual checkpoint file\n",
    "# checkpoint = torch.load(\n",
    "#     output_directory, map_location=\"cpu\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed1cf601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(checkpoint['model'], strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9a701fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CharacterBertTokenizer(strip_accents=None, do_lower_case=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b833509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38859875",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\arifa\\.cache\\huggingface\\datasets\\csv\\default-288983945b809be3\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-e98e8e56455d1237.arrow\n",
      "Loading cached processed dataset at C:\\Users\\arifa\\.cache\\huggingface\\datasets\\csv\\default-04dc69dd625404f0\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-b118f805ce8ac4b0.arrow\n",
      "Loading cached processed dataset at C:\\Users\\arifa\\.cache\\huggingface\\datasets\\csv\\default-934cec49a13b53ff\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-e8a6e6f18f150f3b.arrow\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "tokenized_datasets = news_datasets.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90c5c425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 11284\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1411\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1411\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5648aa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = tokenized_datasets.filter(lambda x:x if tokenizer.unk_token_id in x[\"input_ids\"] else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fedfc6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fbc48ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sample in temp[\"train\"]:\n",
    "#     print(tokenizer.decode(sample[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e711f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"kolkata\":4603,\n",
    "# \"state\":2245,\n",
    "# \"national\":1435,\n",
    "# \"sports\":1289,\n",
    "# \"entertainment\":1186,\n",
    "# \"international\":526"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b77cb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_label(example):\n",
    "    mapping = {\n",
    "        \"kolkata\":0,\n",
    "        \"state\":1,\n",
    "        \"national\":2,\n",
    "        \"sports\":3,\n",
    "        \"entertainment\":4,\n",
    "        \"international\":5\n",
    "    }\n",
    "    example['labels'] = mapping[example['labels']]\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f72f8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\arifa\\.cache\\huggingface\\datasets\\csv\\default-288983945b809be3\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-07499d1639bcd994.arrow\n",
      "Loading cached processed dataset at C:\\Users\\arifa\\.cache\\huggingface\\datasets\\csv\\default-04dc69dd625404f0\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-8fd5c59463e59219.arrow\n",
      "Loading cached processed dataset at C:\\Users\\arifa\\.cache\\huggingface\\datasets\\csv\\default-934cec49a13b53ff\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-ac1979357c97e3dd.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       " 'test': ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       " 'validation': ['labels', 'input_ids', 'token_type_ids', 'attention_mask']}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets.map(assign_label)\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "399bf492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] ржкрзНрж░ржЬрж╛ржкрждрж┐ ржкрзНрж░ржЬрж╛ржкрждрж┐ ржЖржорж╛рж░ ржЗржЪрзНржЫрзЗ рж╣рзЯрзЗ, ржмржирзЗ ржмржирзЗ ржШрж╛рж╕рзЗ ржШрж╛рж╕рзЗ ржУржбрж╝рзЗ ржЖрж░ ржлрзЗрж░рзЗ... рж╕рзЗржЗ рж╕рзБрж░ ржпрж╛ ржлрж┐рж░рж┐рзЯрзЗ ржжрзЗрзЯ рж╢рзИрж╢ржмрзЗрж░ рж╕рзБржЧржирзНржз ред рж╕ржмрж┐рждрж╛ ржЪрзМржзрзБрж░рзА ред ржпрж╛ржБрж░ ржХржгрзНржа ржЕржкрзВрж░рзНржм рж╕ржм ржЧрж╛ржиржЧрзБржЪрзНржЫ ржЙржкрж╣рж╛рж░ ржжрж┐рзЯрзЗржЫрзЗ рж╕ржм ржкрзНрж░ржЬржирзНржорзЗрж░ ржЧрж╛ржиржкрзНрж░рзЗржорзАржжрзЗрж░ ред ржжрзБрж░рж╛рж░рзЛржЧрзНржп ржХрж░рзНржХржЯрж░рзЛржЧрзЗ ржЖржХрзНрж░рж╛ржирзНржд рж╕рзЗржжрж┐ржирзЗрж░ рж╕ржмрж┐рждрж╛ ред ржЧржд ржХрзЯрзЗржХ ржмржЫрж░ ржзрж░рзЗ ржирж┐рж░ржирзНрждрж░ рж▓ржбрж╝рж╛ржЗрзЯрзЗрж░ ржкрж░ ржПржЦржи ржкрзНрж░рж╣рж░ ржЧрзБржиржЫрзЗржи ред ржпрзЗржХрзЛржиржУ ржжрж┐ржиржЗ ржЖрж╕рждрзЗ ржкрж╛рж░рзЗ рж╕рзЗржЗ ржнрзЯржЩрзНржХрж░ ржжрж┐ржиржЯрж┐ ред ржПржЦржи рждрж╛ржБрж░ ржмржбрж╝ ржорзЗрзЯрзЗ ржЕржирзНрждрж░рж╛ ржЪрзМржзрзБрж░рзАрж░ ржХрж╛ржЫрзЗржЗ ржЖржЫрзЗржи рждрж┐ржирж┐ ред рж░рзБржмрж┐ рж╣рж╛рж╕ржкрж╛рждрж╛рж▓рзЗрж░ ржЦрзБржм ржХрж╛ржЫрзЗржЗ ржмржбрж╝ ржорзЗрзЯрзЗрж░ ржХрж╛ржЫрзЗржЗ ржХрж╛ржЯрж╛рждрзЗ ржЪрж╛ржи ржЬрзАржмржирзЗрж░ рж╢рзЗрж╖ ржХрзЯрзЗржХржЯрж┐ ржжрж┐ржи ред ржЖржЬржЗ ржПржмрзЗрж▓рж╛. ржЗржи ржХрзЗ ржЬрж╛ржирж╛рж▓рзЗржи ржЕржирзНрждрж░рж╛ ред рж╕рзНржмрж╛ржорзА рж╕рж▓рж┐рж▓ ржЪрзМржзрзБрж░рзАрж░ ржЕржирзБрж░рзЛржзрзЗржЗ ржкрзНрж░ржержо ржЕрзНржпрж╛рж▓ржмрж╛ржорзЗ ржЧрж╛ржУрзЯрж╛ ржЧрж╛ржи ржорж░рж┐ рж╣рж╛рзЯ ржЧрзЛ рж╣рж╛рзЯ ред рж╕ржмрж┐рждрж╛рж░ ржХржгрзНржарж╕рзНржмрж░рзЗрж░ ржЬрж╛ржжрзБрждрзЗ рж╕рзЗржЗ ржЧрж╛ржи рж╢рж╛рж╢рзНржмржд рж╣рзЯрзЗ ржЖржЫрзЗ ред ржПржЦржи ржПржЗ ржЖржиржирзНржжрзЗрж░ рж╢рж╣рж░рзЗ ржПржХрж▓рж╛ рж╢рзБрзЯрзЗ рж╕ржмрж┐рждрж╛ ред рж╕рзБрж░рзЗрж░ [SEP]'\n",
      "\n",
      "'>>> [CLS] ржжрзАрж░рзНржШ рж╕рж╛ржд ржорж╛рж╕рзЗрж░ ржкрзНрж░рждрзАржХрзНрж╖рж╛рж░ ржкрж░ рж╕рзЛржоржмрж╛рж░ рж╕ржХрж╛рж▓рзЗ ржПрж╕рзЗ ржкрзМржБржЫрж▓ ржХрж▓ржХрж╛рждрж╛ ржорзЗржЯрзНрж░рзЛ рж░рзЗрж▓рзЗрж░ ржирждрзБржи ржПрж╕рж┐ рж░рзЗржХ ред ржЧржд ржбрж┐рж╕рзЗржорзНржмрж░рзЗ ржУржЗ рж░рзЗржХржЯрж┐ ржПрж╕рзЗ ржкрзМржБржЫржирзЛрж░ ржХржерж╛ ржерж╛ржХрж▓рзЗржУ ржПржХрж╛ржзрж┐ржХ ржХрж╛рж░ржгрзЗ рж░рзЗржХ ржкрзЗрждрзЗ ржЕржирзЗржХржЯрж╛ ржжрзЗрж░рж┐ рж╣рж▓ ржмрж▓рзЗ ржорзЗржЯрзНрж░рзЛ рж╕рзВрждрзНрж░рзЗрж░ ржЦржмрж░ ред ржЪрзЗржирзНржирж╛ржЗрзЯрзЗрж░ ржЗржирзНржЯрж┐ржЧрзНрж░рж╛рж▓ ржХрзЛржЪ ржлрзНржпрж╛ржХрзНржЯрж░рж┐ржХрзЗ ( ржЖржЗрж╕рж┐ржПржл ) ржжрзБ тАЩ ржЯрж┐ ржПрж╕рж┐ рж░рзЗржХрзЗрж░ ржмрж░рж╛ржд ржжрзЗржУрзЯрж╛ рж╣рзЯрзЗржЫрж┐рж▓ ред рждрж╛рж░ ржоржзрзНржпрзЗ ржкрзНрж░ржержо рж░рзЗржХржЯрж┐ ржПржжрж┐ржи ржПрж╕рзЗ ржкрзМржБржЫрзЗржЫрзЗ ред ржжрзНржмрж┐рждрзАрзЯ рж░рзЗржХржЯрж┐ ржкрзМржБржЫржирзЛрж░ ржХржерж╛ ржЕржЧрж╕рзНржЯрзЗрж░ рждрзГрждрзАрзЯ рж╕ржкрзНрждрж╛рж╣рзЗ ред ржПржЦржиржХрж╛рж░ ржПрж╕рж┐ рж░рзЗржХржЧрзБрж▓рж┐рж░ рждрзБрж▓ржирж╛рзЯ ржирждрзБржи ржПрж╕рж┐ рж░рзЗржХ ржкрзНрж░ржпрзБржХрзНрждрж┐ржЧрждржнрж╛ржмрзЗ ржЕржирзЗржХржЯрж╛ржЗ ржПржЧрж┐рзЯрзЗ ржмрж▓рзЗ ред ржЬрж╛ржирж╛ ржЧрж┐рзЯрзЗржЫрзЗ, ржЪрж▓рждрж┐ ржмржЫрж░рзЗрж░ рж╢рзЗрж╖рзЗ ржЪрж┐ржирзЗрж░ ржбрж╛рж▓рж┐рзЯрж╛ржи ржерзЗржХрзЗ ржорзЗржЯрзНрж░рзЛрж░ рж░рзЗржХ ржЖрж╕рж╛ рж╢рзБрж░рзБ рж╣ржмрзЗ ред рж╕ржорзНржкрзВрж░рзНржг ржжрзЗрж╢рзАрзЯ ржкрзНрж░ржпрзБржХрзНрждрж┐рждрзЗ рждрзИрж░рж┐ ржЖржЗрж╕рж┐ржПржлрзЗрж░ рж░рзЗржХржЧрзБрж▓рж┐ ржпрж╛рждрзЗ ржХрзЛржиржУржнрж╛ржмрзЗ ржУржЗ рж░рзЗржХрзЗрж░ рждрзБрж▓ржирж╛рзЯ ржкрж┐ржЫрж┐рзЯрзЗ ржирж╛ ржерж╛ржХрзЗ рждрж╛ ржирж┐рж╢рзНржЪрж┐ржд ржХрж░рж╛рж░ ржЪрзЗрж╖рзНржЯрж╛ рж╣рзЯрзЗржЫрзЗ ред ржирждрзБржи рж░рзЗржХрзЗрж░ ржорзЛржЯрж░ - рж╕рж╣ ржорзВрж▓ ржЕржВрж╢ ( ржкрзНрж░ржкрж╛рж▓рж╕ржи ) рждрзИрж░рж┐ [SEP]'\n",
      "\n",
      "'>>> [CLS] ржжрж┐ржШрж╛рж░ рж╣рзЛржЯрзЗрж▓ ржерзЗржХрзЗ ржЙржжрзНржзрж╛рж░ рж╣рж▓ ржпрзБржмржХ - ржпрзБржмрждрзАрж░ ржЭрзБрж▓ржирзНржд ржжрзЗрж╣ ред рж╢рзБржХрзНрж░ржмрж╛рж░ рж╕ржХрж╛рж▓рзЗ ржирж┐ржЙ ржжрж┐ржШрж╛рж░ ржПржХржЯрж┐ рж╣рзЛржЯрзЗрж▓ ржерзЗржХрзЗ ржУржЗ ржпрзБржЧрж▓рзЗрж░ ржжрзЗрж╣ ржЙржжрзНржзрж╛рж░ рж╣рзЯ ред ржорзГрждржжрзЗрж░ ржжрж╛ржо ржЧрзЛржкрж╛рж▓ рж╕рж░ржХрж╛рж░ ржУ ржЕрждрж╕рж┐ рж╕рж░ржХрж╛рж░ ред рж╣рзЛржЯрзЗрж▓ ржХрж░рзНрждрзГржкржХрзНрж╖рзЗрж░ ржХрж╛ржЫрзЗ рждрж╛ржБрж░рж╛ ржПржЗ ржирж╛ржо ржжрж┐рзЯрзЗржЗ рж░рзБржо ржмрзБржХ ржХрж░рзЗржЫрж┐рж▓рзЗржи ржмрж▓рзЗ ржЬрж╛ржирж╛ ржЧрж┐рзЯрзЗржЫрзЗ ред ржПржЗ ржмрж┐рж╖рзЯрзЗ ржЕржирзНржпрж╛ржирзНржп ржЦржмрж░ ржлрзБржЯржлрзБржЯрзЗ ржмрж╛ржЪрзНржЪрж╛ржЯрж┐ ржХрж╛рж░! ржжрж┐ржШрж╛рж░ рж╕рзИржХрждрзЗ рж╢рж┐рж╢рзБрж░ ржХрж╛ржирзНржирж╛, ржЬрж▓рзНржкржирж╛ ржЪрж╛ржкрж╛ ржжрж┐рж▓ рж╕ржорзБржжрзНрж░рзЗрж░ ржЧрж░рзНржЬржи ржжрж┐ржШрж╛рзЯ ржорж╛рж░рж╛рждрзНржоржХ ржЕржнрж┐ржпрзЛржЧ! ржорж╣рж┐рж▓рж╛ ржкрж░рзНржпржЯржХржХрзЗ ржкрзНрж░рж╛рзЯ ржмрж┐ржмрж╕рзНрждрзНрж░ ржХрж░рзЗ рж╣рзЗржирж╕рзНржерж╛ ржЬрж╛ржирж╛ ржЧрж┐рзЯрзЗржЫрзЗ, рж╕рзНржмрж╛ржорзА - рж╕рзНрждрзНрж░рзАрж░ ржкрж░рж┐ржЪрзЯ ржжрж┐рзЯрзЗ ржУржЗ ржпрзБржЧрж▓ ржЧржд рззрзл рждрж╛рж░рж┐ржЦ рж╣рзЛржЯрзЗрж▓рзЗ ржУржарзЗржи ред рж╣рзЛржЯрзЗрж▓рзЗрж░ рж░рзЗржЬрж┐рж╕рзНржЯрж╛рж░рзЗ рж╣рзБржЧрж▓рж┐рж░ рждрж╛рж░ржХрзЗрж╢рзНржмрж░рзЗрж░ ржорзБржХрзНрждрж╛рж░ржкрзБрж░рзЗрж░ ржЧрзНрж░рж╛ржорзЗрж░ ржмрж╛рж╕рж┐ржирзНржжрж╛ ржмрж▓рзЗржи рждрж╛ржБрж░рж╛ ред рж╢рзБржХрзНрж░ржмрж╛рж░ржЗ рждрж╛ржБржжрзЗрж░ ржлрж┐рж░рзЗ ржпрж╛ржУрзЯрж╛рж░ ржХржерж╛ ржЫрж┐рж▓ ред ржПржжрж┐ржи рж╕ржХрж╛рж▓рзЗ рж░рзБржорзЗрж░ ржоржзрзНржпрзЗ ржерзЗржХрзЗ ржпрзБржмржХ - ржпрзБржмрждрзАрж░ ржХрзЛржиржУ рж╕рж╛ржбрж╝рж╛рж╢ржмрзНржж ржирж╛ ржкрж╛ржУрзЯрж╛рзЯ рж╕ржирзНржжрзЗрж╣ рж╣рзЯ рж╣рзЛржЯрзЗрж▓ржХрж░рзНржорзАржжрзЗрж░ ржоржзрзНржпрзЗ ред ржкрж░рзЗ ржжрж░ржЬрж╛ [SEP]'\n",
      "\n",
      "'>>> [CLS] ржХрж┐ржЫрзБржжрж┐ржи ржЖржЧрзЗржЗ рж░рж╛ржЬрзНржпрзЗрж░ ржЧрж╛ржЗржб ржмржирзЗ ржЧрж┐рзЯрзЗржЫрж┐рж▓рзЗржи ржЪрзЗрждрзЗрж╢рзНржмрж░ ржкрзВржЬрж╛рж░рж╛ ред рж░рж╛ржЬржХрзЛржЯрзЗ ржнрж╛рж░ржд - ржУрзЯрзЗрж╕рзНржЯ ржЗржирзНржбрж┐ржЬ ржкрзНрж░ржержо ржЯрзЗрж╕рзНржЯ ржЪрж▓рж╛ржХрж╛рж▓рзАржиржЗ ржкрзВржЬрж╛рж░рж╛ржХрзЗ ржжрзЗржЦрж╛ ржЧрж┐рзЯрзЗржЫрж┐рж▓ ржирзЯрж╛ ржЕржмрждрж╛рж░рзЗ ред ржПржмрж╛рж░ рж╕рзЗржЗ ржкржерзЗржЗ рж╣рж╛ржБржЯрж▓рзЗржи ржорж╣ржорзНржоржж рж╕рж┐рж░рж╛ржЬ ред ржПржЗ ржмрж┐рж╖рзЯрзЗ ржЕржирзНржпрж╛ржирзНржп ржЦржмрж░ ржХрзБржорзНржмрж▓рзЗрж░ ржЧрзБржЧрж▓рж┐рждрзЗ тАШ ржмрзЛрж▓рзНржб тАЩ рж╕рзЛрж╣рж┐ржирзА, ржорж╛ржЭ ржЖржХрж╛рж╢рзЗ ржоржи ржЬрж┐рждрж▓рзЗржи ржХрзНрж░рж┐ржХрзЗржЯрж╛рж░ рж╣рж╛рзЯржжрж░рж╛ржмрж╛ржжрзЗ ржжрзНржмрж┐рждрзАрзЯ ржЯрзЗрж╕рзНржЯ рж╢рзБрж░рзБ рж╣ржЪрзНржЫрзЗ рж╢рзБржХрзНрж░ржмрж╛рж░рзЗржЗ ред рж╕рзЗржЗ ржЯрзЗрж╕рзНржЯ рж╢рзБрж░рзБрж░ ржЖржЧрзЗржЗ рж╣рж╛рзЯржжрж░рж╛ржмрж╛ржжрзЗрж░ ржЯрзНржпрзБрж░ ржЧрж╛ржЗржб рж╣рзЯрзЗ ржЧрзЗрж▓рзЗржи рж╕рж┐рж░рж╛ржЬ ред рж╣рж╛рзЯржжрж░рж╛ржмрж╛ржжрзЗрж░ ржжрзНрж░рж╖рзНржЯржмрзНржп ржХрзА? рж╕рж┐рж░рж╛ржЬ ржЬрж╛ржирж╛ржЪрзНржЫрзЗржи, ржЪрж╛рж░ржорж┐ржирж╛рж░рзЗ ржпрзЗрждрзЗржЗ рж╣ржмрзЗ ред ржУржЦрж╛ржи ржерзЗржХрзЗ ржЪрж╛ржБржж ржжрзЗржЦрж╛рж░ ржЕржнрж┐ржЬрзНржЮрждрж╛ ржжрзБрж░рзНржзрж░рзНрж╖! ржкрж╛рж╢рж╛ржкрж╛рж╢рж┐ рждрж┐ржирж┐ ржирзЗржХрж▓рзЗрж╕ рж░рзЛржбрзЗржУ ржШрзБрж░рзЗ ржЖрж╕рж╛рж░ ржкрж░рж╛ржорж░рзНрж╢ ржжрж┐рзЯрзЗржЫрзЗржи ржнрзНрж░ржоржгрж╛рж░рзНржерзАржжрзЗрж░ ред рж░рж┐рж▓рзНржпрж╛ржХрзНрж╕ ржХрж░рж╛рж░ ржЬржирзНржп ржирж╛ржХрж┐ ржирзЗржХрж▓рзЗрж╕ рж░рзЛржб ржЖржжрж░рзНрж╢ ред ржЯрзНржпрзБрж░рж┐рж╕рзНржЯржжрзЗрж░ рж╣рж╛рзЯржжрж░рж╛ржмрж╛ржжрзЗрж░ ржмрж┐ржЦрзНржпрж╛ржд ржмрж┐рж░рж┐рзЯрж╛ржирж┐ ржЪрзЗржЦрзЗ ржжрзЗржУрзЯрж╛рж░ ржХржерж╛ ржмрж▓рзЗржЫрзЗржи рждрж╛рж░ржХрж╛ ржкрзЗрж╕рж╛рж░ ред рж╕рж┐рж░рж╛ржЬрзЗрж░ ржПржЗ тАШ ржЧрж╛ржЗржб тАЩ рж╣ржУрзЯрж╛рж░ ржнрж┐ржбрж┐ржУ ржмрж┐рж╕рж┐рж╕рж┐ржЖржЗ рж╢рзЗрзЯрж╛рж░ ржХрж░рзЗржЫрзЗржи ржирж┐ржЬрзЗржжрзЗрж░ ржлрзЗрж╕ржмрзБржХ ржкрзЗржЬрзЗ ред ржЗрждрж┐ржоржзрзНржпрзЗржЗ [SEP]'\n",
      "\n",
      "'>>> [CLS] ржорж╛рж░рзБрждрж┐рж░ ржХрж╛рж░ржЦрж╛ржирж╛рзЯ рж╣рж╛ржорж▓рж╛ ржПржмржВ ржорзНржпрж╛ржирзЗржЬрж╛рж░ржХрзЗ ржкрзБржбрж╝рж┐рзЯрзЗ ржорж╛рж░рж╛рж░ ржШржЯржирж╛рзЯ рзйрзз ржЬржиржХрзЗ ржжрзЛрж╖рзА рж╕рж╛ржмрзНржпрж╕рзНржд ржХрж░рж▓ рж╣рж░рж┐рзЯрж╛ржирж╛рж░ ржЖржжрж╛рж▓ржд ред ржжрзЛрж╖рзАрж░рж╛ ржкрзНрж░рждрзНржпрзЗржХрзЗржЗ ржорж╛ржирзЗрж╕рж░рзЗрж░ ржорж╛рж░рзБрждрж┐ - рж╕рзБржЬрзБржХрж┐ ржкрзНрж▓рзНржпрж╛ржирзНржЯрзЗрж░ ржХрж░рзНржорзА ред ржжрж┐рж▓рзНрж▓рж┐рж░ ржкрзНрж░рж╛рзЯ рзкрзж ржХрж┐рж▓рзЛржорж┐ржЯрж╛рж░ ржжржХрзНрж╖рж┐ржгрзЗ ржЕржмрж╕рзНржерж┐ржд ржУржЗ ржкрзНрж▓рзНржпрж╛ржирзНржЯрзЗ рзирзжрззрзи рж╕рж╛рж▓рзЗ ржПржХ ржжрж▓ рж╢рзНрж░ржорж┐ржХрзЗрж░ рж╣рж╛рждрзЗ ржЖржХрзНрж░рж╛ржирзНржд рж╣рзЯрзЗржЫрж┐рж▓рзЗржи рж╕ржВрж╕рзНржерж╛рж░ ржЙржЪрзНржЪржкржжрж╕рзНрже ржХрж░рзНрждрж╛рж░рж╛ ред ржХржиржлрж╛рж░рзЗржирзНрж╕ рж░рзБржорзЗрж░ ржоржзрзНржпрзЗ рж╣рж┐ржЙржорзНржпрж╛ржи рж░рж┐рж╕рзЛрж░рзНрж╕ ржорзНржпрж╛ржирзЗржЬрж╛рж░ ржЕржмржирзАрж╢ ржХрзБржорж╛рж░ ржжрзЗржмрзЗрж░ ржЧрж╛рзЯрзЗ ржЖржЧрзБржи ржзрж░рж┐рзЯрзЗ ржжрзЗржУрзЯрж╛ рж╣рзЯрзЗржЫрж┐рж▓ ред рж╕рзЗржЦрж╛ржирзЗржЗ ржорзГрждрзНржпрзБ рж╣рзЯрзЗржЫрж┐рж▓ рждрж╛ржБрж░ ред ржжрзАрж░рзНржШ ржХрзНрж╖ржг ржзрж░рзЗ ржЪрж▓рждрзЗ ржерж╛ржХрж╛ рждрж╛ржгрзНржбржмрзЗ ржорзНржпрж╛ржирзЗржЬржорзЗржирзНржЯрзЗрж░ ржЕржирзНрждржд рзлрзж ржЬржи ржХрж░рзНрждрж╛ ржЬржЦржо рж╣рзЯрзЗржЫрж┐рж▓рзЗржи, ржЖржХрзНрж░рж╛ржирзНржд рж╣рзЯрзЗржЫрж┐рж▓ ржкрзБрж▓рж┐рж╢ржУ ред ржШржЯржирж╛рж░ ржмржЫрж░ ржЪрж╛рж░рзЗржХ ржкрж░ ржЧрзБрж░рзБржЧрзНрж░рж╛ржорзЗрж░ ржЖржжрж╛рж▓ржд ржорж╛ржорж▓рж╛ржЯрж┐рж░ рж░рж╛рзЯ ржШрзЛрж╖ржгрж╛ ржХрж░рж▓ ред ржорж╛рж░рзБрждрж┐ рж╕ржВрж╕рзНржерж╛ ржкрзНрж░рждрж┐ ржмржЫрж░ ржпрзЗ ржкрж░рж┐ржорж╛ржг ржЧрж╛ржбрж╝рж┐ рждрзИрж░рж┐ ржХрж░рзЗ, рждрж╛рж░ ржПржХ рждрзГрждрзАрзЯрж╛ржВрж╢ржЗ рждрзИрж░рж┐ рж╣рзЯ ржорж╛ржирзЗрж╕рж░рзЗрж░ ржкрзНрж▓рзНржпрж╛ржирзНржЯрзЗ ред рзирзжрззрзи рж╕рж╛рж▓рзЗрж░ ржмрзЗржиржЬрж┐рж░ ржЧрзЛрж▓ржорж╛рж▓рзЗрж░ ржкрж░ рж╕рзЗржЗ ржорж╛ржирзЗрж╕рж░ ржкрзНрж▓рзНржпрж╛ржирзНржЯ ржорж╛рж╕ ржЦрж╛ржирзЗржХрзЗрж░ ржЬржирзНржп ржмржирзНржз ржХрж░рзЗ ржжрж┐рзЯрзЗржЫрж┐рж▓ [SEP]'\n"
     ]
    }
   ],
   "source": [
    "samples = [tokenized_datasets[\"train\"][i] for i in range(5)]\n",
    "samples\n",
    "\n",
    "for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "75eca670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': ['entertainment', 'state', 'state', 'sports', 'national'],\n",
       " 'text': ['ржкрзНрж░ржЬрж╛ржкрждрж┐ ржкрзНрж░ржЬрж╛ржкрждрж┐ ржЖржорж╛рж░ ржЗржЪрзНржЫрзЗ рж╣рзЯрзЗ, ржмржирзЗ ржмржирзЗ ржШрж╛рж╕рзЗ ржШрж╛рж╕рзЗ ржУржбрж╝рзЗ ржЖрж░ ржлрзЗрж░рзЗ...\\nрж╕рзЗржЗ рж╕рзБрж░ ржпрж╛ ржлрж┐рж░рж┐рзЯрзЗ ржжрзЗрзЯ рж╢рзИрж╢ржмрзЗрж░ рж╕рзБржЧржирзНржзред рж╕ржмрж┐рждрж╛ ржЪрзМржзрзБрж░рзАред ржпрж╛ржБрж░ ржХржгрзНржа ржЕржкрзВрж░рзНржм рж╕ржм ржЧрж╛ржиржЧрзБржЪрзНржЫ ржЙржкрж╣рж╛рж░ ржжрж┐рзЯрзЗржЫрзЗ рж╕ржм ржкрзНрж░ржЬржирзНржорзЗрж░ ржЧрж╛ржиржкрзНрж░рзЗржорзАржжрзЗрж░ред ржжрзБрж░рж╛рж░рзЛржЧрзНржп ржХрж░рзНржХржЯрж░рзЛржЧрзЗ ржЖржХрзНрж░рж╛ржирзНржд рж╕рзЗржжрж┐ржирзЗрж░ рж╕ржмрж┐рждрж╛ред ржЧржд ржХрзЯрзЗржХ ржмржЫрж░ ржзрж░рзЗ ржирж┐рж░ржирзНрждрж░ рж▓ржбрж╝рж╛ржЗрзЯрзЗрж░ ржкрж░ ржПржЦржи ржкрзНрж░рж╣рж░ ржЧрзБржиржЫрзЗржи ред ржпрзЗржХрзЛржиржУ ржжрж┐ржиржЗ ржЖрж╕рждрзЗ ржкрж╛рж░рзЗ рж╕рзЗржЗ ржнрзЯржЩрзНржХрж░ ржжрж┐ржиржЯрж┐ред\\nржПржЦржи рждрж╛ржБрж░ ржмржбрж╝ ржорзЗрзЯрзЗ ржЕржирзНрждрж░рж╛ ржЪрзМржзрзБрж░рзАрж░ ржХрж╛ржЫрзЗржЗ ржЖржЫрзЗржи рждрж┐ржирж┐ред рж░рзБржмрж┐ рж╣рж╛рж╕ржкрж╛рждрж╛рж▓рзЗрж░ ржЦрзБржм ржХрж╛ржЫрзЗржЗ ржмржбрж╝ ржорзЗрзЯрзЗрж░ ржХрж╛ржЫрзЗржЗ ржХрж╛ржЯрж╛рждрзЗ ржЪрж╛ржи ржЬрзАржмржирзЗрж░ рж╢рзЗрж╖ ржХрзЯрзЗржХржЯрж┐ ржжрж┐ржиред ржЖржЬржЗ ржПржмрзЗрж▓рж╛.ржЗржи ржХрзЗ ржЬрж╛ржирж╛рж▓рзЗржи ржЕржирзНрждрж░рж╛ред\\nрж╕рзНржмрж╛ржорзА рж╕рж▓рж┐рж▓ ржЪрзМржзрзБрж░рзАрж░ ржЕржирзБрж░рзЛржзрзЗржЗ ржкрзНрж░ржержо ржЕрзНржпрж╛рж▓ржмрж╛ржорзЗ ржЧрж╛ржУрзЯрж╛ ржЧрж╛ржи ржорж░рж┐ рж╣рж╛рзЯ ржЧрзЛ рж╣рж╛рзЯред рж╕ржмрж┐рждрж╛рж░ ржХржгрзНржарж╕рзНржмрж░рзЗрж░ ржЬрж╛ржжрзБрждрзЗ рж╕рзЗржЗ ржЧрж╛ржи рж╢рж╛рж╢рзНржмржд рж╣рзЯрзЗ ржЖржЫрзЗред\\xa0\\nржПржЦржи ржПржЗ ржЖржиржирзНржжрзЗрж░ рж╢рж╣рж░рзЗ ржПржХрж▓рж╛ рж╢рзБрзЯрзЗ рж╕ржмрж┐рждрж╛ред рж╕рзБрж░рзЗрж░ рж╕рзЗржЗ ржЭрж░-ржЭрж░-ржЭрж░ржгрж╛ ржпрзЗ ржЖржЬ ржирж┐рж░рзБржЪрзНржЪрж╛рж░',\n",
       "  'ржжрзАрж░рзНржШ рж╕рж╛ржд ржорж╛рж╕рзЗрж░ ржкрзНрж░рждрзАржХрзНрж╖рж╛рж░ ржкрж░ рж╕рзЛржоржмрж╛рж░ рж╕ржХрж╛рж▓рзЗ ржПрж╕рзЗ ржкрзМржБржЫрж▓ ржХрж▓ржХрж╛рждрж╛ ржорзЗржЯрзНрж░рзЛ рж░рзЗрж▓рзЗрж░ ржирждрзБржи ржПрж╕рж┐ рж░рзЗржХред ржЧржд ржбрж┐рж╕рзЗржорзНржмрж░рзЗ ржУржЗ рж░рзЗржХржЯрж┐ ржПрж╕рзЗ ржкрзМржБржЫржирзЛрж░ ржХржерж╛ ржерж╛ржХрж▓рзЗржУ ржПржХрж╛ржзрж┐ржХ ржХрж╛рж░ржгрзЗ рж░рзЗржХ ржкрзЗрждрзЗ ржЕржирзЗржХржЯрж╛ ржжрзЗрж░рж┐ рж╣рж▓ ржмрж▓рзЗ ржорзЗржЯрзНрж░рзЛ рж╕рзВрждрзНрж░рзЗрж░ ржЦржмрж░ред\\xa0\\nржЪрзЗржирзНржирж╛ржЗрзЯрзЗрж░ ржЗржирзНржЯрж┐ржЧрзНрж░рж╛рж▓ ржХрзЛржЪ ржлрзНржпрж╛ржХрзНржЯрж░рж┐ржХрзЗ (ржЖржЗрж╕рж┐ржПржл) ржжрзБтАЩржЯрж┐ ржПрж╕рж┐ рж░рзЗржХрзЗрж░ ржмрж░рж╛ржд ржжрзЗржУрзЯрж╛ рж╣рзЯрзЗржЫрж┐рж▓ред рждрж╛рж░ ржоржзрзНржпрзЗ ржкрзНрж░ржержо рж░рзЗржХржЯрж┐ ржПржжрж┐ржи ржПрж╕рзЗ ржкрзМржБржЫрзЗржЫрзЗред ржжрзНржмрж┐рждрзАрзЯ рж░рзЗржХржЯрж┐ ржкрзМржБржЫржирзЛрж░ ржХржерж╛ ржЕржЧрж╕рзНржЯрзЗрж░ рждрзГрждрзАрзЯ рж╕ржкрзНрждрж╛рж╣рзЗред ржПржЦржиржХрж╛рж░ ржПрж╕рж┐ рж░рзЗржХржЧрзБрж▓рж┐рж░ рждрзБрж▓ржирж╛рзЯ ржирждрзБржи ржПрж╕рж┐ рж░рзЗржХ ржкрзНрж░ржпрзБржХрзНрждрж┐ржЧрждржнрж╛ржмрзЗ ржЕржирзЗржХржЯрж╛ржЗ ржПржЧрж┐рзЯрзЗ ржмрж▓рзЗред ржЬрж╛ржирж╛ ржЧрж┐рзЯрзЗржЫрзЗ, ржЪрж▓рждрж┐ ржмржЫрж░рзЗрж░ рж╢рзЗрж╖рзЗ ржЪрж┐ржирзЗрж░ ржбрж╛рж▓рж┐рзЯрж╛ржи ржерзЗржХрзЗ ржорзЗржЯрзНрж░рзЛрж░ рж░рзЗржХ ржЖрж╕рж╛ рж╢рзБрж░рзБ рж╣ржмрзЗред рж╕ржорзНржкрзВрж░рзНржг ржжрзЗрж╢рзАрзЯ ржкрзНрж░ржпрзБржХрзНрждрж┐рждрзЗ рждрзИрж░рж┐ ржЖржЗрж╕рж┐ржПржлрзЗрж░ рж░рзЗржХржЧрзБрж▓рж┐ ржпрж╛рждрзЗ ржХрзЛржиржУржнрж╛ржмрзЗ ржУржЗ рж░рзЗржХрзЗрж░ рждрзБрж▓ржирж╛рзЯ ржкрж┐ржЫрж┐рзЯрзЗ ржирж╛ ржерж╛ржХрзЗ рждрж╛ ржирж┐рж╢рзНржЪрж┐ржд ржХрж░рж╛рж░ ржЪрзЗрж╖рзНржЯрж╛ рж╣рзЯрзЗржЫрзЗред\\xa0\\n\\xa0ржирждрзБржи рж░рзЗржХрзЗрж░ ржорзЛржЯрж░-рж╕рж╣ ржорзВрж▓ ржЕржВрж╢ (ржкрзНрж░ржкрж╛рж▓рж╕ржи) рждрзИрж░рж┐ ржХрж░рзЗржЫрзЗ рж╣рж╛рзЯржжрж░рж╛ржмрж╛ржжрзЗрж░ ржПржХржЯрж┐ рж╕ржВрж╕рзНржерж╛ред рж░рзЗржХрзЗрж░ ржмрж╛ржЗрж░рзЗрж░ ржХрж╛ржарж╛ржорзЛ рждрзИрж░рж┐ ржХрж░рзЗржЫрзЗ ржЖржЗрж╕рж┐ржПржлред ржирждрзБржи рж░рзЗржХржЧрзБрж▓рж┐рждрзЗ ржПрж╕рж┐, ржнрзЗрж╕рзНржЯрж┐ржмрж┐ржЙрж▓,рж╕рж╛рж╕ржкрзЗржирж╢ржи, ржмрзНрж░рзЗржХрж┐ржВ рж╕рж┐рж╕рзНржЯрзЗржо рж╕ржм ржХрж┐ржЫрзБрждрзЗржЗ ржмрзНржпрж╛ржкржХ ржмржжрж▓ ржЖржирж╛ рж╣рзЯрзЗржЫрзЗред\\nржирждрзБржи рж░рзЗржХрзЗ ржнрзЗрж╕рзНржЯрж┐ржмрж┐ржЙрж▓рзЗрж░ ржкрзНрж░рж╕рзНрже ржПржХ ржзрж╛ржХрзНржХрж╛рзЯ рзнрзжрзж ржерзЗржХрзЗ ржмрзЗржбрж╝рзЗ рж╣ржЪрзНржЫрзЗ рззрзкрзжрзж ржорж┐рж▓рж┐ржорж┐ржЯрж╛рж░ред ржлрж▓рзЗ ржжрзБтАЩржЯрж┐ ржХрж╛ржорж░рж╛рж░ ржоржзрзНржпрзЗ ржпрж╛рждрж╛рзЯрж╛ржд ржЕржирзЗржХржЯрж╛ржЗ рж╕рж╣ржЬрж╕рж╛ржзрзНржп рж╣ржмрзЗред ржЪрж▓рждрж┐ ржПрж╕рж┐ рж░рзЗржХрзЗ ржжрж░ржЬрж╛рж░ ржжрзБтАЩржкрзНрж░рж╛ржирзНрждрзЗрж░ ржорж╛ржЭрзЗрж░ ржмрзНржпржмржзрж╛ржи рззрзирззрзж ржорж┐рж▓рж┐ржорж┐ржЯрж╛рж░ред ржУржЗ ржмрзНржпржмржзрж╛ржи ржмрзЗржбрж╝рзЗ рж╣ржЪрзНржЫрзЗ рззрзкрзжрзж ржорж┐рж▓рж┐ржорж┐ржЯрж╛рж░ред ржПрж╕рж┐тАЩрж░ ржбрзНрж░рзЗржирзЗржЬ ржкрж╛ржЗржкрзЗрж░ ржЕржмрж╕рзНржерж╛ржиржУ ржмржжрж▓рзЗржЫрзЗред ржЪрж▓рж╛рж░ рж╕ржорзЯ рж░рзЗржХ ржарж╛ржирзНржбрж╛ ржирж╛ рж╣ржУрзЯрж╛рж░ рж╕ржорж╕рзНржпрж╛ ржарзЗржХрж╛рждрзЗ рж░рзБржл ржорж╛ржЙржирзНржЯрж┐ржВ ржЗржЙржирж┐ржЯрзЗрж░ ржкрзНрж░ржпрзБржХрзНрждрж┐рждрзЗ ржмржжрж▓ ржЖржирж╛ рж╣рзЯрзЗржЫрзЗред ржЭрж╛ржБржХрзБржирж┐ ржХржорж╛рждрзЗ рж╕рзНржЯрж┐рж▓ ржХрзЯрзЗрж▓рзЗрж░ ржмржжрж▓рзЗ рж░рж╛ржмрж╛рж░ рж╕рзНржкрзНрж░рж┐ржВ рж╕рж╛рж╕ржкрзЗржирж╢ржи ржмрзНржпржмрж╣рж╛рж░ ржХрж░рж╛ рж╣ржЪрзНржЫрзЗ ржирждрзБржи рж░рзЗржХрзЗред ржЖржорзВрж▓ ржмржжрж▓ ржЖржирж╛ рж╣рзЯрзЗржЫрзЗ ржмрзНрж░рзЗржХрж┐ржВ рж╕рж┐рж╕рзНржЯрзЗржорзЗред ржирждрзБржи рж░рзЗржХрзЗ рж╕рзНржЯрзЗржкрж▓рзЗрж╕ рж░рж┐ржЬрзЗржирж╛рж░рзЗржЯрж┐ржн ржмрзНрж░рзЗржХ ржерж╛ржХржЫрзЗред ржПрждрзЗ ржЕржирзЗржХржЯрж╛ржЗ ржмрж┐ржжрзНржпрзБрзО рж╕рж╛рж╢рзНрж░рзЯ рж╣ржмрзЗред\\xa0\\nржорзЗржЯрзНрж░рзЛрж░ ржПржХ ржЖржзрж┐ржХрж╛рж░рж┐ржХ ржЬрж╛ржирж╛ржи, тАЬржерж╛рж░рзНржб рж░рзЗрж▓рзЗрж░ рж╕ржЩрзНржЧрзЗ рж╕ржВржпрзЛржЧржХрж╛рж░рзА ржЕржВрж╢ ржмрж╕рж╛ржирзЛ ржЫрж╛ржбрж╝рж╛ржУ ржЕржмрж╢рж┐рж╖рзНржЯ ржХрж╛ржЬ ржХрж░рждрзЗ рзкрзл ржжрж┐ржи рж▓рж╛ржЧрждрзЗ ржкрж╛рж░рзЗред рждрж╛рж░ржкрж░ ржУржЗ рж░рзЗржХ ржЪрж▓рж╛рж░ ржЙржкржпрзБржХрзНржд рж╣ржмрзЗредтАЭ',\n",
       "  'ржжрж┐ржШрж╛рж░ рж╣рзЛржЯрзЗрж▓ ржерзЗржХрзЗ ржЙржжрзНржзрж╛рж░ рж╣рж▓ ржпрзБржмржХ-ржпрзБржмрждрзАрж░ ржЭрзБрж▓ржирзНржд ржжрзЗрж╣ред рж╢рзБржХрзНрж░ржмрж╛рж░ рж╕ржХрж╛рж▓рзЗ ржирж┐ржЙ ржжрж┐ржШрж╛рж░ ржПржХржЯрж┐ рж╣рзЛржЯрзЗрж▓ ржерзЗржХрзЗ ржУржЗ ржпрзБржЧрж▓рзЗрж░ ржжрзЗрж╣ ржЙржжрзНржзрж╛рж░ рж╣рзЯред ржорзГрждржжрзЗрж░ ржжрж╛ржо ржЧрзЛржкрж╛рж▓ рж╕рж░ржХрж╛рж░ ржУ ржЕрждрж╕рж┐ рж╕рж░ржХрж╛рж░ред рж╣рзЛржЯрзЗрж▓ ржХрж░рзНрждрзГржкржХрзНрж╖рзЗрж░ ржХрж╛ржЫрзЗ рждрж╛ржБрж░рж╛ ржПржЗ ржирж╛ржо ржжрж┐рзЯрзЗржЗ рж░рзБржо ржмрзБржХ ржХрж░рзЗржЫрж┐рж▓рзЗржи ржмрж▓рзЗ ржЬрж╛ржирж╛ ржЧрж┐рзЯрзЗржЫрзЗред\\n\\nржПржЗ ржмрж┐рж╖рзЯрзЗ ржЕржирзНржпрж╛ржирзНржп ржЦржмрж░\\n\\nржлрзБржЯржлрзБржЯрзЗ ржмрж╛ржЪрзНржЪрж╛ржЯрж┐ ржХрж╛рж░! ржжрж┐ржШрж╛рж░ рж╕рзИржХрждрзЗ рж╢рж┐рж╢рзБрж░ ржХрж╛ржирзНржирж╛, ржЬрж▓рзНржкржирж╛ ржЪрж╛ржкрж╛ ржжрж┐рж▓ рж╕ржорзБржжрзНрж░рзЗрж░ ржЧрж░рзНржЬржи\\nржжрж┐ржШрж╛рзЯ ржорж╛рж░рж╛рждрзНржоржХ ржЕржнрж┐ржпрзЛржЧ! ржорж╣рж┐рж▓рж╛ ржкрж░рзНржпржЯржХржХрзЗ ржкрзНрж░рж╛рзЯ ржмрж┐ржмрж╕рзНрждрзНрж░ ржХрж░рзЗ рж╣рзЗржирж╕рзНржерж╛\\n\\n\\nржЬрж╛ржирж╛ ржЧрж┐рзЯрзЗржЫрзЗ, рж╕рзНржмрж╛ржорзА-рж╕рзНрждрзНрж░рзАрж░ ржкрж░рж┐ржЪрзЯ ржжрж┐рзЯрзЗ ржУржЗ ржпрзБржЧрж▓ ржЧржд рззрзл рждрж╛рж░рж┐ржЦ рж╣рзЛржЯрзЗрж▓рзЗ ржУржарзЗржиред рж╣рзЛржЯрзЗрж▓рзЗрж░ рж░рзЗржЬрж┐рж╕рзНржЯрж╛рж░рзЗ рж╣рзБржЧрж▓рж┐рж░ рждрж╛рж░ржХрзЗрж╢рзНржмрж░рзЗрж░ ржорзБржХрзНрждрж╛рж░ржкрзБрж░рзЗрж░ ржЧрзНрж░рж╛ржорзЗрж░ ржмрж╛рж╕рж┐ржирзНржжрж╛ ржмрж▓рзЗржи рждрж╛ржБрж░рж╛ред рж╢рзБржХрзНрж░ржмрж╛рж░ржЗ рждрж╛ржБржжрзЗрж░ ржлрж┐рж░рзЗ ржпрж╛ржУрзЯрж╛рж░ ржХржерж╛ ржЫрж┐рж▓ред\\xa0\\nржПржжрж┐ржи рж╕ржХрж╛рж▓рзЗ рж░рзБржорзЗрж░ ржоржзрзНржпрзЗ ржерзЗржХрзЗ ржпрзБржмржХ-ржпрзБржмрждрзАрж░ ржХрзЛржиржУ рж╕рж╛ржбрж╝рж╛рж╢ржмрзНржж ржирж╛ ржкрж╛ржУрзЯрж╛рзЯ рж╕ржирзНржжрзЗрж╣ рж╣рзЯ рж╣рзЛржЯрзЗрж▓ржХрж░рзНржорзАржжрзЗрж░ ржоржзрзНржпрзЗред ржкрж░рзЗ ржжрж░ржЬрж╛ ржнрж╛ржЩрждрзЗржЗ рждрж╛ржБржжрзЗрж░ ржЭрзБрж▓ржирзНржд ржжрзЗрж╣ ржжрзЗржЦрждрзЗ ржкрж╛ржи рж╣рзЛржЯрзЗрж▓ржХрж░рзНржорзАрж░рж╛ред ржкрж░рзЗ ржкрзБрж▓рж┐рж╢ ржПрж╕рзЗ ржжрзЗрж╣ржжрзБржЯрж┐ ржЙржжрзНржзрж╛рж░ ржХрж░рзЗ ржорзЯржирж╛рждржжржирзНрждрзЗрж░ ржЬржирзНржп ржкрж╛ржарж╛рзЯред\\nржкрзБрж▓рж┐рж╢рзЗрж░ ржкрзНрж░рж╛ржержорж┐ржХ ржзрж╛рж░ржгрж╛ ржкрзНрж░рзЗржоржШржЯрж┐ржд ржХрж╛рж░ржгрзЗрж░ ржЬрзЗрж░рзЗ ржЖрждрзНржорж╣рждрзНржпрж╛ ржХрж░рзЗржЫрзЗржи ржУржЗ ржпрзБржЧрж▓ред ржШржЯржирж╛рж░ рждржжржирзНрждрзЗ ржирзЗржорзЗржЫрзЗ ржжрж┐ржШрж╛ ржерж╛ржирж╛рж░ ржкрзБрж▓рж┐рж╢ред',\n",
       "  \"ржХрж┐ржЫрзБржжрж┐ржи ржЖржЧрзЗржЗ рж░рж╛ржЬрзНржпрзЗрж░ ржЧрж╛ржЗржб ржмржирзЗ ржЧрж┐рзЯрзЗржЫрж┐рж▓рзЗржи ржЪрзЗрждрзЗрж╢рзНржмрж░ ржкрзВржЬрж╛рж░рж╛ред рж░рж╛ржЬржХрзЛржЯрзЗ ржнрж╛рж░ржд-ржУрзЯрзЗрж╕рзНржЯ ржЗржирзНржбрж┐ржЬ ржкрзНрж░ржержо ржЯрзЗрж╕рзНржЯ ржЪрж▓рж╛ржХрж╛рж▓рзАржиржЗ ржкрзВржЬрж╛рж░рж╛ржХрзЗ ржжрзЗржЦрж╛ ржЧрж┐рзЯрзЗржЫрж┐рж▓ ржирзЯрж╛ ржЕржмрждрж╛рж░рзЗред ржПржмрж╛рж░ рж╕рзЗржЗ ржкржерзЗржЗ рж╣рж╛ржБржЯрж▓рзЗржи ржорж╣ржорзНржоржж рж╕рж┐рж░рж╛ржЬред\\n\\nржПржЗ ржмрж┐рж╖рзЯрзЗ ржЕржирзНржпрж╛ржирзНржп ржЦржмрж░\\n\\nржХрзБржорзНржмрж▓рзЗрж░ ржЧрзБржЧрж▓рж┐рждрзЗ тАШржмрзЛрж▓рзНржбтАЩ рж╕рзЛрж╣рж┐ржирзА, ржорж╛ржЭ ржЖржХрж╛рж╢рзЗ ржоржи ржЬрж┐рждрж▓рзЗржи ржХрзНрж░рж┐ржХрзЗржЯрж╛рж░\\n\\n\\nрж╣рж╛рзЯржжрж░рж╛ржмрж╛ржжрзЗ ржжрзНржмрж┐рждрзАрзЯ ржЯрзЗрж╕рзНржЯ рж╢рзБрж░рзБ рж╣ржЪрзНржЫрзЗ рж╢рзБржХрзНрж░ржмрж╛рж░рзЗржЗред рж╕рзЗржЗ ржЯрзЗрж╕рзНржЯ рж╢рзБрж░рзБрж░ ржЖржЧрзЗржЗ рж╣рж╛рзЯржжрж░рж╛ржмрж╛ржжрзЗрж░ ржЯрзНржпрзБрж░ ржЧрж╛ржЗржб рж╣рзЯрзЗ ржЧрзЗрж▓рзЗржи рж╕рж┐рж░рж╛ржЬред рж╣рж╛рзЯржжрж░рж╛ржмрж╛ржжрзЗрж░ ржжрзНрж░рж╖рзНржЯржмрзНржп ржХрзА? рж╕рж┐рж░рж╛ржЬ ржЬрж╛ржирж╛ржЪрзНржЫрзЗржи, ржЪрж╛рж░ржорж┐ржирж╛рж░рзЗ ржпрзЗрждрзЗржЗ рж╣ржмрзЗред ржУржЦрж╛ржи ржерзЗржХрзЗ ржЪрж╛ржБржж ржжрзЗржЦрж╛рж░ ржЕржнрж┐ржЬрзНржЮрждрж╛ ржжрзБрж░рзНржзрж░рзНрж╖! ржкрж╛рж╢рж╛ржкрж╛рж╢рж┐ рждрж┐ржирж┐ ржирзЗржХрж▓рзЗрж╕ рж░рзЛржбрзЗржУ ржШрзБрж░рзЗ ржЖрж╕рж╛рж░ ржкрж░рж╛ржорж░рзНрж╢ ржжрж┐рзЯрзЗржЫрзЗржи ржнрзНрж░ржоржгрж╛рж░рзНржерзАржжрзЗрж░ред рж░рж┐рж▓рзНржпрж╛ржХрзНрж╕ ржХрж░рж╛рж░ ржЬржирзНржп ржирж╛ржХрж┐ ржирзЗржХрж▓рзЗрж╕ рж░рзЛржб ржЖржжрж░рзНрж╢ред\\nржЯрзНржпрзБрж░рж┐рж╕рзНржЯржжрзЗрж░ рж╣рж╛рзЯржжрж░рж╛ржмрж╛ржжрзЗрж░ ржмрж┐ржЦрзНржпрж╛ржд ржмрж┐рж░рж┐рзЯрж╛ржирж┐ ржЪрзЗржЦрзЗ ржжрзЗржУрзЯрж╛рж░ ржХржерж╛ ржмрж▓рзЗржЫрзЗржи рждрж╛рж░ржХрж╛ ржкрзЗрж╕рж╛рж░ред рж╕рж┐рж░рж╛ржЬрзЗрж░ ржПржЗ тАШржЧрж╛ржЗржбтАЩ рж╣ржУрзЯрж╛рж░ ржнрж┐ржбрж┐ржУ ржмрж┐рж╕рж┐рж╕рж┐ржЖржЗ рж╢рзЗрзЯрж╛рж░ ржХрж░рзЗржЫрзЗржи ржирж┐ржЬрзЗржжрзЗрж░ ржлрзЗрж╕ржмрзБржХ ржкрзЗржЬрзЗред ржЗрждрж┐ржоржзрзНржпрзЗржЗ ржпрж╛ ржнрж╛ржЗрж░рж╛рж▓ред ржнрж┐ржбрж┐ржУрждрзЗ рж╕рж┐рж░рж╛ржЬржХрзЗ ржжрзЗржЦрж╛ ржпрж╛ржЪрзНржЫрзЗ, рж╕рзНржерж╛ржирзАрзЯ ржЙржЪрзНржЪрж╛рж░ржгрзЗ ржХржерж╛ ржмрж▓рждрзЗред\\n\\nLocal boy Siraj's top 3 things to do in Hyderabad ЁЯШО\\n\\nWhat to do when in Hyderabad? Mohammed Siraj lists his top 3 not-to-be-missed things from his home land. P.S In total Hyderabadi style ЁЯдЩ - by @28anand\\n\\nMUST WATCH тЦ╢я╕П https://t.co/ceSRC2lMHK #INDvWI pic.twitter.com/toeN6j70pR\\nтАФ BCCI (@BCCI) October 11, 2018\\n\\nржнрж┐ржбрж┐ржУрждрзЗ рж╕рж┐рж░рж╛ржЬ ржПржХржЬржи тАШржкрзНрж░рж╛ржЙржб рж╣рж╛рзЯржжрж░рж╛ржмрж╛ржжрж┐тАЩред ржШрж░рзЗрж░ ржЫрзЗрж▓рзЗрж░ ржПржоржи ржмржХрзНрждржмрзНржпрзЗрж░ ржкрж░рзЗ рж╣рж╛рзЯржжрж░рж╛ржмрж╛ржжрзЗрж░ ржжрж░рзНрж╢ржХрж░рж╛ ржХрждржЯрж╛ ржорж╛ржа ржнрж░рж╛рзЯ рж╕рзЗржЯрж╛ржЗ ржПржЦржи ржжрзЗржЦрж╛рж░ред\",\n",
       "  'ржорж╛рж░рзБрждрж┐рж░ ржХрж╛рж░ржЦрж╛ржирж╛рзЯ рж╣рж╛ржорж▓рж╛ ржПржмржВ ржорзНржпрж╛ржирзЗржЬрж╛рж░ржХрзЗ ржкрзБржбрж╝рж┐рзЯрзЗ ржорж╛рж░рж╛рж░ ржШржЯржирж╛рзЯ рзйрзз ржЬржиржХрзЗ ржжрзЛрж╖рзА рж╕рж╛ржмрзНржпрж╕рзНржд ржХрж░рж▓ рж╣рж░рж┐рзЯрж╛ржирж╛рж░ ржЖржжрж╛рж▓рждред ржжрзЛрж╖рзАрж░рж╛ ржкрзНрж░рждрзНржпрзЗржХрзЗржЗ ржорж╛ржирзЗрж╕рж░рзЗрж░ ржорж╛рж░рзБрждрж┐-рж╕рзБржЬрзБржХрж┐ ржкрзНрж▓рзНржпрж╛ржирзНржЯрзЗрж░ ржХрж░рзНржорзАред ржжрж┐рж▓рзНрж▓рж┐рж░ ржкрзНрж░рж╛рзЯ рзкрзж ржХрж┐рж▓рзЛржорж┐ржЯрж╛рж░ ржжржХрзНрж╖рж┐ржгрзЗ ржЕржмрж╕рзНржерж┐ржд ржУржЗ ржкрзНрж▓рзНржпрж╛ржирзНржЯрзЗ рзирзжрззрзи рж╕рж╛рж▓рзЗ ржПржХ ржжрж▓ рж╢рзНрж░ржорж┐ржХрзЗрж░ рж╣рж╛рждрзЗ ржЖржХрзНрж░рж╛ржирзНржд рж╣рзЯрзЗржЫрж┐рж▓рзЗржи рж╕ржВрж╕рзНржерж╛рж░ ржЙржЪрзНржЪржкржжрж╕рзНрже ржХрж░рзНрждрж╛рж░рж╛ред ржХржиржлрж╛рж░рзЗржирзНрж╕ рж░рзБржорзЗрж░ ржоржзрзНржпрзЗ рж╣рж┐ржЙржорзНржпрж╛ржи рж░рж┐рж╕рзЛрж░рзНрж╕ ржорзНржпрж╛ржирзЗржЬрж╛рж░ ржЕржмржирзАрж╢ ржХрзБржорж╛рж░ ржжрзЗржмрзЗрж░ ржЧрж╛рзЯрзЗ ржЖржЧрзБржи ржзрж░рж┐рзЯрзЗ ржжрзЗржУрзЯрж╛ рж╣рзЯрзЗржЫрж┐рж▓ред рж╕рзЗржЦрж╛ржирзЗржЗ ржорзГрждрзНржпрзБ рж╣рзЯрзЗржЫрж┐рж▓ рждрж╛ржБрж░ред ржжрзАрж░рзНржШ ржХрзНрж╖ржг ржзрж░рзЗ ржЪрж▓рждрзЗ ржерж╛ржХрж╛ рждрж╛ржгрзНржбржмрзЗ ржорзНржпрж╛ржирзЗржЬржорзЗржирзНржЯрзЗрж░ ржЕржирзНрждржд рзлрзж ржЬржи ржХрж░рзНрждрж╛ ржЬржЦржо рж╣рзЯрзЗржЫрж┐рж▓рзЗржи, ржЖржХрзНрж░рж╛ржирзНржд рж╣рзЯрзЗржЫрж┐рж▓ ржкрзБрж▓рж┐рж╢ржУред ржШржЯржирж╛рж░ ржмржЫрж░ ржЪрж╛рж░рзЗржХ ржкрж░ ржЧрзБрж░рзБржЧрзНрж░рж╛ржорзЗрж░ ржЖржжрж╛рж▓ржд ржорж╛ржорж▓рж╛ржЯрж┐рж░ рж░рж╛рзЯ ржШрзЛрж╖ржгрж╛ ржХрж░рж▓ред ржорж╛рж░рзБрждрж┐ рж╕ржВрж╕рзНржерж╛ ржкрзНрж░рждрж┐ ржмржЫрж░ ржпрзЗ ржкрж░рж┐ржорж╛ржг ржЧрж╛ржбрж╝рж┐ рждрзИрж░рж┐ ржХрж░рзЗ, рждрж╛рж░ ржПржХ рждрзГрждрзАрзЯрж╛ржВрж╢ржЗ рждрзИрж░рж┐ рж╣рзЯ ржорж╛ржирзЗрж╕рж░рзЗрж░ ржкрзНрж▓рзНржпрж╛ржирзНржЯрзЗред рзирзжрззрзи рж╕рж╛рж▓рзЗрж░ ржмрзЗржиржЬрж┐рж░ ржЧрзЛрж▓ржорж╛рж▓рзЗрж░ ржкрж░ рж╕рзЗржЗ ржорж╛ржирзЗрж╕рж░ ржкрзНрж▓рзНржпрж╛ржирзНржЯ ржорж╛рж╕ ржЦрж╛ржирзЗржХрзЗрж░ ржЬржирзНржп ржмржирзНржз ржХрж░рзЗ ржжрж┐рзЯрзЗржЫрж┐рж▓ ржорж╛рж░рзБрждрж┐ ржХрж░рзНрждрзГржкржХрзНрж╖ред рж╢рзНрж░ржорж┐ржХржжрзЗрж░ ржмрж┐рж░рзБржжрзНржзрзЗ рж╢рзГржЩрзНржЦрж▓рж╛ржнржЩрзНржЧрзЗрж░ ржЕржнрж┐ржпрзЛржЧржХрзЗ ржШрж┐рж░рзЗ ржЧрзЛрж▓ржорж╛рж▓рзЗрж░ рж╕рзВрждрзНрж░ржкрж╛ржд рж╣рзЯрзЗржЫрж┐рж▓ред ржорзНржпрж╛ржирзЗржЬржорзЗржирзНржЯрзЗрж░ ржЕржнрж┐ржпрзЛржЧ ржЫрж┐рж▓, ржпрзЗ ржХрзЛржиржУ ржЖрж▓рзЛржЪржирж╛рж░ рж╕ржорзЯрзЗржЗ рж╢рзНрж░ржорж┐ржХ ржЗржЙржирж┐рзЯржи рж╕ржВрж╕рзНржерж╛рж░ ржкржжрж╕рзНрже ржХрж░рзНрждрж╛ржжрзЗрж░ ржЖржХрзНрж░ржоржг ржХрж░рждред ржкрж░рж┐рж╕рзНржерж┐рждрж┐ рж╢рзГржЩрзНржЦрж▓рж╛ржнржЩрзНржЧрзЗрж░ ржкрж░рзНржпрж╛рзЯрзЗ ржпрж╛ржЪрзНржЫрж┐рж▓ ржмрж▓рзЗ ржорзНржпрж╛ржирзЗржЬржорзЗржирзНржЯ ржоржирзЗ ржХрж░ржЫрж┐рж▓ред рждрж╛ржЗ ржХржарзЛрж░ ржкржжржХрзНрж╖рзЗржкрзЗрж░ ржХржерж╛ ржнрж╛ржмрж╛ рж╣рзЯрзЗржЫрж┐рж▓ред рж╕рзЗ рж╕ржм ржирж┐рзЯрзЗ ржжрзАрж░рзНржШ ржжрж┐ржи ржзрж░рзЗ ржорж╛ржирзЗрж╕рж░ ржкрзНрж▓рзНржпрж╛ржирзНржЯрзЗ рж╢рзНрж░ржорж┐ржХ ржЕрж╕ржирзНрждрзЛрж╖ ржЪрж▓ржЫрж┐рж▓ред рж╕рзЗржЗ ржЕрж╕ржирзНрждрзЛрж╖ржЗ рж╢рзЗрж╖ ржжрж┐ржирзЗ рждрж╛ржгрзНржбржмрзЗрж░ ржЪрзЗрж╣рж╛рж░рж╛ ржирзЗрзЯред рж▓рзЛрж╣рж╛рж░ рж░ржб ржПржмржВ ржирж┐рж░рзНржорж┐рзЯржорж╛ржи ржЧрж╛ржбрж╝рж┐рж░ ржжрж░ржЬрж╛рж░ ржкрзНржпрж╛ржирзЗрж▓ ржирж┐рзЯрзЗ рж╣рж╛ржорж▓рж╛ ржЪрж╛рж▓рж╛ржи рж╢рзНрж░ржорж┐ржХрж░рж╛ред рж╣рж┐ржЙржорзНржпрж╛ржи рж░рж┐рж╕рзЛрж░рзНрж╕ ржорзНржпрж╛ржирзЗржЬрж╛рж░ржХрзЗ ржкрзБржбрж╝рж┐рзЯрзЗ ржорж╛рж░рж╛ рж╣рзЯред рж╕рзБржкрж╛рж░ржнрж╛ржЗржЬрж╛рж░ ржПржмржВ ржЕржирзНржп ржкржжрж╕рзНрже ржХрж░рж╛ржжрзЗрж░ ржЦрзБржБржЬрзЗ ржмрж╛рж░ ржХрж░рзЗ рж╣рж╛ржорж▓рж╛ ржЪрж╛рж▓рж╛ржирзЛ рж╣рзЯред ржЧрзЛржЯрж╛ ржХрж╛рж░ржЦрж╛ржирж╛ ржЪрждрзНржмрж░ ржЬрзБржбрж╝рзЗ ржнрж╛ржЩржЪрзБрж░ ржЪрж▓рзЗ, ржЬрж╛рзЯржЧрж╛рзЯ ржЬрж╛рзЯржЧрж╛рзЯ ржЖржЧрзБржиржУ ржЧрж╛рж▓рж┐рзЯрзЗ ржжрзЗржУрзЯрж╛ рж╣рзЯред рззрзирзжрзж ржкрзБрж▓рж┐рж╢ ржкрж╛ржарж┐рзЯрзЗ ржХрж╛рж░ржЦрж╛ржирж╛ ржирж┐рзЯржирзНрждрзНрж░ржгрзЗ ржПржирзЗржЫрж┐рж▓ ржкрзНрж░рж╢рж╛рж╕ржиред рждржмрзЗ рзп ржЬржи ржкрзБрж▓рж┐рж╢ ржХрж░рзНрждрж╛ржУ рж╕ржВржШрж░рзНрж╖рзЗ ржЬржЦржо рж╣рзЯрзЗржЫрж┐рж▓рзЗржиред ржорзНржпрж╛ржирзЗржЬржорзЗржирзНржЯрзЗрж░ рзЯрзЗ рзлрзж ржЬржи ржХрж░рзНрждрж╛ржХрзЗ ржЬржЦржо ржЕржмрж╕рзНржерж╛рзЯ ржЙржжрзНржзрж╛рж░ ржХрж░рж╛ рж╣рзЯрзЗржЫрж┐рж▓, рждрж╛ржБржжрзЗрж░ ржЕржзрж┐ржХрж╛ржВрж╢ржЗ рж░ржХрзНрждрж╛ржХрзНржд ржЫрж┐рж▓рзЗржи, ржЕржирзЗржХрзЗржЗ ржЕржЪрзЗрждржиржУ ржЫрж┐рж▓рзЗржиред рж╢рзНрж░ржорж┐ржХржжрзЗрж░ ржЕржмрж╢рзНржп ржжрж╛ржмрж┐ ржЫрж┐рж▓, ржорзНржпрж╛ржирзЗржЬржорзЗржирзНржЯрзЗрж░ ржмрж╛ржбрж╝рж╛ржЯрзЗ ржмрж╛ржЙржирзНрж╕рж╛рж░рж░рж╛ ржкрзНрж▓рзНржпрж╛ржирзНржЯрзЗрж░ рж╕ржорж╕рзНржд ржжрж░ржЬрж╛ ржмржирзНржз ржХрж░рзЗ ржжрж┐рзЯрзЗ рж╢рзНрж░ржорж┐ржХржжрзЗрж░ ржЙржкрж░ рж╣рж╛ржорж▓рж╛ ржЪрж╛рж▓рж┐рзЯрзЗржЫрж┐рж▓ред рж╕рзЗржЦрж╛ржи ржерзЗржХрзЗржЗ ржирж╛ржХрж┐ ржЧрзЛрж▓ржорж╛рж▓рзЗрж░ рж╕рзВрждрзНрж░ржкрж╛рждред ржХрж╛рж░ржЦрж╛ржирж╛ ржЪрждрзНржмрж░рзЗрж░ ржмрж┐ржнрж┐ржирзНржи рж╕рж┐рж╕рж┐ржЯрж┐ржнрж┐рж░ ржлрзБржЯрзЗржЬ ржЦрждрж┐рзЯрзЗ ржжрзЗржЦрзЗ рж╕ржирзНржжрзЗрж╣ржнрж╛ржЬржиржжрзЗрж░ ржЧрзНрж░рзЗржлрждрж╛рж░ ржХрж░рзЗ ржкрзБрж▓рж┐рж╢ред ржмрж╣рзБ рж╢рзНрж░ржорж┐ржХржХрзЗ ржЬрзЗрж░рж╛ ржХрж░рж╛ рж╣рзЯред ржкрзНрж░рж╛рзЯ рззрзлрзж ржЬржиржХрзЗ ржЕржнрж┐ржпрзБржХрзНржд ржХрж░рж╛ рж╣рзЯред рж╢рзБржХрзНрж░ржмрж╛рж░ ржЧрзБрж░рзБржЧрзНрж░рж╛ржорзЗрж░ ржЖржжрж╛рж▓ржд ржЕржнрж┐ржпрзБржХрзНрждржжрзЗрж░ ржоржзрзНржпрзЗ рзйрзз ржЬржиржХрзЗ ржжрзЛрж╖рзА рж╕рж╛ржмрзНржпрж╕рзНржд ржХрж░рзЗржЫрзЗред рждрж╛ржБржжрзЗрж░ ржмрж┐рж░рзБржжрзНржзрзЗ ржорзНржпрж╛ржирзЗржЬржорзЗржирзНржЯрзЗрж░ ржЙржкрж░ рж╣рж╛ржорж▓рж╛, рж╕ржорзНржкрждрзНрждрж┐ ржирж╖рзНржЯ ржПржмржВ ржорзНржпрж╛ржирзЗржЬрж╛рж░ржХрзЗ ржЦрзБржирзЗрж░ ржЕржнрж┐ржпрзЛржЧ ржкрзНрж░ржорж╛ржгрж┐ржд рж╣рзЯрзЗржЫрзЗ ржмрж▓рзЗ ржЖржжрж╛рж▓рждржЯрж┐ ржЬрж╛ржирж┐рзЯрзЗржЫрзЗред рж╕рж╛ржЬрж╛ ржШрзЛрж╖ржгрж╛ рж╣ржУрзЯрж╛ ржПржЦржиржУ ржмрж╛ржХрж┐ред']}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_datasets[\"train\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6ae60094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# batch_size = 16\n",
    "batch_size = 32\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], shuffle=True, batch_size=batch_size, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"test\"], batch_size=batch_size, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b3f49e99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': torch.Size([32]),\n",
       " 'input_ids': torch.Size([32, 128, 50]),\n",
       " 'token_type_ids': torch.Size([32, 128]),\n",
       " 'attention_mask': torch.Size([32, 128])}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f39d52b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8421) torch.Size([32, 6])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "with torch.no_grad():\n",
    "    outputs = model(**batch)\n",
    "    print(outputs.loss, outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "09c186ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric_fun = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    metric_result = metric_fun.compute(references=labels, predictions=predictions)\n",
    "    return {\n",
    "        \"accuracy\": metric_result[\"accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8cb53020",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  disable weights and biases logging\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "324b2f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# batch_size = 16\n",
    "batch_size = 32\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(tokenized_datasets[\"train\"]) // batch_size\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"models/bert-unigram-bengali-classifier\",\n",
    "    overwrite_output_dir=True,\n",
    "    report_to = None,\n",
    "    logging_dir= None,\n",
    "    save_strategy=\"no\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    #learning_rate=2e-5,\n",
    "    learning_rate=3e-5,\n",
    "    weight_decay=0.01,\n",
    "    #weight_decay=0.02,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    #num_train_epochs=6,\n",
    "    #push_to_hub=True,\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9d94821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import concatenate_datasets\n",
    "\n",
    "# entire_train = concatenate_datasets([tokenized_datasets[\"train\"], tokenized_datasets[\"validation\"]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8085d3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    #train_dataset=entire_train,\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4ef743fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "trainer.remove_callback(transformers.integrations.TensorBoardCallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f026e2e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1411\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='90' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45/45 02:52]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.8308228254318237,\n",
       " 'eval_accuracy': 0.1261516654854713,\n",
       " 'eval_runtime': 10.9291,\n",
       " 'eval_samples_per_second': 129.104,\n",
       " 'eval_steps_per_second': 4.117}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(tokenized_datasets[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7440206a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 11284\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1765\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1765' max='1765' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1765/1765 13:43, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.487805</td>\n",
       "      <td>0.829908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.589400</td>\n",
       "      <td>0.423985</td>\n",
       "      <td>0.866052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.325300</td>\n",
       "      <td>0.350479</td>\n",
       "      <td>0.887314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.325300</td>\n",
       "      <td>0.361164</td>\n",
       "      <td>0.890149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.187200</td>\n",
       "      <td>0.375412</td>\n",
       "      <td>0.896527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1411\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1411\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1411\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1411\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1411\n",
      "  Batch size = 32\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1765, training_loss=0.32843787649873296, metrics={'train_runtime': 824.6313, 'train_samples_per_second': 68.418, 'train_steps_per_second': 2.14, 'total_flos': 2.25781730044416e+17, 'train_loss': 0.32843787649873296, 'epoch': 5.0})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ddefcfac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1411\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45/45 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.29614242911338806,\n",
       " 'eval_accuracy': 0.9192062367115521,\n",
       " 'eval_runtime': 14.0471,\n",
       " 'eval_samples_per_second': 100.448,\n",
       " 'eval_steps_per_second': 3.204,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(tokenized_datasets[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "657bd755",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = \"best-model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4d8e8380",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    {\n",
    "        'model': model.state_dict(),\n",
    "    },\n",
    "    output_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "26a9b187",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in models/bert-unigram-bengali-classifier\\config.json\n",
      "Model weights saved in models/bert-unigram-bengali-classifier\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"models/bert-unigram-bengali-classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "036d1b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "0dd917c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file models/bert-unigram-bengali-classifier\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file models/bert-unigram-bengali-classifier\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at models/bert-unigram-bengali-classifier were not used when initializing BertForSequenceClassification: ['bert.embeddings.word_embeddings.char_conv_4.bias', 'bert.embeddings.word_embeddings.char_conv_0.weight', 'bert.embeddings.word_embeddings._highways._layers.1.bias', 'bert.embeddings.word_embeddings.char_conv_1.weight', 'bert.embeddings.word_embeddings._highways._layers.0.bias', 'bert.embeddings.word_embeddings.char_conv_3.bias', 'bert.embeddings.word_embeddings.char_conv_6.weight', 'bert.embeddings.word_embeddings.char_conv_2.bias', 'bert.embeddings.word_embeddings._projection.weight', 'bert.embeddings.word_embeddings._highways._layers.0.weight', 'bert.embeddings.word_embeddings.char_conv_6.bias', 'bert.embeddings.word_embeddings.char_conv_3.weight', 'bert.embeddings.word_embeddings.char_conv_0.bias', 'bert.embeddings.word_embeddings._char_embedding_weights', 'bert.embeddings.word_embeddings._projection.bias', 'bert.embeddings.word_embeddings.char_conv_1.bias', 'bert.embeddings.word_embeddings.char_conv_5.weight', 'bert.embeddings.word_embeddings.char_conv_2.weight', 'bert.embeddings.word_embeddings.char_conv_4.weight', 'bert.embeddings.word_embeddings._highways._layers.1.weight', 'bert.embeddings.word_embeddings.char_conv_5.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at models/bert-unigram-bengali-classifier and are newly initialized: ['bert.embeddings.word_embeddings.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#### LOADING BERT FOR CLASSIFICATION ####\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"models/bert-unigram-bengali-classifier\", num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "a806f075",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file models/bert-unigram-bengali-classifier\\config.json\n",
      "You are using a model of type bert to instantiate a model of type character_bert. This is not supported for all configurations of models and can yield errors.\n",
      "Model config CharacterBertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"character_embeddings_dim\": 16,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cnn_activation\": \"relu\",\n",
      "  \"cnn_filters\": [\n",
      "    [\n",
      "      1,\n",
      "      32\n",
      "    ],\n",
      "    [\n",
      "      2,\n",
      "      32\n",
      "    ],\n",
      "    [\n",
      "      3,\n",
      "      64\n",
      "    ],\n",
      "    [\n",
      "      4,\n",
      "      128\n",
      "    ],\n",
      "    [\n",
      "      5,\n",
      "      256\n",
      "    ],\n",
      "    [\n",
      "      6,\n",
      "      512\n",
      "    ],\n",
      "    [\n",
      "      7,\n",
      "      1024\n",
      "    ]\n",
      "  ],\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_word_length\": 50,\n",
      "  \"mlm_vocab_size\": 100000,\n",
      "  \"model_type\": \"character_bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_highway_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file models/bert-unigram-bengali-classifier\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at models/bert-unigram-bengali-classifier were not used when initializing CharacterBertModel: ['bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.embeddings.word_embeddings._highways._layers.0.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.embeddings.position_ids', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.embeddings.word_embeddings.char_conv_6.bias', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.pooler.dense.weight', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.embeddings.word_embeddings._highways._layers.1.weight', 'bert.embeddings.word_embeddings.char_conv_0.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.embeddings.word_embeddings._highways._layers.0.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.embeddings.word_embeddings.char_conv_1.bias', 'bert.embeddings.word_embeddings.char_conv_2.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.embeddings.word_embeddings.char_conv_4.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.embeddings.word_embeddings._highways._layers.1.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.embeddings.word_embeddings.char_conv_1.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.embeddings.word_embeddings.char_conv_3.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.embeddings.word_embeddings.char_conv_6.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'classifier.bias', 'bert.embeddings.word_embeddings.char_conv_3.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.embeddings.word_embeddings.char_conv_0.bias', 'bert.embeddings.word_embeddings._char_embedding_weights', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.embeddings.word_embeddings._projection.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.5.output.dense.weight', 'bert.embeddings.word_embeddings.char_conv_4.weight', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.pooler.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'classifier.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.embeddings.word_embeddings.char_conv_2.bias', 'bert.embeddings.word_embeddings._projection.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.1.output.dense.weight', 'bert.embeddings.word_embeddings.char_conv_5.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.embeddings.word_embeddings.char_conv_5.bias']\n",
      "- This IS expected if you are initializing CharacterBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CharacterBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CharacterBertModel were not initialized from the model checkpoint at models/bert-unigram-bengali-classifier and are newly initialized: ['encoder.layer.4.attention.self.key.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.8.output.dense.weight', 'embeddings.word_embeddings.char_conv_6.bias', 'encoder.layer.9.output.LayerNorm.bias', 'embeddings.word_embeddings.char_conv_1.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.11.intermediate.dense.bias', 'embeddings.word_embeddings._char_embedding_weights', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.weight', 'embeddings.word_embeddings.char_conv_5.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.1.output.dense.weight', 'embeddings.word_embeddings._highways._layers.1.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'embeddings.word_embeddings.char_conv_3.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.weight', 'embeddings.word_embeddings.char_conv_2.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.4.output.LayerNorm.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.5.attention.output.dense.bias', 'embeddings.word_embeddings.char_conv_3.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'embeddings.word_embeddings._highways._layers.0.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.weight', 'embeddings.LayerNorm.weight', 'pooler.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'embeddings.word_embeddings.char_conv_4.bias', 'embeddings.word_embeddings._projection.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'embeddings.word_embeddings.char_conv_5.bias', 'encoder.layer.11.intermediate.dense.weight', 'embeddings.word_embeddings.char_conv_6.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'embeddings.word_embeddings._projection.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'embeddings.word_embeddings.char_conv_2.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'embeddings.word_embeddings.char_conv_1.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'embeddings.word_embeddings.char_conv_0.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.3.attention.self.key.weight', 'embeddings.word_embeddings.char_conv_4.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.4.attention.self.query.weight', 'embeddings.word_embeddings._highways._layers.0.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'pooler.dense.weight', 'encoder.layer.8.attention.self.value.bias', 'embeddings.word_embeddings.char_conv_0.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.5.attention.self.value.bias', 'embeddings.word_embeddings._highways._layers.1.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#### REPLACING BERT WITH CHARACTER_BERT ####\n",
    "\n",
    "character_bert_model = CharacterBertModel.from_pretrained(\"models/bert-unigram-bengali-classifier\")\n",
    "model.bert = character_bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "2bb49f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharacterCnn(\n",
       "  (char_conv_0): Conv1d(16, 32, kernel_size=(1,), stride=(1,))\n",
       "  (char_conv_1): Conv1d(16, 32, kernel_size=(2,), stride=(1,))\n",
       "  (char_conv_2): Conv1d(16, 64, kernel_size=(3,), stride=(1,))\n",
       "  (char_conv_3): Conv1d(16, 128, kernel_size=(4,), stride=(1,))\n",
       "  (char_conv_4): Conv1d(16, 256, kernel_size=(5,), stride=(1,))\n",
       "  (char_conv_5): Conv1d(16, 512, kernel_size=(6,), stride=(1,))\n",
       "  (char_conv_6): Conv1d(16, 1024, kernel_size=(7,), stride=(1,))\n",
       "  (_highways): Highway(\n",
       "    (_layers): ModuleList(\n",
       "      (0-1): 2 x Linear(in_features=2048, out_features=4096, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (_projection): Linear(in_features=2048, out_features=768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bert.embeddings.word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "d323c574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): CharacterBertModel(\n",
       "    (embeddings): CharacterBertEmbeddings(\n",
       "      (word_embeddings): CharacterCnn(\n",
       "        (char_conv_0): Conv1d(16, 32, kernel_size=(1,), stride=(1,))\n",
       "        (char_conv_1): Conv1d(16, 32, kernel_size=(2,), stride=(1,))\n",
       "        (char_conv_2): Conv1d(16, 64, kernel_size=(3,), stride=(1,))\n",
       "        (char_conv_3): Conv1d(16, 128, kernel_size=(4,), stride=(1,))\n",
       "        (char_conv_4): Conv1d(16, 256, kernel_size=(5,), stride=(1,))\n",
       "        (char_conv_5): Conv1d(16, 512, kernel_size=(6,), stride=(1,))\n",
       "        (char_conv_6): Conv1d(16, 1024, kernel_size=(7,), stride=(1,))\n",
       "        (_highways): Highway(\n",
       "          (_layers): ModuleList(\n",
       "            (0-1): 2 x Linear(in_features=2048, out_features=4096, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (_projection): Linear(in_features=2048, out_features=768, bias=True)\n",
       "      )\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): CharacterBertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x CharacterBertLayer(\n",
       "          (attention): CharacterBertAttention(\n",
       "            (self): CharacterBertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): CharacterBertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): CharacterBertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): CharacterBertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): CharacterBertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = AutoModelForSequenceClassification.from_pretrained(\"models/bert-unigram-bengali-classifier\")\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7045df9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
