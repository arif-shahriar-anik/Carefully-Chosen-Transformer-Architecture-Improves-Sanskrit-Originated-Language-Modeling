{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26204d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertConfig, CharacterBertModel, \\\n",
    "CharacterBertForPreTraining, CharacterBertConfig, CharacterBertTokenizer\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc26e690",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"Bangla-NER-Splitted-Dataset.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "770a2bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "  \n",
    "# Opening JSON file\n",
    "f = open(file_path, mode=\"r\", encoding=\"utf-8\")\n",
    "  \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2fe11e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['test', 'train', 'validation'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a5a5fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_json_data = json_data['train']\n",
    "val_json_data = json_data['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2efe2e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_json_data = json_data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c6f2274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "train_df = pd.DataFrame(columns=['ner_tags', 'tokens'])\n",
    "val_df = pd.DataFrame(columns=['ner_tags', 'tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "749a4d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(columns=['ner_tags', 'tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05bad6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_file(json_file, df):\n",
    "    token_docs = []\n",
    "    tag_docs = []\n",
    "\n",
    "    for idx,doc in enumerate(json_file):\n",
    "        tokens = doc['sentence']\n",
    "        tags = doc['iob_tags']\n",
    "        token_docs.append(tokens)\n",
    "        tag_docs.append(tags)\n",
    "        df.loc[idx] = pd.Series({'ner_tags':tags, 'tokens':tokens})\n",
    "    \n",
    "    return df, token_docs, tag_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8151e197",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df,_,tag_docs = read_json_file(train_json_data, train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ee12907",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df,_,_ = read_json_file(val_json_data, val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59bcd957",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df,_,_ = read_json_file(test_json_data, val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f8dd7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[O, O, O, B-PER, B-PER, I-PER, I-PER, O, O, O,...</td>\n",
       "      <td>[ত্রাণ, ও, সমাজকল্যাণ, সম্পাদক, সুজিত, রায়, নন...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            ner_tags  \\\n",
       "0  [O, O, O, B-PER, B-PER, I-PER, I-PER, O, O, O,...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [ত্রাণ, ও, সমাজকল্যাণ, সম্পাদক, সুজিত, রায়, নন...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17837784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[O, O, O, B-LOC, O, O, O, O]</td>\n",
       "      <td>[৫%, তার, চাইতে, পশ্চিমোরে, এর, সাক্ষরতার, হার...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       ner_tags  \\\n",
       "0  [O, O, O, B-LOC, O, O, O, O]   \n",
       "\n",
       "                                              tokens  \n",
       "0  [৫%, তার, চাইতে, পশ্চিমোরে, এর, সাক্ষরতার, হার...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43231717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[O, O, O, B-LOC, O, O, O, O]</td>\n",
       "      <td>[৫%, তার, চাইতে, পশ্চিমোরে, এর, সাক্ষরতার, হার...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       ner_tags  \\\n",
       "0  [O, O, O, B-LOC, O, O, O, O]   \n",
       "\n",
       "                                              tokens  \n",
       "0  [৫%, তার, চাইতে, পশ্চিমোরে, এর, সাক্ষরতার, হার...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c04ab588",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "val_ds = Dataset.from_pandas(val_df)\n",
    "test_ds = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2e0e68a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = DatasetDict()\n",
    "datasets['train'] = train_ds\n",
    "datasets['validation'] = val_ds\n",
    "datasets['test'] = test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "aea65542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['পরিকল্পনা',\n",
       " 'অনুযায়ী',\n",
       " 'তারা',\n",
       " 'বাসায়',\n",
       " 'ঢুকে',\n",
       " 'দুই',\n",
       " 'অতিথিকে',\n",
       " 'নগ্ন',\n",
       " 'করে',\n",
       " 'তাদের',\n",
       " 'মাঝখানে',\n",
       " 'এক',\n",
       " 'ছাত্রীকে',\n",
       " 'বসিয়ে',\n",
       " 'ছবি',\n",
       " 'তোলেন']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][1][\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e6186007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'B-OBJ',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'O',\n",
       " 'B-OBJ',\n",
       " 'O']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][1][\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "043d58e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create set from list\n",
    "expanded_tag_docs = []\n",
    "\n",
    "for tags in tag_docs:\n",
    "    for tag in tags:\n",
    "        expanded_tag_docs.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "38e07600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-LOC', 'B-OBJ', 'B-ORG', 'B-PER', 'I-LOC', 'I-OBJ', 'I-ORG', 'I-PER', 'O'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_tags = set(expanded_tag_docs)\n",
    "unique_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "752dbf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_label(examples):\n",
    "    mapping = {'B-LOC':0, 'B-OBJ':1, 'B-ORG':2, 'B-PER':3, 'I-LOC':4, 'I-OBJ':5, 'I-ORG':6, 'I-PER':7, 'O':8}\n",
    "    ner_labels = []\n",
    "    for example in examples[\"ner_tags\"]:\n",
    "        ner_labels.append(mapping[example])\n",
    "    examples[\"ner_labels\"] = ner_labels\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bbb87bf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/64155 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3565 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3565 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasets = datasets.map(assign_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6eff5a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = CharacterBertTokenizer(strip_accents=None, do_lower_case=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6751623b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(' '.join(datasets[\"train\"][0][\"tokens\"]))\n",
    "# inputs = tokenizer('ত্রাণ ও সমাজকল্যাণ সম্পাদক সুজিত রায় নন্দী রমুখ সংবাদ সম্মেলনে উপস্থিত ছিলেন')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c3adb8ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ত্রাণ',\n",
       " 'ও',\n",
       " 'সমাজকল্যাণ',\n",
       " 'সম্পাদক',\n",
       " 'সুজিত',\n",
       " 'রায়',\n",
       " 'নন্দী',\n",
       " 'প্রমুখ',\n",
       " 'সংবাদ',\n",
       " 'সম্মেলনে',\n",
       " 'উপস্থিত',\n",
       " 'ছিলেন']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][0][\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ac826761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    for word_id in word_ids:\n",
    "        if word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            new_labels.append(labels[word_id])\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7f1388d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_ids(input_tokens):\n",
    "    word_ids = []\n",
    "    count = 0\n",
    "    special_tokens_list = [tokenizer.pad_token_id, tokenizer.unk_token_id, tokenizer.cls_token_id, tokenizer.mask_token_id,\\\n",
    "                       tokenizer.unk_token_id, tokenizer.cls_token_id, tokenizer.sep_token_id]\n",
    "    for input_token in input_tokens:\n",
    "        if input_token in special_tokens_list:\n",
    "            word_id = None\n",
    "            word_ids.append(word_id)\n",
    "        else:\n",
    "            word_id = count\n",
    "            word_ids.append(word_id)\n",
    "            count += 1\n",
    "    return word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "00327d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, None]\n",
      "[8, 8, 8, 3, 3, 7, 7, 8, 8, 8, 8, 8]\n",
      "[-100, 8, 8, 8, 3, 3, 7, 7, 8, 8, 8, 8, 8, -100]\n"
     ]
    }
   ],
   "source": [
    "labels = datasets[\"train\"][0][\"ner_labels\"]\n",
    "word_ids = get_word_ids(inputs['input_ids'])\n",
    "print(word_ids)\n",
    "print(labels)\n",
    "print(align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0dd7f08d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'O', 'O', 'B-PER', 'B-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][0]['ner_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b79e80d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-LOC', 'B-OBJ', 'B-ORG', 'B-PER', 'I-LOC', 'I-OBJ', 'I-ORG', 'I-PER', 'O']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping = {'B-LOC':0, 'B-OBJ':1, 'B-ORG':2, 'B-PER':3, 'I-LOC':4, 'I-OBJ':5, 'I-ORG':6, 'I-PER':7, 'O':8}\n",
    "label_names = list()\n",
    "for k, v in mapping.items():\n",
    "    label_names.append(k)\n",
    "\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "26f55a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_and_align_labels(examples):\n",
    "#     temp_examples = [' '.join(example) for example in examples[\"tokens\"]]\n",
    "#     #print(temp_examples)\n",
    "#     tokenized_inputs = tokenizer(\n",
    "#         temp_examples\n",
    "#     )\n",
    "#     all_labels = examples[\"ner_labels\"]\n",
    "#     new_labels = []\n",
    "#     for i, labels in enumerate(all_labels):\n",
    "#         word_ids = get_word_ids(tokenized_inputs['input_ids'][i])\n",
    "#         print(len(word_ids))\n",
    "#         new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "#         print(len(labels))\n",
    "\n",
    "#     tokenized_inputs[\"labels\"] = new_labels\n",
    "#     return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e9272202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    temp_examples = [' '.join(example) for example in examples[\"tokens\"]]\n",
    "    #print(temp_examples)\n",
    "    tokenized_inputs = tokenizer(\n",
    "        temp_examples, max_length=128\n",
    "    )\n",
    "    \n",
    "    all_labels = examples[\"ner_labels\"]\n",
    "    new_labels = []\n",
    "    \n",
    "    for i, labels in enumerate(all_labels):\n",
    "        new_labels.append([-100]+labels+[-100])\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2a63ce7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = datasets[\"train\"][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d4cf3542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ত্রাণ',\n",
       " 'ও',\n",
       " 'সমাজকল্যাণ',\n",
       " 'সম্পাদক',\n",
       " 'সুজিত',\n",
       " 'রায়',\n",
       " 'নন্দী',\n",
       " 'প্রমুখ',\n",
       " 'সংবাদ',\n",
       " 'সম্মেলনে',\n",
       " 'উপস্থিত',\n",
       " 'ছিলেন']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples['tokens'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0af2176d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[[259, 257, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261], [259, 225, 167, 165, 225, 168, 142, 225, 167, 177, 225, 167, 191, 225, 167, 164, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261], [259, 225, 167, 148, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261], [259, 225, 167, 185, 225, 167, 175, 225, 167, 191, 225, 167, 157, 225, 167, 150, 225, 167, 179, 225, 168, 142, 225, 167, 176, 225, 167, 191, 225, 167, 164, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261], [259, 225, 167, 185, 225, 167, 175, 225, 168, 142, 225, 167, 171, 225, 167, 191, 225, 167, 167, 225, 167, 150, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261], [259, 225, 167, 185, 225, 168, 130, 225, 167, 157, 225, 167, 192, 225, 167, 165, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261], [259, 225, 167, 177, 225, 167, 191, 225, 168, 160, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261], [259, 225, 167, 169, 225, 167, 169, 225, 168, 142, 225, 167, 167, 225, 168, 129, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261], [259, 225, 167, 171, 225, 168, 142, 225, 167, 177, 225, 167, 175, 225, 168, 130, 225, 167, 151, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261], [259, 225, 167, 185, 225, 167, 131, 225, 167, 173, 225, 167, 191, 225, 167, 167, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261], [259, 225, 167, 185, 225, 167, 175, 225, 168, 142, 225, 167, 175, 225, 168, 136, 225, 167, 179, 225, 167, 169, 225, 168, 136, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261], [259, 225, 167, 138, 225, 167, 171, 225, 167, 185, 225, 168, 142, 225, 167, 166, 225, 167, 192, 225, 167, 165, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261], [259, 225, 167, 156, 225, 167, 192, 225, 167, 179, 225, 168, 136, 225, 167, 169, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261], [259, 258, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261]], [[259, 257, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261], [259, 225, 167, 171, 225, 167, 177, 225, 167, 192, 225, 167, 150, 225, 167, 179, 225, 168, 142, 225, 167, 171, 225, 167, 169, 225, 167, 191, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261], [259, 225, 167, 134, 225, 167, 169, 225, 168, 130, 225, 167, 176, 225, 167, 191, 225, 168, 160, 225, 168, 129, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261], [259, 225, 167, 165, 225, 167, 191, 225, 167, 177, 225, 167, 191, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261], [259, 225, 167, 173, 225, 167, 191, 225, 167, 185, 225, 167, 191, 225, 168, 160, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261], [259, 225, 167, 163, 225, 168, 130, 225, 167, 150, 225, 168, 136, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261], [259, 225, 167, 167, 225, 168, 130, 225, 167, 136, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261], [259, 225, 167, 134, 225, 167, 165, 225, 167, 192, 225, 167, 166, 225, 167, 192, 225, 167, 150, 225, 168, 136, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261], [259, 225, 167, 169, 225, 167, 152, 225, 168, 142, 225, 167, 169, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261], [259, 225, 167, 150, 225, 167, 177, 225, 168, 136, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261], [259, 225, 167, 165, 225, 167, 191, 225, 167, 167, 225, 168, 136, 225, 167, 177, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261], [259, 225, 167, 175, 225, 167, 191, 225, 167, 158, 225, 167, 151, 225, 167, 191, 225, 167, 169, 225, 168, 136, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261], [259, 225, 167, 144, 225, 167, 150, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261], [259, 225, 167, 156, 225, 167, 191, 225, 167, 165, 225, 168, 142, 225, 167, 177, 225, 168, 129, 225, 167, 150, 225, 168, 136, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261], [259, 225, 167, 173, 225, 167, 185, 225, 167, 192, 225, 168, 160, 225, 168, 136, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261], [259, 225, 167, 156, 225, 167, 173, 225, 167, 192, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261], [259, 225, 167, 165, 225, 168, 140, 225, 167, 179, 225, 168, 136, 225, 167, 169, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261], [259, 258, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261]]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 8, 8, 8, 3, 3, 7, 7, 8, 8, 8, 8, 8, -100], [-100, 8, 8, 3, 1, 8, 8, 3, 8, 8, 3, 8, 8, 3, 8, 1, 8, -100]]}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_and_align_labels(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4ab34ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/64155 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3565 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3565 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=datasets[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4b702bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 64155\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3565\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3565\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "150de5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7ffd947c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100,    8,    8,    8,    3,    3,    7,    7,    8,    8,    8,    8,\n",
       "            8, -100, -100, -100, -100, -100],\n",
       "        [-100,    8,    8,    3,    1,    8,    8,    3,    8,    8,    3,    8,\n",
       "            8,    3,    8,    1,    8, -100]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(2)])\n",
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "db94d940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 8, 8, 8, 3, 3, 7, 7, 8, 8, 8, 8, 8, -100]\n",
      "[-100, 8, 8, 3, 1, 8, 8, 3, 8, 8, 3, 8, 8, 3, 8, 1, 8, -100]\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    print(tokenized_datasets[\"train\"][i][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "05c70eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "381bce00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cb356d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'O', 'O', 'B-PER', 'B-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = datasets[\"train\"][0][\"ner_labels\"]\n",
    "labels = [label_names[i] for i in labels]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f8ffd02f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PER': {'precision': 0.6666666666666666,\n",
       "  'recall': 1.0,\n",
       "  'f1': 0.8,\n",
       "  'number': 2},\n",
       " 'overall_precision': 0.6666666666666666,\n",
       " 'overall_recall': 1.0,\n",
       " 'overall_f1': 0.8,\n",
       " 'overall_accuracy': 0.9166666666666666}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = labels.copy()\n",
    "predictions[2] = 'B-PER'\n",
    "metric.compute(predictions=[predictions], references=[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3fd8db7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b108e951",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a9b5a63e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'B-LOC',\n",
       " 1: 'B-OBJ',\n",
       " 2: 'B-ORG',\n",
       " 3: 'B-PER',\n",
       " 4: 'I-LOC',\n",
       " 5: 'I-OBJ',\n",
       " 6: 'I-ORG',\n",
       " 7: 'I-PER',\n",
       " 8: 'O'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9d741805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-LOC': 0,\n",
       " 'B-OBJ': 1,\n",
       " 'B-ORG': 2,\n",
       " 'B-PER': 3,\n",
       " 'I-LOC': 4,\n",
       " 'I-OBJ': 5,\n",
       " 'I-ORG': 6,\n",
       " 'I-PER': 7,\n",
       " 'O': 8}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bae49bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\arifa/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"B-LOC\",\n",
      "    \"1\": \"B-OBJ\",\n",
      "    \"2\": \"B-ORG\",\n",
      "    \"3\": \"B-PER\",\n",
      "    \"4\": \"I-LOC\",\n",
      "    \"5\": \"I-OBJ\",\n",
      "    \"6\": \"I-ORG\",\n",
      "    \"7\": \"I-PER\",\n",
      "    \"8\": \"O\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-LOC\": 0,\n",
      "    \"B-OBJ\": 1,\n",
      "    \"B-ORG\": 2,\n",
      "    \"B-PER\": 3,\n",
      "    \"I-LOC\": 4,\n",
      "    \"I-OBJ\": 5,\n",
      "    \"I-ORG\": 6,\n",
      "    \"I-PER\": 7,\n",
      "    \"O\": 8\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\arifa/.cache\\huggingface\\transformers\\a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#### LOADING BERT FOR CLASSIFICATION ####\n",
    "\n",
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3c02475c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file E:\\Documents\\Character Bert\\Question Classification\\character-bert\\config.json\n",
      "Model config CharacterBertConfig {\n",
      "  \"_name_or_path\": \"helboukkouri/character-bert\",\n",
      "  \"architectures\": [\n",
      "    \"CharacterBertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_character_bert.CharacterBertConfig\",\n",
      "    \"AutoModel\": \"modeling_character_bert.CharacterBertForPreTraining\",\n",
      "    \"AutoModelForMaskedLM\": \"modeling_character_bert.CharacterBertForMaskedLM\"\n",
      "  },\n",
      "  \"character_embeddings_dim\": 16,\n",
      "  \"cnn_activation\": \"relu\",\n",
      "  \"cnn_filters\": [\n",
      "    [\n",
      "      1,\n",
      "      32\n",
      "    ],\n",
      "    [\n",
      "      2,\n",
      "      32\n",
      "    ],\n",
      "    [\n",
      "      3,\n",
      "      64\n",
      "    ],\n",
      "    [\n",
      "      4,\n",
      "      128\n",
      "    ],\n",
      "    [\n",
      "      5,\n",
      "      256\n",
      "    ],\n",
      "    [\n",
      "      6,\n",
      "      512\n",
      "    ],\n",
      "    [\n",
      "      7,\n",
      "      1024\n",
      "    ]\n",
      "  ],\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_word_length\": 50,\n",
      "  \"mlm_vocab_size\": 30522,\n",
      "  \"model_type\": \"character_bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_highway_layers\": 2,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true\n",
      "}\n",
      "\n",
      "loading weights file E:\\Documents\\Character Bert\\Question Classification\\character-bert\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at E:\\Documents\\Character Bert\\Question Classification\\character-bert were not used when initializing CharacterBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing CharacterBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CharacterBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of CharacterBertModel were initialized from the model checkpoint at E:\\Documents\\Character Bert\\Question Classification\\character-bert.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use CharacterBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#### REPLACING BERT WITH CHARACTER_BERT ####\n",
    "\n",
    "character_bert_model = CharacterBertModel.from_pretrained(\n",
    "    \"E:\\Documents\\Character Bert\\Question Classification\\character-bert\")\n",
    "model.bert = character_bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d1575043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharacterCnn(\n",
       "  (char_conv_0): Conv1d(16, 32, kernel_size=(1,), stride=(1,))\n",
       "  (char_conv_1): Conv1d(16, 32, kernel_size=(2,), stride=(1,))\n",
       "  (char_conv_2): Conv1d(16, 64, kernel_size=(3,), stride=(1,))\n",
       "  (char_conv_3): Conv1d(16, 128, kernel_size=(4,), stride=(1,))\n",
       "  (char_conv_4): Conv1d(16, 256, kernel_size=(5,), stride=(1,))\n",
       "  (char_conv_5): Conv1d(16, 512, kernel_size=(6,), stride=(1,))\n",
       "  (char_conv_6): Conv1d(16, 1024, kernel_size=(7,), stride=(1,))\n",
       "  (_highways): Highway(\n",
       "    (_layers): ModuleList(\n",
       "      (0-1): 2 x Linear(in_features=2048, out_features=4096, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (_projection): Linear(in_features=2048, out_features=768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bert.embeddings.word_embeddings  # wordpieces are replaced with a CharacterCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d757b3ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "888e5143",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  disable weights and biases logging\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7b04e011",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "batch_size = 32\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"models/ner\",\n",
    "    report_to = None,\n",
    "    logging_dir= None,\n",
    "    save_strategy=\"no\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    #save_strategy=\"epoch\",\n",
    "    #learning_rate=2e-5,\n",
    "    learning_rate=3e-5,\n",
    "    #num_train_epochs=4,\n",
    "    num_train_epochs=3,\n",
    "    warmup_ratio = 0.06,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f0435c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "15020690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "trainer.remove_callback(transformers.integrations.TensorBoardCallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f1c286dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3565\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='224' max='112' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [112/112 07:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.238935708999634,\n",
       " 'eval_precision': 0.029888284494355735,\n",
       " 'eval_recall': 0.13650327233872045,\n",
       " 'eval_f1': 0.04903913053909455,\n",
       " 'eval_accuracy': 0.15092304247827237,\n",
       " 'eval_runtime': 9.3275,\n",
       " 'eval_samples_per_second': 382.204,\n",
       " 'eval_steps_per_second': 12.008}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "df7b7e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 64155\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6015\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6015' max='6015' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6015/6015 21:57, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.284300</td>\n",
       "      <td>0.275357</td>\n",
       "      <td>0.684292</td>\n",
       "      <td>0.646454</td>\n",
       "      <td>0.664835</td>\n",
       "      <td>0.902648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.236300</td>\n",
       "      <td>0.247792</td>\n",
       "      <td>0.708327</td>\n",
       "      <td>0.655536</td>\n",
       "      <td>0.680910</td>\n",
       "      <td>0.908612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.195800</td>\n",
       "      <td>0.240454</td>\n",
       "      <td>0.702036</td>\n",
       "      <td>0.695472</td>\n",
       "      <td>0.698739</td>\n",
       "      <td>0.912866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3565\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3565\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3565\n",
      "  Batch size = 32\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6015, training_loss=0.2646684057991998, metrics={'train_runtime': 1318.1278, 'train_samples_per_second': 146.014, 'train_steps_per_second': 4.563, 'total_flos': 1.6044202762080662e+17, 'train_loss': 0.2646684057991998, 'epoch': 3.0})"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6567a706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "421a9484",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3565\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='112' max='112' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [112/112 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.24045395851135254,\n",
       " 'eval_precision': 0.7020358635566941,\n",
       " 'eval_recall': 0.6954721517296647,\n",
       " 'eval_f1': 0.6987385936661299,\n",
       " 'eval_accuracy': 0.9128656042010136,\n",
       " 'eval_runtime': 9.7401,\n",
       " 'eval_samples_per_second': 366.012,\n",
       " 'eval_steps_per_second': 11.499,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(tokenized_datasets[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "53d5bbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 3565\n",
      "  Batch size = 32\n"
     ]
    }
   ],
   "source": [
    "y_preds, y_true, _ = trainer.predict(tokenized_datasets[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ddf2d768",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.argmax(y_preds, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9807b7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove ignored index (special tokens) and convert to labels\n",
    "true_labels = [[label_names[l] for l in label if l != -100] for label in y_true]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "44836106",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, y_true)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b035386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified = [i for i in range(len(true_predictions)) if (true_predictions[i] != true_labels[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b0d97d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 12,\n",
       " 13,\n",
       " 15,\n",
       " 17,\n",
       " 19,\n",
       " 21,\n",
       " 23,\n",
       " 25,\n",
       " 28,\n",
       " 29,\n",
       " 32,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 38,\n",
       " 39,\n",
       " 44,\n",
       " 47,\n",
       " 49,\n",
       " 53,\n",
       " 56,\n",
       " 57,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 70,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 78,\n",
       " 79,\n",
       " 81,\n",
       " 83,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 95,\n",
       " 96,\n",
       " 98,\n",
       " 99,\n",
       " 101,\n",
       " 102,\n",
       " 104,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 130,\n",
       " 131,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 138,\n",
       " 139,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 159,\n",
       " 160,\n",
       " 164,\n",
       " 165,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 188,\n",
       " 190,\n",
       " 191,\n",
       " 194,\n",
       " 195,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 202,\n",
       " 203,\n",
       " 207,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 214,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 222,\n",
       " 227,\n",
       " 228,\n",
       " 230,\n",
       " 232,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 244,\n",
       " 246,\n",
       " 248,\n",
       " 250,\n",
       " 256,\n",
       " 258,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 267,\n",
       " 271,\n",
       " 273,\n",
       " 274,\n",
       " 275,\n",
       " 277,\n",
       " 278,\n",
       " 280,\n",
       " 283,\n",
       " 287,\n",
       " 289,\n",
       " 293,\n",
       " 294,\n",
       " 296,\n",
       " 298,\n",
       " 300,\n",
       " 301,\n",
       " 303,\n",
       " 305,\n",
       " 307,\n",
       " 308,\n",
       " 311,\n",
       " 312,\n",
       " 313,\n",
       " 315,\n",
       " 316,\n",
       " 317,\n",
       " 318,\n",
       " 320,\n",
       " 321,\n",
       " 323,\n",
       " 324,\n",
       " 325,\n",
       " 326,\n",
       " 328,\n",
       " 329,\n",
       " 330,\n",
       " 331,\n",
       " 332,\n",
       " 334,\n",
       " 335,\n",
       " 337,\n",
       " 339,\n",
       " 341,\n",
       " 343,\n",
       " 345,\n",
       " 346,\n",
       " 347,\n",
       " 349,\n",
       " 350,\n",
       " 351,\n",
       " 352,\n",
       " 353,\n",
       " 354,\n",
       " 356,\n",
       " 357,\n",
       " 358,\n",
       " 362,\n",
       " 364,\n",
       " 369,\n",
       " 370,\n",
       " 371,\n",
       " 373,\n",
       " 374,\n",
       " 375,\n",
       " 377,\n",
       " 381,\n",
       " 382,\n",
       " 384,\n",
       " 385,\n",
       " 386,\n",
       " 387,\n",
       " 390,\n",
       " 391,\n",
       " 396,\n",
       " 397,\n",
       " 401,\n",
       " 402,\n",
       " 404,\n",
       " 405,\n",
       " 407,\n",
       " 409,\n",
       " 410,\n",
       " 411,\n",
       " 413,\n",
       " 414,\n",
       " 416,\n",
       " 417,\n",
       " 418,\n",
       " 420,\n",
       " 421,\n",
       " 427,\n",
       " 431,\n",
       " 434,\n",
       " 435,\n",
       " 438,\n",
       " 439,\n",
       " 440,\n",
       " 442,\n",
       " 447,\n",
       " 448,\n",
       " 449,\n",
       " 454,\n",
       " 457,\n",
       " 458,\n",
       " 459,\n",
       " 460,\n",
       " 461,\n",
       " 465,\n",
       " 466,\n",
       " 467,\n",
       " 468,\n",
       " 470,\n",
       " 471,\n",
       " 474,\n",
       " 475,\n",
       " 478,\n",
       " 481,\n",
       " 482,\n",
       " 483,\n",
       " 484,\n",
       " 487,\n",
       " 489,\n",
       " 490,\n",
       " 492,\n",
       " 493,\n",
       " 494,\n",
       " 495,\n",
       " 497,\n",
       " 499,\n",
       " 501,\n",
       " 502,\n",
       " 503,\n",
       " 504,\n",
       " 505,\n",
       " 508,\n",
       " 509,\n",
       " 511,\n",
       " 512,\n",
       " 513,\n",
       " 515,\n",
       " 518,\n",
       " 520,\n",
       " 521,\n",
       " 523,\n",
       " 525,\n",
       " 527,\n",
       " 530,\n",
       " 531,\n",
       " 532,\n",
       " 533,\n",
       " 535,\n",
       " 536,\n",
       " 540,\n",
       " 541,\n",
       " 543,\n",
       " 545,\n",
       " 546,\n",
       " 547,\n",
       " 548,\n",
       " 549,\n",
       " 552,\n",
       " 553,\n",
       " 554,\n",
       " 555,\n",
       " 556,\n",
       " 557,\n",
       " 561,\n",
       " 562,\n",
       " 567,\n",
       " 574,\n",
       " 576,\n",
       " 578,\n",
       " 580,\n",
       " 581,\n",
       " 582,\n",
       " 583,\n",
       " 584,\n",
       " 585,\n",
       " 586,\n",
       " 589,\n",
       " 591,\n",
       " 597,\n",
       " 598,\n",
       " 600,\n",
       " 603,\n",
       " 604,\n",
       " 605,\n",
       " 606,\n",
       " 607,\n",
       " 608,\n",
       " 611,\n",
       " 612,\n",
       " 613,\n",
       " 615,\n",
       " 616,\n",
       " 617,\n",
       " 620,\n",
       " 622,\n",
       " 623,\n",
       " 624,\n",
       " 625,\n",
       " 626,\n",
       " 628,\n",
       " 629,\n",
       " 633,\n",
       " 635,\n",
       " 637,\n",
       " 638,\n",
       " 639,\n",
       " 641,\n",
       " 643,\n",
       " 646,\n",
       " 647,\n",
       " 650,\n",
       " 651,\n",
       " 652,\n",
       " 657,\n",
       " 660,\n",
       " 661,\n",
       " 662,\n",
       " 666,\n",
       " 670,\n",
       " 671,\n",
       " 672,\n",
       " 676,\n",
       " 679,\n",
       " 680,\n",
       " 681,\n",
       " 682,\n",
       " 683,\n",
       " 685,\n",
       " 686,\n",
       " 687,\n",
       " 689,\n",
       " 696,\n",
       " 698,\n",
       " 699,\n",
       " 700,\n",
       " 701,\n",
       " 702,\n",
       " 703,\n",
       " 706,\n",
       " 707,\n",
       " 708,\n",
       " 709,\n",
       " 712,\n",
       " 713,\n",
       " 714,\n",
       " 715,\n",
       " 716,\n",
       " 718,\n",
       " 722,\n",
       " 725,\n",
       " 726,\n",
       " 729,\n",
       " 730,\n",
       " 731,\n",
       " 734,\n",
       " 737,\n",
       " 738,\n",
       " 739,\n",
       " 740,\n",
       " 741,\n",
       " 745,\n",
       " 746,\n",
       " 747,\n",
       " 748,\n",
       " 749,\n",
       " 751,\n",
       " 752,\n",
       " 753,\n",
       " 755,\n",
       " 758,\n",
       " 759,\n",
       " 761,\n",
       " 762,\n",
       " 764,\n",
       " 765,\n",
       " 766,\n",
       " 767,\n",
       " 769,\n",
       " 771,\n",
       " 772,\n",
       " 778,\n",
       " 781,\n",
       " 782,\n",
       " 783,\n",
       " 785,\n",
       " 786,\n",
       " 787,\n",
       " 788,\n",
       " 789,\n",
       " 793,\n",
       " 794,\n",
       " 799,\n",
       " 800,\n",
       " 801,\n",
       " 804,\n",
       " 808,\n",
       " 810,\n",
       " 812,\n",
       " 813,\n",
       " 814,\n",
       " 815,\n",
       " 820,\n",
       " 822,\n",
       " 824,\n",
       " 825,\n",
       " 827,\n",
       " 828,\n",
       " 832,\n",
       " 835,\n",
       " 836,\n",
       " 839,\n",
       " 840,\n",
       " 848,\n",
       " 851,\n",
       " 852,\n",
       " 853,\n",
       " 854,\n",
       " 856,\n",
       " 857,\n",
       " 860,\n",
       " 861,\n",
       " 863,\n",
       " 868,\n",
       " 869,\n",
       " 870,\n",
       " 871,\n",
       " 872,\n",
       " 878,\n",
       " 879,\n",
       " 880,\n",
       " 881,\n",
       " 883,\n",
       " 885,\n",
       " 888,\n",
       " 891,\n",
       " 892,\n",
       " 893,\n",
       " 894,\n",
       " 896,\n",
       " 898,\n",
       " 899,\n",
       " 900,\n",
       " 901,\n",
       " 902,\n",
       " 903,\n",
       " 904,\n",
       " 906,\n",
       " 908,\n",
       " 909,\n",
       " 910,\n",
       " 911,\n",
       " 912,\n",
       " 913,\n",
       " 915,\n",
       " 917,\n",
       " 920,\n",
       " 922,\n",
       " 923,\n",
       " 924,\n",
       " 926,\n",
       " 928,\n",
       " 932,\n",
       " 933,\n",
       " 934,\n",
       " 937,\n",
       " 939,\n",
       " 940,\n",
       " 942,\n",
       " 945,\n",
       " 946,\n",
       " 947,\n",
       " 948,\n",
       " 949,\n",
       " 952,\n",
       " 954,\n",
       " 955,\n",
       " 956,\n",
       " 957,\n",
       " 958,\n",
       " 959,\n",
       " 960,\n",
       " 962,\n",
       " 963,\n",
       " 966,\n",
       " 967,\n",
       " 969,\n",
       " 971,\n",
       " 972,\n",
       " 976,\n",
       " 978,\n",
       " 979,\n",
       " 980,\n",
       " 984,\n",
       " 988,\n",
       " 989,\n",
       " 990,\n",
       " 991,\n",
       " 993,\n",
       " 995,\n",
       " 998,\n",
       " 1000,\n",
       " 1001,\n",
       " 1003,\n",
       " 1004,\n",
       " 1005,\n",
       " 1009,\n",
       " 1012,\n",
       " 1013,\n",
       " 1014,\n",
       " 1015,\n",
       " 1020,\n",
       " 1021,\n",
       " 1022,\n",
       " 1023,\n",
       " 1024,\n",
       " 1025,\n",
       " 1026,\n",
       " 1029,\n",
       " 1030,\n",
       " 1031,\n",
       " 1032,\n",
       " 1034,\n",
       " 1038,\n",
       " 1039,\n",
       " 1040,\n",
       " 1042,\n",
       " 1046,\n",
       " 1047,\n",
       " 1049,\n",
       " 1054,\n",
       " 1055,\n",
       " 1056,\n",
       " 1059,\n",
       " 1060,\n",
       " 1062,\n",
       " 1064,\n",
       " 1065,\n",
       " 1066,\n",
       " 1067,\n",
       " 1069,\n",
       " 1073,\n",
       " 1075,\n",
       " 1077,\n",
       " 1079,\n",
       " 1080,\n",
       " 1081,\n",
       " 1082,\n",
       " 1083,\n",
       " 1087,\n",
       " 1092,\n",
       " 1093,\n",
       " 1095,\n",
       " 1096,\n",
       " 1097,\n",
       " 1098,\n",
       " 1099,\n",
       " 1100,\n",
       " 1101,\n",
       " 1104,\n",
       " 1106,\n",
       " 1112,\n",
       " 1116,\n",
       " 1118,\n",
       " 1119,\n",
       " 1121,\n",
       " 1123,\n",
       " 1129,\n",
       " 1130,\n",
       " 1133,\n",
       " 1135,\n",
       " 1136,\n",
       " 1140,\n",
       " 1143,\n",
       " 1145,\n",
       " 1146,\n",
       " 1147,\n",
       " 1148,\n",
       " 1150,\n",
       " 1151,\n",
       " 1157,\n",
       " 1158,\n",
       " 1163,\n",
       " 1164,\n",
       " 1166,\n",
       " 1167,\n",
       " 1168,\n",
       " 1169,\n",
       " 1171,\n",
       " 1172,\n",
       " 1174,\n",
       " 1176,\n",
       " 1177,\n",
       " 1179,\n",
       " 1182,\n",
       " 1183,\n",
       " 1185,\n",
       " 1186,\n",
       " 1187,\n",
       " 1188,\n",
       " 1189,\n",
       " 1190,\n",
       " 1193,\n",
       " 1194,\n",
       " 1195,\n",
       " 1198,\n",
       " 1199,\n",
       " 1200,\n",
       " 1203,\n",
       " 1204,\n",
       " 1205,\n",
       " 1210,\n",
       " 1213,\n",
       " 1218,\n",
       " 1219,\n",
       " 1220,\n",
       " 1221,\n",
       " 1225,\n",
       " 1227,\n",
       " 1230,\n",
       " 1231,\n",
       " 1232,\n",
       " 1234,\n",
       " 1235,\n",
       " 1238,\n",
       " 1241,\n",
       " 1243,\n",
       " 1244,\n",
       " 1245,\n",
       " 1246,\n",
       " 1247,\n",
       " 1248,\n",
       " 1251,\n",
       " 1252,\n",
       " 1254,\n",
       " 1255,\n",
       " 1258,\n",
       " 1263,\n",
       " 1265,\n",
       " 1266,\n",
       " 1267,\n",
       " 1268,\n",
       " 1269,\n",
       " 1271,\n",
       " 1272,\n",
       " 1273,\n",
       " 1275,\n",
       " 1277,\n",
       " 1278,\n",
       " 1281,\n",
       " 1282,\n",
       " 1283,\n",
       " 1284,\n",
       " 1285,\n",
       " 1289,\n",
       " 1290,\n",
       " 1291,\n",
       " 1292,\n",
       " 1293,\n",
       " 1294,\n",
       " 1295,\n",
       " 1296,\n",
       " 1298,\n",
       " 1303,\n",
       " 1307,\n",
       " 1309,\n",
       " 1313,\n",
       " 1314,\n",
       " 1316,\n",
       " 1318,\n",
       " 1322,\n",
       " 1325,\n",
       " 1326,\n",
       " 1329,\n",
       " 1330,\n",
       " 1331,\n",
       " 1332,\n",
       " 1333,\n",
       " 1334,\n",
       " 1336,\n",
       " 1337,\n",
       " 1339,\n",
       " 1341,\n",
       " 1342,\n",
       " 1344,\n",
       " 1345,\n",
       " 1346,\n",
       " 1347,\n",
       " 1348,\n",
       " 1353,\n",
       " 1358,\n",
       " 1360,\n",
       " 1361,\n",
       " 1362,\n",
       " 1363,\n",
       " 1364,\n",
       " 1367,\n",
       " 1368,\n",
       " 1369,\n",
       " 1370,\n",
       " 1371,\n",
       " 1372,\n",
       " 1374,\n",
       " 1375,\n",
       " 1377,\n",
       " 1378,\n",
       " 1379,\n",
       " 1381,\n",
       " 1383,\n",
       " 1384,\n",
       " 1386,\n",
       " 1387,\n",
       " 1388,\n",
       " 1390,\n",
       " 1391,\n",
       " 1393,\n",
       " 1394,\n",
       " 1397,\n",
       " 1398,\n",
       " 1400,\n",
       " 1404,\n",
       " 1405,\n",
       " 1406,\n",
       " 1409,\n",
       " 1415,\n",
       " 1416,\n",
       " 1417,\n",
       " 1419,\n",
       " 1421,\n",
       " 1422,\n",
       " 1423,\n",
       " 1426,\n",
       " 1427,\n",
       " 1429,\n",
       " 1430,\n",
       " 1431,\n",
       " 1432,\n",
       " 1433,\n",
       " 1435,\n",
       " 1436,\n",
       " 1437,\n",
       " 1438,\n",
       " 1439,\n",
       " 1442,\n",
       " 1443,\n",
       " 1444,\n",
       " 1445,\n",
       " 1447,\n",
       " 1448,\n",
       " 1451,\n",
       " 1453,\n",
       " 1454,\n",
       " 1457,\n",
       " 1458,\n",
       " 1459,\n",
       " 1460,\n",
       " 1461,\n",
       " 1462,\n",
       " 1463,\n",
       " 1465,\n",
       " 1466,\n",
       " 1468,\n",
       " 1469,\n",
       " 1470,\n",
       " 1474,\n",
       " 1476,\n",
       " 1477,\n",
       " 1482,\n",
       " 1483,\n",
       " 1487,\n",
       " 1488,\n",
       " 1490,\n",
       " 1491,\n",
       " 1492,\n",
       " 1494,\n",
       " 1495,\n",
       " 1499,\n",
       " 1500,\n",
       " 1501,\n",
       " 1503,\n",
       " 1506,\n",
       " 1511,\n",
       " 1512,\n",
       " 1514,\n",
       " 1515,\n",
       " 1516,\n",
       " 1519,\n",
       " 1522,\n",
       " 1524,\n",
       " 1529,\n",
       " 1530,\n",
       " 1532,\n",
       " 1534,\n",
       " 1535,\n",
       " 1537,\n",
       " 1538,\n",
       " 1539,\n",
       " 1542,\n",
       " 1543,\n",
       " 1545,\n",
       " 1548,\n",
       " 1549,\n",
       " 1550,\n",
       " 1551,\n",
       " 1552,\n",
       " 1554,\n",
       " 1555,\n",
       " 1558,\n",
       " 1560,\n",
       " 1561,\n",
       " 1564,\n",
       " 1568,\n",
       " 1572,\n",
       " 1574,\n",
       " 1575,\n",
       " 1576,\n",
       " 1578,\n",
       " 1579,\n",
       " 1581,\n",
       " 1585,\n",
       " 1586,\n",
       " 1587,\n",
       " 1588,\n",
       " 1589,\n",
       " 1590,\n",
       " 1594,\n",
       " 1595,\n",
       " 1596,\n",
       " 1597,\n",
       " 1598,\n",
       " 1599,\n",
       " 1600,\n",
       " 1601,\n",
       " 1602,\n",
       " 1606,\n",
       " 1611,\n",
       " 1613,\n",
       " 1614,\n",
       " 1615,\n",
       " 1618,\n",
       " 1619,\n",
       " 1625,\n",
       " 1629,\n",
       " 1630,\n",
       " 1633,\n",
       " 1635,\n",
       " 1637,\n",
       " 1639,\n",
       " 1641,\n",
       " 1645,\n",
       " 1646,\n",
       " 1647,\n",
       " 1648,\n",
       " 1649,\n",
       " 1650,\n",
       " 1656,\n",
       " 1659,\n",
       " 1660,\n",
       " 1665,\n",
       " 1666,\n",
       " 1668,\n",
       " 1670,\n",
       " 1671,\n",
       " 1673,\n",
       " 1674,\n",
       " 1677,\n",
       " 1678,\n",
       " 1679,\n",
       " 1681,\n",
       " 1682,\n",
       " 1684,\n",
       " 1685,\n",
       " 1686,\n",
       " 1687,\n",
       " 1689,\n",
       " 1690,\n",
       " 1691,\n",
       " 1692,\n",
       " 1693,\n",
       " 1695,\n",
       " 1696,\n",
       " 1697,\n",
       " 1698,\n",
       " 1699,\n",
       " 1702,\n",
       " 1705,\n",
       " 1706,\n",
       " 1707,\n",
       " 1708,\n",
       " 1709,\n",
       " 1710,\n",
       " 1711,\n",
       " 1712,\n",
       " 1715,\n",
       " 1716,\n",
       " 1718,\n",
       " 1719,\n",
       " 1720,\n",
       " 1723,\n",
       " 1724,\n",
       " 1725,\n",
       " 1726,\n",
       " 1728,\n",
       " 1729,\n",
       " 1731,\n",
       " 1732,\n",
       " 1733,\n",
       " 1734,\n",
       " 1735,\n",
       " 1737,\n",
       " 1738,\n",
       " 1741,\n",
       " 1742,\n",
       " 1743,\n",
       " 1744,\n",
       " 1745,\n",
       " 1746,\n",
       " 1748,\n",
       " 1749,\n",
       " 1750,\n",
       " 1753,\n",
       " 1755,\n",
       " 1756,\n",
       " 1758,\n",
       " 1759,\n",
       " 1761,\n",
       " 1762,\n",
       " 1763,\n",
       " 1764,\n",
       " 1765,\n",
       " 1766,\n",
       " 1767,\n",
       " 1769,\n",
       " 1770,\n",
       " 1772,\n",
       " 1773,\n",
       " 1777,\n",
       " 1782,\n",
       " 1783,\n",
       " 1785,\n",
       " ...]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8061568b",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = test_ds.select(misclassified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "abefd85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3210\n",
      "['বাঙালি', 'নদী', 'বাংলাদেশের', 'উত্তর-পশ্চিমাঞ্চলের', 'গাইবান্ধা', 'বগুড়া', 'এবং', 'সিরাজগঞ্জ', 'জেলার', 'একটি', 'নদী']\n"
     ]
    }
   ],
   "source": [
    "# search_query = 'বাংলাদেশ'\n",
    "# search_query = 'শান্ত'\n",
    "search_query = 'উত্তর-পশ্চিমাঞ্চলের'\n",
    "found_example = \"\"\n",
    "for index, example in zip(misclassified, temp['tokens']):\n",
    "    if search_query in example:\n",
    "        print(index)\n",
    "        print(example)\n",
    "        found_index = index\n",
    "        found_example = \" \".join(example)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5bdf44d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['বাঙালি',\n",
       " 'নদী',\n",
       " 'বাংলাদেশের',\n",
       " 'উত্তর',\n",
       " '-',\n",
       " 'পশ্চিমাঞ্চলের',\n",
       " 'গাইবান্ধা',\n",
       " 'বগুড়া',\n",
       " 'এবং',\n",
       " 'সিরাজগঞ্জ',\n",
       " 'জেলার',\n",
       " 'একটি',\n",
       " 'নদী']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(found_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "047b5e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-LOC',\n",
       " 'I-LOC',\n",
       " 'B-LOC',\n",
       " 'B-LOC',\n",
       " 'B-LOC',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LOC']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels[found_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ce358df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-LOC',\n",
       " 'I-LOC',\n",
       " 'B-LOC',\n",
       " 'B-LOC',\n",
       " 'I-LOC',\n",
       " 'I-LOC',\n",
       " 'B-LOC',\n",
       " 'B-LOC',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'B-LOC']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_predictions[found_index]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
