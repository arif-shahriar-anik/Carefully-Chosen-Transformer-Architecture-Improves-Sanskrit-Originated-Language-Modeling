{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7788b296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bb4637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ca8941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import math\n",
    "import random\n",
    "import logging\n",
    "import argparse\n",
    "import datetime\n",
    "import warnings\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data import (\n",
    "    DataLoader, RandomSampler, SequentialSampler, Dataset\n",
    ")\n",
    "\n",
    "# from apex.contrib.optimizers import FusedLAMB\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from schedulers import LinearWarmUpScheduler, PolyWarmUpScheduler\n",
    "from transformers import (\n",
    "    BertConfig, BertTokenizer, BertForPreTraining,\n",
    "    CharacterBertConfig, CharacterBertTokenizer, CharacterBertForPreTraining\n",
    ")\n",
    "\n",
    "from utils.distributed import is_main_process\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "WORKDIR = os.environ['WORKDIR']\n",
    "LOGGING_FORMAT = \"%(asctime)s | PID: %(process)d | %(filename)s | %(levelname)s - %(message)s\"\n",
    "logging.basicConfig(format=LOGGING_FORMAT, datefmt=\"%d/%m/%Y %H:%M:%S\", level=logging.INFO)\n",
    "\n",
    "IGNORE_INDEX = torch.nn.CrossEntropyLoss().ignore_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2679f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_all_random_seeds(random_seed: int, verbose: bool = True):\n",
    "    r\"\"\"Sets the initial random seed to a specific value.\"\"\"\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(random_seed)\n",
    "    if verbose:\n",
    "        logging.info(\"Setting random seed to: %d\", random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bad9f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainingDataset(Dataset):\n",
    "    r\"\"\"\n",
    "    PyTorch Dataset subclass that allows easy access to the pre-training\n",
    "    data previously stored in an .hdf5 file.\n",
    "    Args:\n",
    "    hdf5_fpath (:obj:`str`):\n",
    "        Path to an .hdf5 file contraining the pre-training data.\n",
    "    max_masked_tokens_per_input (:obj:`int`):\n",
    "        Hard limit on the number of masked tokens per input sequence.\n",
    "        This is therefore also a limit on the number of MLM predictions\n",
    "        per input sequence.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "        hdf5_fpath: str,\n",
    "        max_masked_tokens_per_input\n",
    "    ):\n",
    "        self.hdf5_fpath = hdf5_fpath\n",
    "        self.max_masked_tokens_per_input = max_masked_tokens_per_input\n",
    "        file_in = h5py.File(hdf5_fpath, \"r\")\n",
    "        keys = [\n",
    "            'input_ids',\n",
    "            'input_mask',\n",
    "            'segment_ids',\n",
    "            'masked_lm_positions',\n",
    "            'masked_lm_ids',\n",
    "            'next_sentence_labels'\n",
    "        ]\n",
    "        self.inputs = [np.asarray(file_in[key][:]) for key in keys]\n",
    "        file_in.close()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in the pre-training dataset.\"\"\"\n",
    "        return len(self.inputs[0])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns the sample at the provided index.\"\"\"\n",
    "        # Get elements at `index` as torch tensors\n",
    "        [\n",
    "            input_ids, input_mask, segment_ids,\n",
    "            masked_lm_positions, masked_lm_ids, next_sentence_labels\n",
    "        ] = [\n",
    "            torch.from_numpy(element[index].astype(np.int64)) if i < 5\n",
    "            else torch.from_numpy(np.asarray(element[index].astype(np.int64)))\n",
    "            for i, element in enumerate(self.inputs)\n",
    "        ]\n",
    "\n",
    "        # MLM labels is IGNORE_INDEX everywhere and `token_id` at masked positions\n",
    "        index = self.max_masked_tokens_per_input\n",
    "        masked_lm_labels = IGNORE_INDEX * torch.ones((input_ids.shape[0],), dtype=torch.long)\n",
    "        padded_mask_indices = (masked_lm_positions == 0).nonzero()\n",
    "        if len(padded_mask_indices) != 0:\n",
    "            index = padded_mask_indices[0].item()\n",
    "        masked_lm_labels[masked_lm_positions[:index]] = masked_lm_ids[:index]\n",
    "\n",
    "        return [\n",
    "            input_ids, segment_ids, input_mask,\n",
    "            masked_lm_labels, next_sentence_labels\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49569119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretraining_dataloader(\n",
    "        hdf5_fpath: str,\n",
    "        max_masked_tokens_per_input: int,\n",
    "        batch_size: int\n",
    "        ):\n",
    "        r\"\"\"\n",
    "        Makes a PyTorch DataLoader for producing random batches of pre-training\n",
    "        tensors using data stored in an .hdf5 file. This also returns the path\n",
    "        to the .hdf5 file for ... TODO: figure out why?\n",
    "        Args:\n",
    "        hdf5_fpath (:obj:`str`):\n",
    "            Path to an .hdf5 file contraining the pre-training data.\n",
    "        max_masked_tokens_per_input (:obj:`int`):\n",
    "            Hard limit on the number of masked tokens per input sequence.\n",
    "            This is therefore also a limit on the number of MLM predictions\n",
    "            per input sequence.\n",
    "        batch_size (:obj:`int`):\n",
    "            Batch size of tensors returned by the DataLoader.\n",
    "        \"\"\"\n",
    "        pretraining_data = PretrainingDataset(\n",
    "            hdf5_fpath=hdf5_fpath,\n",
    "            max_masked_tokens_per_input=max_masked_tokens_per_input\n",
    "        )\n",
    "        train_sampler = RandomSampler(pretraining_data)\n",
    "        train_dataloader = DataLoader(\n",
    "            pretraining_data,\n",
    "            sampler=train_sampler,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        return train_dataloader, hdf5_fpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309797f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "# Required parameters:\n",
    "# ---------------------------------------------------------------\n",
    "# - input/output dirs\n",
    "# - model config (BertConfig / CharacterBertConfig)\n",
    "# - a flag for pre-training CharacterBERT instead of BERT\n",
    "##################################################################\n",
    "max_vocabulary_size = 30522\n",
    "\n",
    "hdf5_directory = \"data\\hdf5\\80_12\"\n",
    "    \n",
    "\n",
    "output_directory = \"pretrained-models\\model\"\n",
    "\n",
    "\n",
    "is_character_bert = True\n",
    "\n",
    "##################################################################\n",
    "# Other parameters\n",
    "##################################################################\n",
    "# Parameters related to checkpoint handling\n",
    "\n",
    "random_seed = 42\n",
    "\n",
    "\n",
    "local_rank = -1\n",
    "    \n",
    "phase1_end_step=7038\n",
    "# \"Number of training steps (backprops) in pre-training phase n°1: \"\n",
    "# \"`max_input_length=128`and `max_masked_tokens_per_input=20`.\"\n",
    "\n",
    "\n",
    "phase2 = False\n",
    "\n",
    "# \"Whether it is pre-training phase n°2: \"\n",
    "# \"`max_input_length=512`and `max_masked_tokens_per_input=80`.\"\n",
    "\n",
    "init_checkpoint = None\n",
    "# \"An initial checkpoint to start pre-training from.\"\n",
    "\n",
    "# resume_pretraining = True\n",
    "resume_pretraining = False\n",
    "# \"Whether to resume pre-training from a checkpoint.\"\n",
    "\n",
    "resume_step=-1\n",
    "# \"Step to resume pre-training from. By default, this is `-1` \"\n",
    "# \"which results in resuming from the latest checkpoint available.\"\n",
    "\n",
    "\n",
    "##################################################################\n",
    "# Training hyperparameters\n",
    "\n",
    "max_input_length = 80\n",
    "# max_input_length = 20\n",
    "# \"Maximum sequence length for the model input. \"\n",
    "# \"Set this according to the input .hdf5 files contents.\"\n",
    "\n",
    "max_masked_tokens_per_input=12\n",
    "# max_masked_tokens_per_input=3\n",
    "# \"Hard limit on the number of tokens that can be masked. \"\n",
    "# \"Set this according to the input .hdf5 files contents.\"\n",
    "\n",
    "# num_accumulation_steps=512\n",
    "num_accumulation_steps=1\n",
    "# \"Number of steps (forward passes) during which gradients are \"\n",
    "# \"accumulated before running a single model parameters update.\"\n",
    "\n",
    "# target_batch_size=8192\n",
    "# target_batch_size=32\n",
    "target_batch_size=64\n",
    "# \"Target batch size post-accumulation (actual batch size is \"\n",
    "# \"derived from the number of accumulation steps). For example, if \"\n",
    "# \"`target_batch_size=32` and `num_accumulation_steps=4` then the \"\n",
    "# \"actual batch size will be `32/4 = 8`. This is useful for \"\n",
    "# \"achieving larger batch sizes while keeping an actual batch size \"\n",
    "# \"that is small enough to fit in memory.\"\n",
    "\n",
    "# learning_rate=6e-3\n",
    "learning_rate=2e-5\n",
    "# learning_rate = 3e-5\n",
    "# \"The initial learning rate for the FusedLAMB optimizer.\"\n",
    "\n",
    "warmup_proportion=0.2843\n",
    "# \"A value of X means that learning rate will increase during \"\n",
    "# \"(100*X)%% of pre-training steps before reaching the desired value \"\n",
    "# \"then decrease to 0 during the rest of pre-training steps.\"\n",
    "\n",
    "total_steps=2500\n",
    "#\"Total number of pre-training steps to perform.\"\n",
    "\n",
    "##################################################################\n",
    "# fp16 related parameters\n",
    "\n",
    "fp16=False\n",
    "# \"Whether to use 16-bit float precision instead of 32-bit\"\n",
    "\n",
    "loss_scale=0.0\n",
    "# 'Loss scaling, positive power of 2 values can improve fp16 convergence.'\n",
    "\n",
    "allreduce_post_accumulation=False\n",
    "#help=\"Whether to do allreduces during gradient accumulation steps.\"\n",
    "\n",
    "allreduce_post_accumulation_fp16=False\n",
    "# \"Whether to do fp16 allreduce post accumulation.\")\n",
    "\n",
    "##################################################################\n",
    "# Logging and checkpointing\n",
    "\n",
    "do_validation=False\n",
    "# Whether to run a validation step before checkpointing\n",
    "\n",
    "checkpoint_interval=200\n",
    "# \"Number of model updates before a model checkpoint is saved.\"\n",
    "\n",
    "num_checkpoints_to_keep=3\n",
    "# \"Maximum number of checkpoints to keep.\"\n",
    "\n",
    "log_freq=1.0\n",
    "#help='Frequency of logging loss.'\n",
    "\n",
    "tensorboard_id='default'\n",
    "# help=\"Name of the directory where Tensorboard logs will be saved.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f71287e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelPretrainer:\n",
    "    r\"\"\"A helper class for pre-training BERT and CharacterBERT models.\"\"\"\n",
    "    def __init__(self):\n",
    "\n",
    "        self.start_datetime = datetime.datetime.now()\n",
    "\n",
    "        # Set attributes from parsed arguments\n",
    "        self.hdf5_directory = hdf5_directory\n",
    "        self.output_directory = output_directory\n",
    "        self.tensorboard_id = tensorboard_id\n",
    "        self.is_character_bert = is_character_bert\n",
    "        self.local_rank = local_rank\n",
    "        self.phase1_end_step = phase1_end_step\n",
    "        self.phase2 = phase2\n",
    "        self.init_checkpoint = init_checkpoint\n",
    "        self.resume_pretraining = resume_pretraining\n",
    "        self.resume_step = resume_step\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_masked_tokens_per_input = max_masked_tokens_per_input\n",
    "        self.target_batch_size = target_batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.total_steps = total_steps\n",
    "        self.warmup_proportion = warmup_proportion\n",
    "        self.num_accumulation_steps = num_accumulation_steps\n",
    "        self.allreduce_post_accumulation = allreduce_post_accumulation\n",
    "        self.fp16 = fp16\n",
    "        self.loss_scale = loss_scale\n",
    "        self.allreduce_post_accumulation_fp16 = allreduce_post_accumulation_fp16\n",
    "        self.log_freq = log_freq\n",
    "        self.do_validation = do_validation\n",
    "        self.checkpoint_interval = checkpoint_interval\n",
    "        self.num_checkpoints_to_keep = num_checkpoints_to_keep\n",
    "        self.random_seed = random_seed\n",
    "        self.is_main_process = (\n",
    "            self.local_rank in [-1, 0]) and is_main_process()\n",
    "\n",
    "        if self.is_main_process:\n",
    "            logging.info('Preparing to run pre-training using parameters:')\n",
    "\n",
    "        # Set the random seed for reproducibility\n",
    "        set_all_random_seeds(self.random_seed, verbose=self.is_main_process)\n",
    "\n",
    "        # Make sure CUDA is available (it won't be if you're not using GPUs):\n",
    "        assert torch.cuda.is_available(), \"CUDA is unavailable (are you using GPUs?)\"\n",
    "\n",
    "        # Set CUDA-related attributes\n",
    "        self.training_is_distributed = (self.local_rank != -1)\n",
    "        if self.training_is_distributed:\n",
    "            torch.cuda.set_device(self.local_rank)\n",
    "            self.device = torch.device(\"cuda\", self.local_rank)\n",
    "            # Initialize distributed backend (takes care of sychronizing nodes/GPUs)\n",
    "            torch.distributed.init_process_group(backend='nccl', init_method='env://')\n",
    "            self.n_gpu = 1\n",
    "        else:\n",
    "            # TODO: test this\n",
    "            self.device = torch.device(\"cuda\")\n",
    "            self.n_gpu = torch.cuda.device_count()\n",
    "            self.allreduce_post_accumulation = False\n",
    "            self.allreduce_post_accumulation_fp16 = False\n",
    "\n",
    "        if self.num_accumulation_steps == 1:\n",
    "            self.allreduce_post_accumulation = False\n",
    "            self.allreduce_post_accumulation_fp16 = False\n",
    "\n",
    "        logging.info(\n",
    "            \"Distributed Training: %s, Number of GPUs: %d, Device: `%s`, Local Rank: `%s` (is_main: `%s`)\",\n",
    "            self.training_is_distributed, self.n_gpu, self.device, self.local_rank, self.is_main_process,\n",
    "        )\n",
    "\n",
    "        # Derive actual batch size from target batch size and accumulation steps:\n",
    "        assert self.num_accumulation_steps >= 1, \\\n",
    "            \"`num_accumulation_steps` should be greater or equal to 1\"\n",
    "        assert self.target_batch_size % self.num_accumulation_steps == 0, \\\n",
    "            \"`target_batch_size` should be divisible by `num_accumulation_steps`\"\n",
    "        self.batch_size = self.target_batch_size // self.num_accumulation_steps\n",
    "\n",
    "        # Make sure self.output_directory is empty when starting a training from scratch:\n",
    "        if not self.resume_pretraining:\n",
    "            os.makedirs(self.output_directory, exist_ok=True)\n",
    "            assert not any([\n",
    "                fname.startswith('ckpt')\n",
    "                for fname in os.listdir(self.output_directory)]), \\\n",
    "            \"Output directory should be empty when not resuming from a previous checkpoint\"\n",
    "\n",
    "        self.global_step = None  # training step counter\n",
    "        self.checkpoint = None  # checkpoint for resuming training\n",
    "        self.model = None  # actual model we are pre-training\n",
    "        self.optimizer = None  # the optimizer (FusedLAMB)\n",
    "        self.lr_scheduler = None  # the scheduler (PolyWarmUpScheduler)\n",
    "        self.tensorboard_writer = None  # helper for logging loss to Tensorboard\n",
    "        self.best_validation_loss = float(1e6)  # best val. loss achieved so far\n",
    "        self.most_recent_ckpts_paths = []  # list of most recent ckpt paths\n",
    "        \n",
    "    def prepare_model_optimizer_and_scheduler(self):\n",
    "        r\"\"\"Prepares the model, the optimizer and the learning rate scheduler.\"\"\"\n",
    "\n",
    "        ###################################################################\n",
    "        # MODEL PREPARATION\n",
    "        # -----------------\n",
    "        # - step 1: Initialize a random model from config\n",
    "        # - step 2: Load model weights from checkpoint if any\n",
    "        # - step 3: Move model to device (GPU)\n",
    "        ###################################################################\n",
    "\n",
    "        # Initialize a random model according to a specific config:\n",
    "        # NOTE: here we load from a physical path instead of using a keyword\n",
    "        # as compute nodes may not allow downloading from online hubs\n",
    "        if self.is_character_bert:\n",
    "            model_config = CharacterBertConfig.from_pretrained(\n",
    "                os.path.join(WORKDIR, 'data', 'character-bert'))\n",
    "            model = CharacterBertForPreTraining(model_config)\n",
    "        else:\n",
    "            model_config = BertConfig.from_pretrained(\n",
    "                os.path.join(WORKDIR, 'data', 'bert-base-uncased'))\n",
    "            model = BertForPreTraining(model_config)\n",
    "        if self.is_main_process:\n",
    "            logging.info(\n",
    "                \"Initialized %s using Config:\\n%s\",\n",
    "                \"CharacterBERT\" if self.is_character_bert else \"BERT\",\n",
    "                model_config\n",
    "            )\n",
    "\n",
    "        # Load checkpoint if any:\n",
    "        if not self.resume_pretraining:\n",
    "            # CASE: no checkpoint -> training from scratch\n",
    "            self.global_step = 0\n",
    "            if self.is_main_process:\n",
    "                logging.info(\"Pre-training from scratch (good luck!)\")\n",
    "        else:\n",
    "            if self.init_checkpoint:\n",
    "                # CASE: load checkpoint from direct path\n",
    "                self.global_step = 0\n",
    "                init_checkpoint = self.init_checkpoint\n",
    "                if self.is_main_process:\n",
    "                    logging.info(\n",
    "                        \"Resuming pre-training from specific checkpoint `%s`\",\n",
    "                        init_checkpoint\n",
    "                    )\n",
    "            else:\n",
    "                # CASE: load checkpoint from resume_step\n",
    "                if self.is_main_process:\n",
    "                    logging.info(\n",
    "                        \"Resuming pre-training from step `%s`. \"\n",
    "                        \"Looking inside `output_directory` for checkpoints...\",\n",
    "                        self.resume_step\n",
    "                    )\n",
    "\n",
    "                if self.resume_step == -1:\n",
    "                    # CASE: resume_step == -1, load latest checkpoint\n",
    "                    model_names = [\n",
    "                        fname\n",
    "                        for fname in os.listdir(self.output_directory)\n",
    "                        if fname.endswith(\".pt\")]\n",
    "                    assert model_names, \"Could not find any checkpoints to resume from.\"\n",
    "                    self.resume_step = max([\n",
    "                        int(x.split('.pt')[0].split('_')[1].strip())\n",
    "                        for x in model_names])  # TODO: find a better way for this\n",
    "                    if self.is_main_process:\n",
    "                        logging.info(\n",
    "                            \"Resuming from latest checkpoint: ckpt_%s.pt\",\n",
    "                            self.resume_step\n",
    "                        )\n",
    "                else:\n",
    "                    # CASE: resume_step == X, load checkpoint: `ckpt_X.pt`\n",
    "                    if self.is_main_process:\n",
    "                        logging.info(\n",
    "                            \"Resuming from checkpoint: ckpt_%s.pt\",\n",
    "                            self.resume_step\n",
    "                        )\n",
    "                self.global_step = self.resume_step\n",
    "                init_checkpoint = os.path.join(\n",
    "                    self.output_directory, f\"ckpt_{self.resume_step}.pt\")\n",
    "\n",
    "            # Load the actual checkpoint file\n",
    "            self.checkpoint = torch.load(\n",
    "                init_checkpoint, map_location=\"cpu\"\n",
    "            )\n",
    "\n",
    "            # NOTE: Keeping these lines below as a reminder that re-training on\n",
    "            # a different domain with CharacterBERT requires changing the\n",
    "            # output layer with a topK tokens matrix from the new domain.\n",
    "\n",
    "            # # Case where we would retrain a general_domain CharacterBERT\n",
    "            # # on the medical domain. Don't use the general domain output layer:\n",
    "            # if self.is_medical_domain and self.is_character_bert and (not self.phase2):\n",
    "            #     model.load_state_dict(\n",
    "            #         {\n",
    "            #             k: v for (k, v) in self.checkpoint['model'].items()\n",
    "            #             # Don't load output matrix from general domain model\n",
    "            #             if not k.startswith('cls.predictions')  # ignoring the old output layer\n",
    "            #         },\n",
    "            #         strict=False)\n",
    "            #     if self.is_main_process:\n",
    "            #         logging.warning(\n",
    "            #             \"Loaded model weights from `%s`, \"\n",
    "            #             \"but ignored the `cls.predictions` module.\",\n",
    "            #             init_checkpoint)\n",
    "\n",
    "            # # General case: load weights from checkpoint\n",
    "            # else:\n",
    "            #     model.load_state_dict(self.checkpoint['model'], strict=True)\n",
    "            #     if self.is_main_process:\n",
    "            #         logging.info('Loaded model weights from `%s`',\n",
    "            #                      init_checkpoint)\n",
    "\n",
    "            # General case: load weights from checkpoint\n",
    "            model.load_state_dict(self.checkpoint['model'], strict=True)\n",
    "            if self.is_main_process:\n",
    "                logging.info('Loaded model weights from `%s`', init_checkpoint)\n",
    "\n",
    "            # Deduce previous steps from phase1 when in phase2\n",
    "            if self.phase2 and not self.init_checkpoint:\n",
    "                self.global_step -= self.phase1_end_step\n",
    "\n",
    "            if self.is_main_process:\n",
    "                logging.info(\"Training will start at global_step=%s\", self.global_step)\n",
    "\n",
    "        #Move model to GPU:\n",
    "        model.to(self.device)\n",
    "        if self.is_main_process:\n",
    "            logging.info(\"Model was moved to device: %s\", self.device)\n",
    "\n",
    "        ###################################################################\n",
    "        # OPTIMIZER / SCHEDULER PREPARATION\n",
    "        # ---------------------------------\n",
    "        # - step 1: Define the optimizer (FusedLAMB w/ some weight decay)\n",
    "        # - step 2: Define the learning rate scheduler (PolyWarmUpScheduler)\n",
    "        ###################################################################\n",
    "\n",
    "        # Initialize an optimizer:\n",
    "        no_decay = ['bias', 'gamma', 'beta', 'LayerNorm']  # no weight decay\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                'params': [\n",
    "                    param for name, param in model.named_parameters()\n",
    "                    if not any((nd in name) for nd in no_decay)],\n",
    "                'weight_decay': 0.01\n",
    "            },\n",
    "            {\n",
    "                'params': [\n",
    "                    param for name, param in model.named_parameters()\n",
    "                    if any((nd in name) for nd in no_decay)],\n",
    "                'weight_decay': 0.0\n",
    "            }\n",
    "        ]\n",
    "#         optimizer = FusedLAMB(\n",
    "#             optimizer_grouped_parameters, lr=self.learning_rate)\n",
    "        optimizer = AdamW(\n",
    "        optimizer_grouped_parameters, lr=self.learning_rate)\n",
    "    \n",
    "        if self.is_main_process:\n",
    "            logging.info(\"Using optimizer: %s\", optimizer)\n",
    "        \n",
    "        \n",
    "        # Initialize a learning rate scheduler:\n",
    "        self.lr_scheduler = PolyWarmUpScheduler(\n",
    "            optimizer,\n",
    "            warmup=self.warmup_proportion,\n",
    "            total_steps=self.total_steps\n",
    "        )\n",
    "        if self.is_main_process:\n",
    "            logging.info(\"Using scheduler: %s\", self.lr_scheduler)\n",
    "\n",
    "        ###################################################################\n",
    "        # OTHER PREPARATION STEPS\n",
    "        # -----------------------\n",
    "        # - step 1: Set up Mixed Precision training (fp16) if required\n",
    "        # - step 2: Load optimizer stat from checkpoint if any\n",
    "        # - step 2: Set up DataParallel\n",
    "        ###################################################################\n",
    "\n",
    "        # Set up fp16:\n",
    "        if self.fp16:\n",
    "            if self.is_main_process:\n",
    "                logging.info(\"Setting up `Almost FP16` Mixed Precision...\")\n",
    "            if self.loss_scale == 0:\n",
    "                model, optimizer = amp.initialize(\n",
    "                    model, optimizer, opt_level=\"O2\", loss_scale=\"dynamic\")\n",
    "            else:\n",
    "                model, optimizer = amp.initialize(\n",
    "                    model, optimizer, opt_level=\"O2\", loss_scale=self.loss_scale)\n",
    "            amp._amp_state.loss_scalers[0]._loss_scale = 2**20\n",
    "\n",
    "        # Load optimizer state from checkpoint\n",
    "        if self.resume_pretraining:\n",
    "            if self.is_main_process:\n",
    "                logging.info(\"Loading optimizer state from checkpoint...\")\n",
    "            if self.phase2 or self.init_checkpoint:\n",
    "                keys = list(self.checkpoint['optimizer']['state'].keys())\n",
    "                # Override hyperparameters from previous self.checkpoint\n",
    "                for key in keys:\n",
    "                    self.checkpoint['optimizer']['state'][key]['step'] = self.global_step\n",
    "                for i, _ in enumerate(self.checkpoint['optimizer']['param_groups']):\n",
    "                    self.checkpoint['optimizer']['param_groups'][i]['step'] = self.global_step\n",
    "                    self.checkpoint['optimizer']['param_groups'][i]['t_total'] = self.total_steps\n",
    "                    self.checkpoint['optimizer']['param_groups'][i]['warmup'] = self.warmup_proportion\n",
    "                    self.checkpoint['optimizer']['param_groups'][i]['lr'] = self.learning_rate\n",
    "                if self.is_main_process:\n",
    "                    logging.info(\"Overwrote the following parameters with new values:\")\n",
    "                    logging.info(\"* step: %s\", self.global_step)\n",
    "                    logging.info(\"* t_total: %s\", self.total_steps)\n",
    "                    logging.info(\"* warmup: %s\", self.warmup_proportion)\n",
    "                    logging.info(\"* lr: %s\", self.learning_rate)\n",
    "            optimizer.load_state_dict(self.checkpoint['optimizer'])\n",
    "            # Restore AMP master parameters\n",
    "            if self.fp16:\n",
    "                if self.is_main_process:\n",
    "                    logging.info(\"Restoring AMP master parameters (optimizer)...\")\n",
    "                optimizer._lazy_init_maybe_master_weights()\n",
    "                optimizer._amp_stash.lazy_init_called = True\n",
    "                optimizer.load_state_dict(self.checkpoint['optimizer'])\n",
    "                for param, saved_param in zip(amp.master_params(optimizer), self.checkpoint['master params']):\n",
    "                    param.data.copy_(saved_param.data)\n",
    "\n",
    "        # Distribute model\n",
    "        if self.training_is_distributed:\n",
    "            if not self.allreduce_post_accumulation:\n",
    "                model = DistributedDataParallel(\n",
    "                    model,\n",
    "                    message_size=250000000,\n",
    "                    gradient_predivide_factor=\\\n",
    "                        torch.distributed.get_world_size()\n",
    "                )\n",
    "            else:\n",
    "                flat_dist_call(\n",
    "                    [param.data for param in model.parameters()],\n",
    "                    torch.distributed.broadcast,\n",
    "                    (0,)\n",
    "                )\n",
    "        elif self.n_gpu > 1:\n",
    "            model = torch.nn.DataParallel(model)\n",
    "\n",
    "        # Set the values of self.model and self.optimizer\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def take_optimizer_step(self, overflow_buf):\n",
    "        r\"\"\"Takes an optimizer step (updates the model weights).\"\"\"\n",
    "        if self.allreduce_post_accumulation:\n",
    "            # manually allreduce gradients after all accumulation steps\n",
    "            # check for Inf/NaN\n",
    "            # 1. allocate an uninitialized buffer for flattened gradient\n",
    "            scaler = _amp_state.loss_scalers[0]\n",
    "            master_grads = [\n",
    "                p.grad\n",
    "                for p in amp.master_params(self.optimizer)\n",
    "                if p.grad is not None\n",
    "            ]\n",
    "            flat_grad_size = sum(p.numel() for p in master_grads)\n",
    "            allreduce_dtype = \\\n",
    "                torch.float16 \\\n",
    "                if self.allreduce_post_accumulation_fp16 \\\n",
    "                else torch.float32\n",
    "            flat_raw = torch.empty(\n",
    "                flat_grad_size,\n",
    "                device='cuda', dtype=allreduce_dtype)\n",
    "            # 2. combine unflattening and predivision of unscaled 'raw' gradient\n",
    "            allreduced_views = apex_C.unflatten(flat_raw, master_grads)\n",
    "            overflow_buf.zero_()\n",
    "            amp_C.multi_tensor_scale(\n",
    "                65536,\n",
    "                overflow_buf,\n",
    "                [master_grads, allreduced_views],\n",
    "                scaler.loss_scale() /\n",
    "                (torch.distributed.get_world_size()\n",
    "                 * self.num_accumulation_steps)\n",
    "            )\n",
    "            # 3. sum gradient across ranks. Because of the predivision, this averages the gradient\n",
    "            torch.distributed.all_reduce(flat_raw)\n",
    "            # 4. combine unscaling and unflattening of allreduced gradient\n",
    "            overflow_buf.zero_()\n",
    "            amp_C.multi_tensor_scale(\n",
    "                65536,\n",
    "                overflow_buf,\n",
    "                [allreduced_views, master_grads],\n",
    "                1./scaler.loss_scale()\n",
    "            )\n",
    "            # 5. update loss scale\n",
    "            scaler = _amp_state.loss_scalers[0]\n",
    "            old_overflow_buf = scaler._overflow_buf\n",
    "            scaler._overflow_buf = overflow_buf\n",
    "            had_overflow = scaler.update_scale()\n",
    "            scaler._overfloat_buf = old_overflow_buf\n",
    "            # 6. call optimizer step function\n",
    "            if had_overflow == 0:\n",
    "                self.optimizer.step()\n",
    "                self.global_step += 1\n",
    "            else:\n",
    "                # Overflow detected, print message and clear gradients\n",
    "                if self.is_main_process:\n",
    "                    logging.info(\n",
    "                        f\"Rank {torch.distributed.get_rank()} \"\n",
    "                        \":: Gradient overflow.  Skipping step, \"\n",
    "                        f\"reducing loss scale to {scaler.loss_scale()}\"\n",
    "                    )\n",
    "                if _amp_state.opt_properties.master_weights:\n",
    "                    for param in self.optimizer._amp_stash.all_fp32_from_fp16_params:\n",
    "                        param.grad = None\n",
    "            for param in self.model.parameters():\n",
    "                param.grad = None\n",
    "        else:\n",
    "            self.optimizer.step()\n",
    "            # NOTE: This basically does: optimizer.zero_grad()\n",
    "            for param in self.model.parameters():\n",
    "                param.grad = None\n",
    "            self.global_step += 1\n",
    "            \n",
    "    def make_checkpoint(self, f_id, files):\n",
    "        r\"\"\"Saves a checkpoint of the model.\"\"\"\n",
    "        logging.info(\"Saving a checkpoint of the current model...\")\n",
    "\n",
    "        # NOTE: model may be an instance of apex.parallel.distributed.DistributedDataParallel\n",
    "        # in this case, model.module is the actual pytorch module\n",
    "        model_to_save = \\\n",
    "            self.model.module \\\n",
    "            if hasattr(self.model, 'module') \\\n",
    "            else self.model\n",
    "\n",
    "        # Save model weights, optimizer state, AMP master parameters and\n",
    "        # the list of .hdf5 that are yet to be used (e.g. for resuming pre-training)\n",
    "        if self.resume_step < 0 or not self.phase2:\n",
    "            output_save_file = os.path.join(\n",
    "                self.output_directory,\n",
    "                f\"ckpt_{self.global_step}.pt\")\n",
    "        else:\n",
    "            output_save_file = os.path.join(\n",
    "                self.output_directory,\n",
    "                f\"ckpt_{self.global_step + self.phase1_end_step}.pt\")\n",
    "        torch.save(\n",
    "            {\n",
    "                'model': model_to_save.state_dict(),\n",
    "                'optimizer': self.optimizer.state_dict(),\n",
    "                'master params': list(amp.master_params(self.optimizer)),\n",
    "                'files': [f_id] + files\n",
    "            },\n",
    "            output_save_file\n",
    "        )\n",
    "\n",
    "        # Keep only a specific number of 'best' checkpoints\n",
    "        self.most_recent_ckpts_paths.append(output_save_file)\n",
    "        if len(self.most_recent_ckpts_paths) > self.num_checkpoints_to_keep:\n",
    "            checkpoint_to_remove = \\\n",
    "                self.most_recent_ckpts_paths.pop(0)\n",
    "            os.remove(checkpoint_to_remove)\n",
    "\n",
    "    def run_pretraining(self):\n",
    "        r\"\"\"Runs the pre-training process.\"\"\"\n",
    "        if self.is_main_process:\n",
    "            logging.info(\"*********************************\")\n",
    "            logging.info(\"***   Starting pre-training   ***\")\n",
    "            logging.info(\"*********************************\")\n",
    "            logging.info(\"Training on GPU: %s\", torch.cuda.get_device_name(0))\n",
    "            logging.info(\"Target batch size: %s\", self.target_batch_size)\n",
    "            logging.info(\"Number of accumulation steps: %s\", self.num_accumulation_steps)\n",
    "            logging.info(\"Actual batch size: %s\", self.batch_size)\n",
    "\n",
    "        self.model.train()\n",
    "        self.most_recent_ckpts_paths = []\n",
    "        average_loss = 0.0  # averaged loss every self.log_freq steps\n",
    "        epoch = 0\n",
    "        training_steps = 0\n",
    "        pool = ProcessPoolExecutor(1)\n",
    "        if self.is_main_process:\n",
    "            tensorboard_log_fpath = os.path.join(\n",
    "                    WORKDIR,\n",
    "                    '.tensorboard_logs',\n",
    "                    self.tensorboard_id,\n",
    "                    self.start_datetime.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "            )\n",
    "            logging.info(\n",
    "                \"Writing TensorBoard logs in: %s\",\n",
    "                tensorboard_log_fpath.replace(WORKDIR, '$WORKDIR'))\n",
    "            self.tensorboard_writer = SummaryWriter(tensorboard_log_fpath)\n",
    "\n",
    "        # NOTE: Infinite loop over epochs, termination is handled via iteration count\n",
    "        while True:\n",
    "\n",
    "            # If beginning of pre-training: read files from hdf5_directory and shuffle\n",
    "            if (not self.resume_pretraining) or (epoch > 0) \\\n",
    "                    or (self.phase2 and self.global_step < 1) or self.init_checkpoint:\n",
    "                    files = []\n",
    "                    for fname in os.listdir(self.hdf5_directory):\n",
    "                        fpath = os.path.join(self.hdf5_directory, fname)\n",
    "                        if os.path.isfile(fpath) and fname.startswith('training.') and fname.endswith('.hdf5'):\n",
    "                            files.append(fpath)\n",
    "                    f_start_id = 0\n",
    "                    files.sort()\n",
    "                    random.Random(self.random_seed + epoch).shuffle(files)\n",
    "            # Else: get id of next file\n",
    "            else:\n",
    "                f_start_id = self.checkpoint['files'][0]\n",
    "                files = self.checkpoint['files'][1:]\n",
    "                self.resume_pretraining = False\n",
    "            num_files = len(files)\n",
    "            \n",
    "            # Get the current process hdf5 file\n",
    "            # and handle case where there are more processes than files left:\n",
    "            if \\\n",
    "                    torch.distributed.is_initialized() \\\n",
    "                    and torch.distributed.get_world_size() > num_files:\n",
    "\n",
    "                remainder = torch.distributed.get_world_size() % num_files\n",
    "                hdf5_fpath = files[\n",
    "                    (\n",
    "                        f_start_id * torch.distributed.get_world_size()\n",
    "                        + torch.distributed.get_rank()\n",
    "                        + remainder * f_start_id\n",
    "                    ) % num_files\n",
    "                ]\n",
    "            else:\n",
    "                hdf5_fpath = files[f_start_id]\n",
    "    \n",
    "            # Set previous_file variable for next iteration\n",
    "            previous_file = hdf5_fpath\n",
    "\n",
    "            # Load the pre-training data from the .hdf5 file\n",
    "            pretraining_data = PretrainingDataset(\n",
    "                hdf5_fpath=hdf5_fpath,\n",
    "                max_masked_tokens_per_input=self.max_masked_tokens_per_input\n",
    "            )\n",
    "            train_sampler = RandomSampler(pretraining_data)\n",
    "            train_dataloader = DataLoader(\n",
    "                pretraining_data,\n",
    "                sampler=train_sampler,\n",
    "                batch_size=self.batch_size * self.n_gpu,\n",
    "                num_workers=4, pin_memory=True\n",
    "            )\n",
    "            overflow_buf = None\n",
    "            if self.allreduce_post_accumulation:\n",
    "                overflow_buf = torch.cuda.IntTensor([0])\n",
    "\n",
    "            # Loop over the rest of pre-training data files\n",
    "            if len(files) == 1:\n",
    "                f_start_id = -1\n",
    "            for f_id in range(f_start_id + 1, len(files)):\n",
    "\n",
    "                # Submit creation of next DataLoader\n",
    "                hdf5_fpath = files[f_id]\n",
    "                \n",
    "                if self.is_main_process:\n",
    "                    logging.info(\n",
    "                        \"Local rank: %s | File n° %s: %s\",\n",
    "                        self.local_rank, f_id, os.path.basename(previous_file)\n",
    "                    )\n",
    "                previous_file = hdf5_fpath\n",
    "                dataset_future = pool.submit(\n",
    "                    create_pretraining_dataloader,\n",
    "                    hdf5_fpath,\n",
    "                    self.max_masked_tokens_per_input,\n",
    "                    self.batch_size * self.n_gpu,\n",
    "                )\n",
    "\n",
    "                # Iterate over batches (w/ progress bar for main process)\n",
    "                training_batches = tqdm(\n",
    "                    train_dataloader,\n",
    "                    desc=\"Pre-training...\"\n",
    "                    ) if self.is_main_process else train_dataloader\n",
    "                for batch in training_batches:\n",
    "                    print(\"taking steps\")\n",
    "                    training_steps += 1\n",
    "                    (\n",
    "                        input_ids,\n",
    "                        segment_ids,\n",
    "                        input_mask,\n",
    "                        masked_lm_labels,\n",
    "                        next_sentence_labels\n",
    "                    ) = [tensor.to(self.device) for tensor in batch]\n",
    "\n",
    "                    # Forward Pass\n",
    "                    model_output = self.model(\n",
    "                        input_ids=input_ids,\n",
    "                        token_type_ids=segment_ids,\n",
    "                        attention_mask=input_mask,\n",
    "                        labels=masked_lm_labels,\n",
    "                        next_sentence_label=next_sentence_labels)\n",
    "                    loss = model_output['loss']\n",
    "                    if self.n_gpu > 1:\n",
    "                        loss = loss.mean()  # mean() to average on multi-gpu.\n",
    "\n",
    "                    divisor = self.num_accumulation_steps\n",
    "                    if self.num_accumulation_steps > 1:\n",
    "                        if not self.allreduce_post_accumulation:\n",
    "                            # this division was merged into predivision\n",
    "                            loss = loss / self.num_accumulation_steps\n",
    "                            divisor = 1.0\n",
    "\n",
    "                    # Compute gradients\n",
    "                    if self.fp16:\n",
    "                        with amp.scale_loss(\n",
    "                                loss, self.optimizer,\n",
    "                                delay_overflow_check=self.allreduce_post_accumulation) as scaled_loss:\n",
    "                            scaled_loss.backward()\n",
    "                    else:\n",
    "                        loss.backward()\n",
    "\n",
    "                    average_loss += loss.item()\n",
    "\n",
    "                    # Take optimizer/scheduler step every (gradient_acc_steps) steps\n",
    "                    # This is the model parameter update:\n",
    "                    if training_steps % self.num_accumulation_steps == 0:\n",
    "                        self.lr_scheduler.step()  # learning rate warmup\n",
    "                        self.take_optimizer_step(overflow_buf)\n",
    "\n",
    "                    # If reached max steps save everything and log final loss:\n",
    "                    if self.global_step >= self.total_steps:\n",
    "                        last_num_steps = int(\n",
    "                            training_steps / self.num_accumulation_steps\n",
    "                        ) % self.log_freq\n",
    "                        last_num_steps = self.log_freq if last_num_steps == 0 else last_num_steps\n",
    "                        average_loss = torch.tensor(average_loss, dtype=torch.float32).cuda()\n",
    "                        average_loss = average_loss / (last_num_steps * divisor)\n",
    "                        if torch.distributed.is_initialized():\n",
    "                            average_loss /= torch.distributed.get_world_size()\n",
    "                            torch.distributed.all_reduce(average_loss)\n",
    "                        if self.is_main_process:\n",
    "                            logging.info(\n",
    "                                \"Total Steps: %s | Final Loss = %.3f\",\n",
    "                                int(training_steps / self.num_accumulation_steps),\n",
    "                                average_loss.item()\n",
    "                            )\n",
    "                            self.tensorboard_writer.add_scalar(\n",
    "                                \"Avg. training loss\",\n",
    "                                average_loss.item(), global_step=self.global_step)\n",
    "\n",
    "                    # If at a logging step:\n",
    "                    elif training_steps % (self.log_freq * self.num_accumulation_steps) == 0:\n",
    "                        if self.is_main_process:\n",
    "                            logging_message = (\n",
    "                                f\"Global step: {self.global_step} | \"\n",
    "                                f\"Learning Rate: {self.optimizer.param_groups[0]['lr']:.2E} | \"\n",
    "                                f\"Step Loss: {loss.item() * self.num_accumulation_steps / divisor:.3f} | \"\n",
    "                                f\"Avg. Loss: {average_loss / (self.log_freq * divisor):.3f}\"\n",
    "                            )\n",
    "                            # Update the tqdm description\n",
    "                            training_batches.set_description(logging_message, refresh=True)\n",
    "                            # Log average training loss to TensorBoard:\n",
    "                            self.tensorboard_writer.add_scalar(\n",
    "                                \"Avg. training loss\",\n",
    "                                average_loss / (self.log_freq * divisor),\n",
    "                                global_step=self.global_step)\n",
    "                        average_loss = 0\n",
    "\n",
    "                    # If reached max steps at log step or reached checkpoint step:\n",
    "                    if \\\n",
    "                        self.global_step >= self.total_steps \\\n",
    "                        or training_steps % \\\n",
    "                            (self.checkpoint_interval * self.num_accumulation_steps) == 0:\n",
    "\n",
    "                        # Check if model has improved then save a checkpoint if so\n",
    "                        if self.do_validation:\n",
    "                            model_has_improved = self.run_validation()\n",
    "                        else:\n",
    "                            model_has_improved = True\n",
    "                        if self.is_main_process and model_has_improved:\n",
    "                            self.make_checkpoint(f_id, files)\n",
    "\n",
    "                        # End pre-training if reached max steps\n",
    "                        if self.global_step >= self.total_steps:\n",
    "                            del train_dataloader\n",
    "                            return  # NOTE: breaks out of the training loop\n",
    "\n",
    "                # Move to next file after using up all batches of current file\n",
    "                del train_dataloader\n",
    "                train_dataloader, hdf5_fpath = \\\n",
    "                    dataset_future.result(timeout=None)\n",
    "\n",
    "            # Update epoch after going through all .hdf5 files\n",
    "            epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c0832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CharacterBertModel\n",
    "model_config = CharacterBertConfig.from_pretrained(os.path.join(WORKDIR, 'data', 'character-bert'))\n",
    "model = CharacterBertForPreTraining(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c23afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d161f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = 0\n",
    "for p in model.parameters():\n",
    "    if len(p.shape) == 2:\n",
    "        total_params += p.shape[0] * p.shape[1]\n",
    "        \n",
    "print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8602762c",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "training_steps = 0\n",
    "if is_main_process:\n",
    "    logging.info(\"Pre-training from scratch (good luck!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701021c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move model to GPU:\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "if is_main_process:\n",
    "    logging.info(\"Model was moved to device: %s\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc7fbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.optim import AdamW\n",
    "\n",
    "# # Initialize an optimizer:\n",
    "# no_decay = ['bias', 'gamma', 'beta', 'LayerNorm']  # no weight decay\n",
    "# optimizer_grouped_parameters = [\n",
    "#     {\n",
    "#         'params': [\n",
    "#             param for name, param in model.named_parameters()\n",
    "#             if not any((nd in name) for nd in no_decay)],\n",
    "#         'weight_decay': 0.01\n",
    "#     },\n",
    "#     {\n",
    "#         'params': [\n",
    "#             param for name, param in model.named_parameters()\n",
    "#             if any((nd in name) for nd in no_decay)],\n",
    "#         'weight_decay': 0.0\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "\n",
    "# if is_main_process:\n",
    "#         logging.info(\"Using optimizer: %s\", optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61b1c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3775d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize a learning rate scheduler:\n",
    "# lr_scheduler = PolyWarmUpScheduler(\n",
    "#     optimizer,\n",
    "#     warmup=warmup_proportion,\n",
    "#     total_steps=total_steps\n",
    "# )\n",
    "# if is_main_process:\n",
    "#     logging.info(\"Using scheduler: %s\", lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9506e523",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = target_batch_size // num_accumulation_steps\n",
    "if is_main_process:\n",
    "    logging.info(\"*********************************\")\n",
    "    logging.info(\"***   Starting pre-training   ***\")\n",
    "    logging.info(\"*********************************\")\n",
    "    logging.info(\"Training on GPU: %s\", torch.cuda.get_device_name(0))\n",
    "    logging.info(\"Target batch size: %s\", target_batch_size)\n",
    "    logging.info(\"Number of accumulation steps: %s\", num_accumulation_steps)\n",
    "    logging.info(\"Actual batch size: %s\", batch_size)\n",
    "\n",
    "model.train()\n",
    "most_recent_ckpts_paths = []\n",
    "average_loss = 0.0  # averaged loss every log_freq steps\n",
    "epoch = 0\n",
    "training_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01277de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_recent_ckpts_paths = []\n",
    "average_loss = 0.0  # averaged loss every log_freq steps\n",
    "epoch = 0\n",
    "training_steps = 0\n",
    "start_datetime = datetime.datetime.now()\n",
    "if num_accumulation_steps == 1:\n",
    "        allreduce_post_accumulation = False\n",
    "        allreduce_post_accumulation_fp16 = False\n",
    "\n",
    "if is_main_process:\n",
    "        tensorboard_log_fpath = os.path.join(\n",
    "                WORKDIR,\n",
    "                '.tensorboard_logs',\n",
    "                tensorboard_id,\n",
    "                start_datetime.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "        )\n",
    "        logging.info(\n",
    "            \"Writing TensorBoard logs in: %s\",\n",
    "            tensorboard_log_fpath.replace(WORKDIR, '$WORKDIR'))\n",
    "        tensorboard_writer = SummaryWriter(tensorboard_log_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8458f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_fpath = \"data\\hdf5\\80_12\\training.Hindi_LiveHindustan.formatted.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd71ebbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If beginning of pre-training: read files from hdf5_directory and shuffle\n",
    "if (not resume_pretraining) or (epoch > 0) \\\n",
    "        or (phase2 and global_step < 1) or init_checkpoint:\n",
    "    files = []\n",
    "    for fname in os.listdir(hdf5_directory):\n",
    "        fpath = os.path.join(hdf5_directory, fname)\n",
    "        if os.path.isfile(fpath) and fname.startswith('training.') and fname.endswith('.hdf5'):\n",
    "            files.append(fpath)\n",
    "    f_start_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8796666c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_files = len(files)\n",
    "num_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c9510c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hdf5_fpath = files[f_start_id]\n",
    "hdf5_fpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781b0e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-training data from the .hdf5 file\n",
    "pretraining_data = PretrainingDataset(\n",
    "    hdf5_fpath=hdf5_fpath,\n",
    "    max_masked_tokens_per_input=max_masked_tokens_per_input\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158dbec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretraining_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fa3c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = RandomSampler(pretraining_data)\n",
    "train_dataloader = DataLoader(\n",
    "    pretraining_data,\n",
    "    sampler=train_sampler,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d75cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    #num_warmup_steps=warmup_proportion * num_training_steps,\n",
    "    num_warmup_steps = 0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1a122c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69a5e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"models/char/bdnews24-pretrained\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24afa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_hdf5_fpath = 'data\\\\hdf5\\\\80_12\\\\error_validation.Bangla_Prothom_Alo.formatted.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278122be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-training data from the .hdf5 file\n",
    "validation_data = PretrainingDataset(\n",
    "    hdf5_fpath=validation_hdf5_fpath,\n",
    "    max_masked_tokens_per_input=max_masked_tokens_per_input\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc3c94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_sampler = RandomSampler(validation_data)  # This could be SequentialSampler\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    validation_data,\n",
    "    sampler=validation_sampler,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86b82e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c82fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data[0][0][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f1ac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_checkpoint():\n",
    "    r\"\"\"Saves a checkpoint of the model.\"\"\"\n",
    "    logging.info(\"Saving a checkpoint of the current model...\")\n",
    "\n",
    "    model_to_save = model\n",
    "\n",
    "    torch.save(\n",
    "        {\n",
    "            'model': model_to_save.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        },\n",
    "        output_directory\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b71a142",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_train_epochs):\n",
    "    model.train()\n",
    "    # Iterate over batches (w/ progress bar for main process)\n",
    "    training_batches = tqdm(\n",
    "        train_dataloader,\n",
    "        desc=\"Pre-training...\"\n",
    "        ) if is_main_process else train_dataloader\n",
    "    \n",
    "    training_losses = []\n",
    "    for batch in training_batches:\n",
    "        training_steps += 1\n",
    "        (\n",
    "            input_ids,\n",
    "            segment_ids,\n",
    "            input_mask,\n",
    "            masked_lm_labels,\n",
    "            next_sentence_labels\n",
    "        ) = [tensor.to(device) for tensor in batch]\n",
    "\n",
    "        # Forward Pass\n",
    "        model_output = model(\n",
    "            input_ids=input_ids,\n",
    "            token_type_ids=segment_ids,\n",
    "            attention_mask=input_mask,\n",
    "            labels=masked_lm_labels,\n",
    "            next_sentence_label=next_sentence_labels\n",
    "        )\n",
    "        total_loss = model_output['loss']\n",
    "        loss = F.cross_entropy(\n",
    "                model_output['prediction_logits'].transpose(1, 2),\n",
    "                masked_lm_labels,\n",
    "                ignore_index = IGNORE_INDEX\n",
    "            )\n",
    "\n",
    "        # Update average\n",
    "        loss.backward()\n",
    "        lr_scheduler.step()  # learning rate warmup\n",
    "        optimizer.step()\n",
    "        # NOTE: This basically does: optimizer.zero_grad()\n",
    "#         for param in model.parameters():\n",
    "#             param.grad = None\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1\n",
    "        \n",
    "        training_losses.append(torch.tensor(loss.item(), dtype=torch.float32).repeat(batch_size))\n",
    "        \n",
    "    training_losses = torch.cat(training_losses)\n",
    "    training_losses = training_losses[: len(pretraining_data)]\n",
    "    \n",
    "    print(f\">>> Epoch {epoch}: Training Loss: {torch.mean(training_losses)}\")\n",
    "    \n",
    "    #model.save_pretrained(output_dir)\n",
    "    make_checkpoint()\n",
    "    \n",
    "    steps = 0\n",
    "    # Use model in `evaluation mode`\n",
    "    validation_losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        # Iterate over batches (w/ progress bar for main process)\n",
    "        validation_batches = tqdm(\n",
    "            validation_dataloader,\n",
    "            desc=\"Computing loss on the validation set...\"\n",
    "            ) if is_main_process else validation_dataloader\n",
    "\n",
    "        for batch in validation_batches:\n",
    "            steps += 1\n",
    "            (\n",
    "                input_ids,\n",
    "                segment_ids,\n",
    "                input_mask,\n",
    "                masked_lm_labels,\n",
    "                next_sentence_labels\n",
    "            ) = [tensor.to(device) for tensor in batch]\n",
    "\n",
    "            # Forward Pass\n",
    "            model_output = model(\n",
    "                input_ids=input_ids,\n",
    "                token_type_ids=segment_ids,\n",
    "                attention_mask=input_mask,\n",
    "                labels=masked_lm_labels,\n",
    "                next_sentence_label=next_sentence_labels)\n",
    "\n",
    "            loss = F.cross_entropy(\n",
    "                    model_output['prediction_logits'].transpose(1, 2),\n",
    "                    masked_lm_labels,\n",
    "                    ignore_index = IGNORE_INDEX\n",
    "                )\n",
    "\n",
    "            validation_losses.append(torch.tensor(loss.item(), dtype=torch.float32).repeat(batch_size))\n",
    "\n",
    "        validation_losses = torch.cat(validation_losses)\n",
    "        validation_losses = validation_losses[: len(validation_data)]\n",
    "\n",
    "        print(f\">>> Validation Loss: {torch.mean(validation_losses)}\")   \n",
    "        try:\n",
    "            perplexity = math.exp(torch.mean(validation_losses))\n",
    "        except OverflowError:\n",
    "            perplexity = float(\"inf\")\n",
    "\n",
    "        print(f\">>>Perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bebfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc345dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the actual checkpoint file\n",
    "checkpoint = torch.load(\n",
    "    output_directory, map_location=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f289c8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(checkpoint['model'], strict=True)\n",
    "if is_main_process:\n",
    "    logging.info('Loaded model weights from `%s`', output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2cff6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CharacterBertForPreTraining.from_pretrained(output_dir)\n",
    "# model.to(device)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935eae8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 0\n",
    "# Use model in `evaluation mode`\n",
    "validation_losses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    # Iterate over batches (w/ progress bar for main process)\n",
    "    validation_batches = tqdm(\n",
    "        validation_dataloader,\n",
    "        desc=\"Computing loss on the validation set...\"\n",
    "        ) if is_main_process else validation_dataloader\n",
    "    \n",
    "    for batch in validation_batches:\n",
    "        steps += 1\n",
    "        (\n",
    "            input_ids,\n",
    "            segment_ids,\n",
    "            input_mask,\n",
    "            masked_lm_labels,\n",
    "            next_sentence_labels\n",
    "        ) = [tensor.to(device) for tensor in batch]\n",
    "\n",
    "        # Forward Pass\n",
    "        model_output = model(\n",
    "            input_ids=input_ids,\n",
    "            token_type_ids=segment_ids,\n",
    "            attention_mask=input_mask,\n",
    "            labels=masked_lm_labels,\n",
    "            next_sentence_label=next_sentence_labels)\n",
    "        \n",
    "        loss = F.cross_entropy(\n",
    "                model_output['prediction_logits'].transpose(1, 2),\n",
    "                masked_lm_labels,\n",
    "                ignore_index = IGNORE_INDEX\n",
    "            )\n",
    "        \n",
    "        validation_losses.append(torch.tensor(loss.item(), dtype=torch.float32).repeat(batch_size))\n",
    "    \n",
    "    validation_losses = torch.cat(validation_losses)\n",
    "    validation_losses = validation_losses[: len(validation_data)]\n",
    "\n",
    "    print(f\">>> Validation Loss: {torch.mean(validation_losses)}\")   \n",
    "    try:\n",
    "        perplexity = math.exp(torch.mean(validation_losses))\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "\n",
    "    print(f\">>>Perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d89fc41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "character_bert",
   "language": "python",
   "name": "character_bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
