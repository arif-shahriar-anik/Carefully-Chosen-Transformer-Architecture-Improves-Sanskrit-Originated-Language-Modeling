{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abd764b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "review_dataset = load_dataset(\"csv\", data_files=\"..\\datasets\\Hindi Product Review.csv\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1c1826",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6965d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_dataset.set_format(\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecadf5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get label counts for both classes\n",
    "label_counts = review_dataset[\"Label\"].value_counts()\n",
    "num_labels = (len(label_counts.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822e429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed49cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_token_length = max(review_dataset['Text'].str.len())\n",
    "max_token_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf07240",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_dataset.reset_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fd7e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "\n",
    "set_seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe21642",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertConfig, CharacterBertModel, CharacterBertTokenizer\n",
    "\n",
    "#### LOADING BERT FOR CLASSIFICATION ####\n",
    "\n",
    "config = BertConfig.from_pretrained('bert-base-uncased', num_labels=2)  # binary classification\n",
    "model = BertForSequenceClassification(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c89eca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### REPLACING BERT WITH CHARACTER_BERT ####\n",
    "\n",
    "character_bert_model = CharacterBertModel.from_pretrained(\n",
    "    \"E:\\Documents\\Character Bert\\Hate Speech\\character-bert-hindi\")\n",
    "model.bert = character_bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f829941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CharacterBertTokenizer(strip_accents=None, do_lower_case=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab199977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"Text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e188b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "tokenized_dataset = review_dataset.map(tokenize_function, batched=True, remove_columns=[\"Text\"])\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8451fd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b93b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.filter(lambda x:x if x[\"Label\"] != 'Label' else None)\n",
    "tokenized_dataset = tokenized_dataset.filter(lambda x:x if (x[\"Label\"] != 'Label' and x[\"Label\"] != 'neutral') else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b3c677",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00db1906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_label(example):\n",
    "    #mapping = {\"neutral\":0, \"positive\":1, \"negative\":2}\n",
    "    mapping = {\"positive\":0, \"negative\":1}\n",
    "    example['Label'] = mapping[example['Label']]\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46142a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.map(assign_label)\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"Label\", \"labels\")\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "tokenized_dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbdeae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [tokenized_dataset[i] for i in range(20)]\n",
    "samples\n",
    "\n",
    "for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acefe412",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_dataset[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93559af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsampled_dataset = tokenized_dataset.train_test_split(\n",
    "#     train_size=0.8, seed=42\n",
    "# )\n",
    "# downsampled_dataset\n",
    "\n",
    "stratify_column_name = \"labels\"\n",
    "\n",
    "# create class label column and stratify\n",
    "downsampled_dataset = tokenized_dataset.class_encode_column(\n",
    "    stratify_column_name\n",
    ").train_test_split(\n",
    "    test_size=0.2, \n",
    "    seed = 42,\n",
    "    #stratify_by_column=stratify_column_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abab80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a34189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 32\n",
    "# batch_size = 4\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    downsampled_dataset[\"train\"], shuffle=True, batch_size=batch_size, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    downsampled_dataset[\"test\"], batch_size=batch_size, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccc01a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72da652a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "with torch.no_grad():\n",
    "    outputs = model(**batch)\n",
    "    print(outputs.loss, outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15803ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=4e-5)\n",
    "# optimizer = AdamW(model.parameters(), lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d7af02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bf8a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342eaf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric_fun = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    metric_result = metric_fun.compute(predictions=predictions, references=labels, average='macro')\n",
    "    return {\n",
    "        \"f1\": metric_result[\"f1\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4867ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  disable weights and biases logging\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adf566d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 32\n",
    "# batch_size = 16\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(downsampled_dataset[\"train\"]) // batch_size\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    report_to = None,\n",
    "    output_dir=\"models/bert-unigram-bengali-classifier\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    #learning_rate=3e-5,\n",
    "    learning_rate=4e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=8,\n",
    "    #num_train_epochs=7,\n",
    "    #push_to_hub=True,\n",
    "    fp16=True,\n",
    "    logging_steps=logging_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901aaefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=downsampled_dataset[\"train\"],\n",
    "    eval_dataset=downsampled_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3456882c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6598241a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e80ce93",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_f1 = []\n",
    "\n",
    "for obj in trainer.state.log_history:\n",
    "    if 'eval_f1' in obj.keys():\n",
    "        batch_f1.append(obj['eval_f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c861a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(batch_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c63766c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0074825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2725c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = list()\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# First make the kfold object\n",
    "folds = StratifiedKFold(n_splits=5)\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.shuffle(seed=42)\n",
    "\n",
    "# Now make our splits based off of the labels. \n",
    "# We can use `np.zeros()` here since it only works off of indices, we really care about the labels\n",
    "splits = folds.split(np.zeros(tokenized_dataset.num_rows), tokenized_dataset[\"labels\"])\n",
    "\n",
    "# In this case I'm overriding the train/val/test\n",
    "for train_idxs, val_idxs in splits:\n",
    "    fold_dataset = DatasetDict({\n",
    "    \"train\":tokenized_dataset.select(train_idxs),\n",
    "    \"validation\":tokenized_dataset.select(val_idxs),\n",
    "    })\n",
    "\n",
    "    #### LOADING BERT FOR CLASSIFICATION ####\n",
    "\n",
    "    config = BertConfig.from_pretrained('bert-base-uncased', num_labels=2)  # binary classification\n",
    "    model = BertForSequenceClassification(config=config)\n",
    "    \n",
    "    #### REPLACING BERT WITH CHARACTER_BERT ####\n",
    "    character_bert_model = CharacterBertModel.from_pretrained(\n",
    "        \"E:\\Documents\\Character Bert\\Hate Speech\\character-bert-hindi\")\n",
    "    model.bert = character_bert_model\n",
    "    model.to(device)\n",
    "\n",
    "    batch_size = 32\n",
    "    # Show the training loss with every epoch\n",
    "    logging_steps = len(fold_dataset[\"train\"]) // batch_size\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        report_to = None,\n",
    "        output_dir=\"models/bert-unigram-bengali-classifier\",\n",
    "        overwrite_output_dir=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",\n",
    "        learning_rate=3e-5,\n",
    "        weight_decay=0.01,\n",
    "        warmup_ratio=0.1,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=6,\n",
    "        fp16=True,\n",
    "        logging_steps=logging_steps,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=fold_dataset[\"train\"],\n",
    "        eval_dataset=fold_dataset[\"validation\"],\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    fold_f1 = []\n",
    "\n",
    "    for obj in trainer.state.log_history:\n",
    "        if 'eval_f1' in obj.keys():\n",
    "            fold_f1.append(obj['eval_f1'])\n",
    "            \n",
    "    scores.append(max(fold_f1))\n",
    "    print(max(fold_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e9d83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a597c8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(scores)/len(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
