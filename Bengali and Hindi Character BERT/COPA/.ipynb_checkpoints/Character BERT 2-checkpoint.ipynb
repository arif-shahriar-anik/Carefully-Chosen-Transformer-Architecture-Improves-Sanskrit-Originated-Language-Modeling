{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "209e3f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/arifa/.cache/huggingface/datasets/json/default-4aa02a47edd70562/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"json\", data_files=\"..\\datasets\\copa-translated\\\\hi\\\\train.jsonl\", \\\n",
    "                             split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1cb0550",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/arifa/.cache/huggingface/datasets/json/default-bfbd5b7cdea343df/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    }
   ],
   "source": [
    "val_dataset = load_dataset(\"json\", data_files=\"..\\datasets\\copa-translated\\\\hi\\\\val.jsonl\", \\\n",
    "                            split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f25a14c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/arifa/.cache/huggingface/datasets/json/default-0e1a32d75e6ec46c/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    }
   ],
   "source": [
    "test_dataset = load_dataset(\"json\", data_files=\"..\\datasets\\copa-translated\\\\hi\\\\test.jsonl\", \\\n",
    "                             split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0161e6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertConfig, CharacterBertModel, CharacterBertTokenizer\n",
    "\n",
    "tokenizer = CharacterBertTokenizer(strip_accents=None, do_lower_case=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fdc4473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.DataFrame(columns=['premise', 'cause', 'label'])\n",
    "val_df = pd.DataFrame(columns=['premise', 'cause','label'])\n",
    "test_df = pd.DataFrame(columns=['premise', 'cause','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8fc03b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "choice_names = ['choice1', 'choice2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20322843",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = {\n",
    "    'cause': ' कारण क्या है? ',\n",
    "    'effect':' परिणाम क्या है? '\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e01d81a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'क्योंक'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = \"क्योंक|\"\n",
    "temp[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6010dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, df):\n",
    "    idx = 0\n",
    "    for sample in examples:\n",
    "        df.loc[idx] = pd.Series({'premise':sample[\"premise\"]+ translation[sample[\"question\"]] +sample['choice1'], \\\n",
    "                                 'cause':sample[\"premise\"]+ translation[sample[\"question\"]]  +sample['choice2'],\\\n",
    "                                     'label': sample[\"label\"]})\n",
    "        idx = idx + 1\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ee7545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = preprocess_function(train_dataset, train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abca64ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = preprocess_function(val_dataset, val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e33f8f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = preprocess_function(test_dataset, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "737b3711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "362"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82eb26cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "val_ds = Dataset.from_pandas(val_df)\n",
    "test_ds = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ea152b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = DatasetDict()\n",
    "datasets['train'] = train_ds\n",
    "datasets['validation'] = val_ds\n",
    "datasets['test'] = test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0aed53e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': 'मेरे शरीर ने घास पर छाया डाली। कारण क्या है? सूरज उग रहा था।',\n",
       " 'cause': 'मेरे शरीर ने घास पर छाया डाली। कारण क्या है? घास काटी गई।',\n",
       " 'label': 0,\n",
       " '__index_level_0__': 0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fe16823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': ['मेरे शरीर ने घास पर छाया डाली।'],\n",
       " 'choice1': ['सूरज उग रहा था।'],\n",
       " 'choice2': ['घास काटी गई।'],\n",
       " 'question': ['cause'],\n",
       " 'idx': [0],\n",
       " 'label': [0]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f74a9a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"premise\"],example[\"cause\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edf215d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'cause', 'label', '__index_level_0__'],\n",
       "        num_rows: 362\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['premise', 'cause', 'label', '__index_level_0__'],\n",
       "        num_rows: 88\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['premise', 'cause', 'label', '__index_level_0__'],\n",
       "        num_rows: 449\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06f0df9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/362 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/88 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/449 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True, remove_columns=['premise','cause','__index_level_0__'])\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a96082e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 362\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 88\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 449\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ccca171a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def assign_label(example):\n",
    "#     mapping = {' परिणाम क्या है? 0':0, ' परिणाम क्या है? 1':1,' कारण क्या है? 0':2, ' कारण क्या है? 1':3}\n",
    "#     example['labels'] = mapping[example['label']]\n",
    "#     return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4af3b9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets = tokenized_datasets.map(assign_label).remove_columns('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7aba82e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_datasets[\"train\"]['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17027574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] मेरे शरीर ने घास पर छाया डाली । कारण क्या है? सूरज उग रहा था । [SEP] मेरे शरीर ने घास पर छाया डाली । कारण क्या है? घास काटी गई । [SEP]\n",
      "0\n",
      "[CLS] महिला ने अपने दोस्त के कठिन व्यवहार को सहन किया । कारण क्या है? महिला को पता था कि उसका दोस्त कठिन समय से गुजर रहा है । [SEP] महिला ने अपने दोस्त के कठिन व्यवहार को सहन किया । कारण क्या है? महिला को लगा कि उसके दोस्त ने उसकी दया का फायदा उठाया । [SEP]\n",
      "0\n",
      "[CLS] महिलाएं कॉफी के लिए मिलीं । कारण क्या है? एक नए स्थान में कैफे फिर से खुल गया । [SEP] महिलाएं कॉफी के लिए मिलीं । कारण क्या है? वे एक - दूसरे को पकड़ना चाहते थे । [SEP]\n",
      "1\n",
      "[CLS] धावक ने शॉर्ट्स पहनी थी । कारण क्या है? पूर्वानुमान में उच्च तापमान की भविष्यवाणी की गई थी । [SEP] धावक ने शॉर्ट्स पहनी थी । कारण क्या है? उसने समुद्र तट के साथ दौड़ने की योजना बनाई । [SEP]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for index, sample in enumerate(tokenized_datasets[\"train\"][:5]):\n",
    "    print(tokenizer.decode(tokenized_datasets[\"train\"]['input_ids'][index]))\n",
    "    print(tokenized_datasets[\"train\"]['label'][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7433a341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] आइटम को बबल रैप में पैक किया गया था । कारण क्या है? यह नाजुक था । [SEP] आइटम को बबल रैप में पैक किया गया था । कारण क्या है? छोटा था । [SEP]\n",
      "0\n",
      "[CLS] मैंने अपनी जेबें खाली कर दीं । परिणाम क्या है? मैंने एक टिकट स्टब को पुनः प्राप्त किया । [SEP] मैंने अपनी जेबें खाली कर दीं । परिणाम क्या है? मुझे एक हथियार मिला । [SEP]\n",
      "0\n",
      "[CLS] दीमक ने घर पर आक्रमण कर दिया । परिणाम क्या है? दीमक घर से गायब हो गए । [SEP] दीमक ने घर पर आक्रमण कर दिया । परिणाम क्या है? दीमक घर में लकड़ी के माध्यम से खाया । [SEP]\n",
      "1\n",
      "[CLS] यात्री सीमा पर पहुंच गए । परिणाम क्या है? गश्ती एजेंट ने उनके पासपोर्ट की जाँच की । [SEP] यात्री सीमा पर पहुंच गए । परिणाम क्या है? गश्त एजेंट ने उन पर तस्करी का आरोप लगाया । [SEP]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for index, sample in enumerate(tokenized_datasets[\"test\"][:5]):\n",
    "    print(tokenizer.decode(tokenized_datasets[\"test\"]['input_ids'][index]))\n",
    "    print(tokenized_datasets[\"test\"]['label'][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ef0041f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 362\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 88\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 449\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65cb9c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab6c0db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a041fa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "# set_seed(42)\n",
    "set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b3a51ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\arifa/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertConfig, CharacterBertModel, CharacterBertTokenizer,\\\n",
    "     TrainingArguments, Trainer\n",
    "\n",
    "#### LOADING BERT FOR CLASSIFICATION ####\n",
    "\n",
    "config = BertConfig.from_pretrained('bert-base-uncased', num_labels=2, classifier_dropout=0.1)  # binary classification\n",
    "model = BertForSequenceClassification(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f3c10f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30522, 768, padding_idx=0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bert.embeddings.word_embeddings  # wordpiece embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e2806fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file E:\\Documents\\Character Bert\\Hate Speech\\character-bert-hindi\\config.json\n",
      "Model config CharacterBertConfig {\n",
      "  \"_name_or_path\": \"helboukkouri/character-bert\",\n",
      "  \"architectures\": [\n",
      "    \"CharacterBertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_character_bert.CharacterBertConfig\",\n",
      "    \"AutoModel\": \"modeling_character_bert.CharacterBertForPreTraining\",\n",
      "    \"AutoModelForMaskedLM\": \"modeling_character_bert.CharacterBertForMaskedLM\"\n",
      "  },\n",
      "  \"character_embeddings_dim\": 16,\n",
      "  \"cnn_activation\": \"relu\",\n",
      "  \"cnn_filters\": [\n",
      "    [\n",
      "      1,\n",
      "      32\n",
      "    ],\n",
      "    [\n",
      "      2,\n",
      "      32\n",
      "    ],\n",
      "    [\n",
      "      3,\n",
      "      64\n",
      "    ],\n",
      "    [\n",
      "      4,\n",
      "      128\n",
      "    ],\n",
      "    [\n",
      "      5,\n",
      "      256\n",
      "    ],\n",
      "    [\n",
      "      6,\n",
      "      512\n",
      "    ],\n",
      "    [\n",
      "      7,\n",
      "      1024\n",
      "    ]\n",
      "  ],\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_word_length\": 50,\n",
      "  \"mlm_vocab_size\": 30522,\n",
      "  \"model_type\": \"character_bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_highway_layers\": 2,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true\n",
      "}\n",
      "\n",
      "loading weights file E:\\Documents\\Character Bert\\Hate Speech\\character-bert-hindi\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at E:\\Documents\\Character Bert\\Hate Speech\\character-bert-hindi were not used when initializing CharacterBertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing CharacterBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CharacterBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of CharacterBertModel were initialized from the model checkpoint at E:\\Documents\\Character Bert\\Hate Speech\\character-bert-hindi.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use CharacterBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#### REPLACING BERT WITH CHARACTER_BERT ####\n",
    "\n",
    "character_bert_model = CharacterBertModel.from_pretrained(\n",
    "    \"E:\\Documents\\Character Bert\\Hate Speech\\character-bert-hindi\")\n",
    "model.bert = character_bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c8361a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharacterCnn(\n",
       "  (char_conv_0): Conv1d(16, 32, kernel_size=(1,), stride=(1,))\n",
       "  (char_conv_1): Conv1d(16, 32, kernel_size=(2,), stride=(1,))\n",
       "  (char_conv_2): Conv1d(16, 64, kernel_size=(3,), stride=(1,))\n",
       "  (char_conv_3): Conv1d(16, 128, kernel_size=(4,), stride=(1,))\n",
       "  (char_conv_4): Conv1d(16, 256, kernel_size=(5,), stride=(1,))\n",
       "  (char_conv_5): Conv1d(16, 512, kernel_size=(6,), stride=(1,))\n",
       "  (char_conv_6): Conv1d(16, 1024, kernel_size=(7,), stride=(1,))\n",
       "  (_highways): Highway(\n",
       "    (_layers): ModuleList(\n",
       "      (0-1): 2 x Linear(in_features=2048, out_features=4096, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (_projection): Linear(in_features=2048, out_features=768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bert.embeddings.word_embeddings  # wordpieces are replaced with a CharacterCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cb0137c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  disable weights and biases logging\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "056872a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "batch_size=32\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"character_bert_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=2e-5,\n",
    "    #learning_rate=4e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    #num_train_epochs=2,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    #weight_decay=0.04,\n",
    "    #fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    #eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "71865add",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "trainer.remove_callback(transformers.integrations.TensorBoardCallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "88e79967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 449\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.701155960559845,\n",
       " 'eval_accuracy': 0.512249443207127,\n",
       " 'eval_runtime': 1.8708,\n",
       " 'eval_samples_per_second': 240.001,\n",
       " 'eval_steps_per_second': 8.018}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6bbbae53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 362\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 36\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36/36 00:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.703843</td>\n",
       "      <td>0.472160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.702336</td>\n",
       "      <td>0.507795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.707110</td>\n",
       "      <td>0.496659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 449\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 449\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 449\n",
      "  Batch size = 32\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=36, training_loss=0.6701971160040961, metrics={'train_runtime': 16.6725, 'train_samples_per_second': 65.137, 'train_steps_per_second': 2.159, 'total_flos': 1873413827551200.0, 'train_loss': 0.6701971160040961, 'epoch': 3.0})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "41bd5bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "eabd6736",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6950161457061768,\n",
       " 'eval_accuracy': 0.48863636363636365,\n",
       " 'eval_runtime': 0.3773,\n",
       " 'eval_samples_per_second': 233.206,\n",
       " 'eval_steps_per_second': 7.95,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(tokenized_datasets[\"validation\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
