{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "209e3f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/arifa/.cache/huggingface/datasets/json/default-4aa02a47edd70562/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"json\", data_files=\"..\\datasets\\copa-translated\\\\hi\\\\train.jsonl\", \\\n",
    "                             split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f1cb0550",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/arifa/.cache/huggingface/datasets/json/default-bfbd5b7cdea343df/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    }
   ],
   "source": [
    "val_dataset = load_dataset(\"json\", data_files=\"..\\datasets\\copa-translated\\\\hi\\\\val.jsonl\", \\\n",
    "                            split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f25a14c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/arifa/.cache/huggingface/datasets/json/default-0e1a32d75e6ec46c/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    }
   ],
   "source": [
    "test_dataset = load_dataset(\"json\", data_files=\"..\\datasets\\copa-translated\\\\hi\\\\test.jsonl\", \\\n",
    "                             split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "13ddcec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset = load_dataset(\"ai4bharat/IndicCOPA\", \"translation-hi\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4d7b97f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset = test_dataset.remove_columns(\"changed\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0161e6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertConfig, CharacterBertModel, CharacterBertTokenizer\n",
    "\n",
    "tokenizer = CharacterBertTokenizer(strip_accents=None, do_lower_case=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4fdc4473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.DataFrame(columns=['premise', 'cause', 'labels'])\n",
    "val_df = pd.DataFrame(columns=['premise', 'cause','labels'])\n",
    "test_df = pd.DataFrame(columns=['premise', 'cause','labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b8fc03b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "choice_names = ['choice1', 'choice2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "20322843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# translation = {\n",
    "#     'cause': ' कारण क्या है? ',\n",
    "#     'effect':' परिणाम क्या है? '\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4b466eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = {\n",
    "    'cause': ' क्योंकि ',\n",
    "    'effect':' इसलिए '\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e01d81a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'क्योंक'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = \"क्योंक|\"\n",
    "temp[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c6010dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, df):\n",
    "    idx = 0\n",
    "    for sample in examples:\n",
    "        labels = [0.,0.]\n",
    "        labels[sample[\"label\"]] = 1.\n",
    "        df.loc[idx] = pd.Series({'premise':sample[\"premise\"][:-1]+ translation[sample[\"question\"]] +sample['choice1'], \\\n",
    "                                 'cause':sample[\"premise\"][:-1]+ translation[sample[\"question\"]]  +sample['choice2'],\\\n",
    "                                     'labels': labels})\n",
    "        idx = idx + 1\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5b819ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_function(examples, df):\n",
    "#     idx = 0\n",
    "#     for sample in examples:\n",
    "#         labels = [0.,0.]\n",
    "#         labels[sample[\"label\"]] = 1.\n",
    "#         df.loc[idx] = pd.Series({'premise':sample[\"premise\"]+ ' ' +sample['choice1'], \\\n",
    "#                                  'cause':sample[\"premise\"]+ ' '  +sample['choice2'],\\\n",
    "#                                      'labels': labels})\n",
    "#         idx = idx + 1\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8ee7545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = preprocess_function(train_dataset, train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "abca64ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = preprocess_function(val_dataset, val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e33f8f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = preprocess_function(test_dataset, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "737b3711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "362"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "82eb26cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "val_ds = Dataset.from_pandas(val_df)\n",
    "test_ds = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0ea152b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = DatasetDict()\n",
    "datasets['train'] = train_ds\n",
    "datasets['validation'] = val_ds\n",
    "datasets['test'] = test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a0aed53e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': 'मेरे शरीर ने घास पर छाया डाली क्योंकि सूरज उग रहा था।',\n",
       " 'cause': 'मेरे शरीर ने घास पर छाया डाली क्योंकि घास काटी गई।',\n",
       " 'labels': [1.0, 0.0],\n",
       " '__index_level_0__': 0}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6fe16823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': ['मेरे शरीर ने घास पर छाया डाली।'],\n",
       " 'choice1': ['सूरज उग रहा था।'],\n",
       " 'choice2': ['घास काटी गई।'],\n",
       " 'question': ['cause'],\n",
       " 'idx': [0],\n",
       " 'label': [0]}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f74a9a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"premise\"],example[\"cause\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "edf215d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'cause', 'labels', '__index_level_0__'],\n",
       "        num_rows: 362\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['premise', 'cause', 'labels', '__index_level_0__'],\n",
       "        num_rows: 88\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['premise', 'cause', 'labels', '__index_level_0__'],\n",
       "        num_rows: 449\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "06f0df9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/362 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/88 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/449 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True, remove_columns=['premise','cause','__index_level_0__'])\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3a96082e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 362\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 88\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 449\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ccca171a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def assign_label(example):\n",
    "#     mapping = {' परिणाम क्या है? 0':0, ' परिणाम क्या है? 1':1,' कारण क्या है? 0':2, ' कारण क्या है? 1':3}\n",
    "#     example['labels'] = mapping[example['label']]\n",
    "#     return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4af3b9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets = tokenized_datasets.map(assign_label).remove_columns('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7aba82e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_datasets[\"train\"]['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "17027574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] मेरे शरीर ने घास पर छाया डाली क्योंकि सूरज उग रहा था । [SEP] मेरे शरीर ने घास पर छाया डाली क्योंकि घास काटी गई । [SEP]\n",
      "[1.0, 0.0]\n",
      "[CLS] महिला ने अपने दोस्त के कठिन व्यवहार को सहन किया क्योंकि महिला को पता था कि उसका दोस्त कठिन समय से गुजर रहा है । [SEP] महिला ने अपने दोस्त के कठिन व्यवहार को सहन किया क्योंकि महिला को लगा कि उसके दोस्त ने उसकी दया का फायदा उठाया । [SEP]\n",
      "[1.0, 0.0]\n",
      "[CLS] महिलाएं कॉफी के लिए मिलीं क्योंकि एक नए स्थान में कैफे फिर से खुल गया । [SEP] महिलाएं कॉफी के लिए मिलीं क्योंकि वे एक - दूसरे को पकड़ना चाहते थे । [SEP]\n",
      "[0.0, 1.0]\n",
      "[CLS] धावक ने शॉर्ट्स पहनी थी क्योंकि पूर्वानुमान में उच्च तापमान की भविष्यवाणी की गई थी । [SEP] धावक ने शॉर्ट्स पहनी थी क्योंकि उसने समुद्र तट के साथ दौड़ने की योजना बनाई । [SEP]\n",
      "[1.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "for index, sample in enumerate(tokenized_datasets[\"train\"][:5]):\n",
    "    print(tokenizer.decode(tokenized_datasets[\"train\"]['input_ids'][index]))\n",
    "    print(tokenized_datasets[\"train\"]['labels'][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7433a341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] आइटम को बबल रैप में पैक किया गया था क्योंकि यह नाजुक था । [SEP] आइटम को बबल रैप में पैक किया गया था क्योंकि छोटा था । [SEP]\n",
      "[1.0, 0.0]\n",
      "[CLS] मैंने अपनी जेबें खाली कर दीं इसलिए मैंने एक टिकट स्टब को पुनः प्राप्त किया । [SEP] मैंने अपनी जेबें खाली कर दीं इसलिए मुझे एक हथियार मिला । [SEP]\n",
      "[1.0, 0.0]\n",
      "[CLS] दीमक ने घर पर आक्रमण कर दिया इसलिए दीमक घर से गायब हो गए । [SEP] दीमक ने घर पर आक्रमण कर दिया इसलिए दीमक घर में लकड़ी के माध्यम से खाया । [SEP]\n",
      "[0.0, 1.0]\n",
      "[CLS] यात्री सीमा पर पहुंच गए इसलिए गश्ती एजेंट ने उनके पासपोर्ट की जाँच की । [SEP] यात्री सीमा पर पहुंच गए इसलिए गश्त एजेंट ने उन पर तस्करी का आरोप लगाया । [SEP]\n",
      "[1.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "for index, sample in enumerate(tokenized_datasets[\"test\"][:5]):\n",
    "    print(tokenizer.decode(tokenized_datasets[\"test\"]['input_ids'][index]))\n",
    "    print(tokenized_datasets[\"test\"]['labels'][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0ef0041f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 362\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 88\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 449\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4c0f0773",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6964bbe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0.])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"][0]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f7da0002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[259, 257, 260,  ..., 261, 261, 261],\n",
       "        [259, 225, 165,  ..., 261, 261, 261],\n",
       "        [259, 225, 165,  ..., 261, 261, 261],\n",
       "        ...,\n",
       "        [259, 225, 165,  ..., 261, 261, 261],\n",
       "        [259, 225, 166,  ..., 261, 261, 261],\n",
       "        [259, 258, 260,  ..., 261, 261, 261]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train']['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8e358bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #forward pass\n",
    "# outputs = model(input_ids=tokenized_datasets['train']['input_ids'][0].unsqueeze(0), labels=tokenized_datasets['train'][0]['labels'].unsqueeze(0))\n",
    "# outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "65cb9c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "# from transformers import EvalPrediction\n",
    "# import torch\n",
    "    \n",
    "# # source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "# def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "#     # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "#     sigmoid = torch.nn.Sigmoid()\n",
    "#     probs = sigmoid(torch.Tensor(predictions))\n",
    "#     # next, use threshold to turn them into integer predictions\n",
    "#     y_pred = np.zeros(probs.shape)\n",
    "#     y_pred[np.where(probs >= threshold)] = 1\n",
    "#     # finally, compute metrics\n",
    "#     y_true = labels\n",
    "#     accuracy = accuracy_score(y_true, y_pred)\n",
    "#     # return as dictionary\n",
    "#     metrics = {'accuracy': accuracy}\n",
    "#     return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5166dfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "    \n",
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "#     print(\"y true\",labels)\n",
    "#     print(\"y pred\",y_pred)\n",
    "    y_true = np.argmax(labels,axis=1)\n",
    "    y_pred = np.argmax(y_pred,axis=1)\n",
    "#     print(\"y true\",y_true)\n",
    "#     print(\"y pred\",y_pred)\n",
    "    \n",
    "    accuracy = np.equal(y_true, y_pred).mean()\n",
    "    # return as dictionary\n",
    "    metrics = {'accuracy': accuracy}\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "96a46ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi_label_metrics(np.array([[0.3614,  -0.4645],[-0.3614,  0.4645]]), np.array([[1., 0.],[1., 0.]]), 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ab6c0db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a041fa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "set_seed(55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b3a51ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\arifa/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"transformers_version\": \"4.16.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\arifa/.cache\\huggingface\\transformers\\a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertConfig, CharacterBertModel, CharacterBertTokenizer,\\\n",
    "     TrainingArguments, Trainer\n",
    "\n",
    "#### LOADING BERT FOR CLASSIFICATION ####\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2, problem_type=\"multi_label_classification\")  # binary classification\n",
    "# model = BertForSequenceClassification(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0f3c10f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30522, 768, padding_idx=0)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bert.embeddings.word_embeddings  # wordpiece embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e2806fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file E:\\Documents\\Character Bert\\Hate Speech\\character-bert-hindi\\config.json\n",
      "Model config CharacterBertConfig {\n",
      "  \"_name_or_path\": \"helboukkouri/character-bert\",\n",
      "  \"architectures\": [\n",
      "    \"CharacterBertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_character_bert.CharacterBertConfig\",\n",
      "    \"AutoModel\": \"modeling_character_bert.CharacterBertForPreTraining\",\n",
      "    \"AutoModelForMaskedLM\": \"modeling_character_bert.CharacterBertForMaskedLM\"\n",
      "  },\n",
      "  \"character_embeddings_dim\": 16,\n",
      "  \"cnn_activation\": \"relu\",\n",
      "  \"cnn_filters\": [\n",
      "    [\n",
      "      1,\n",
      "      32\n",
      "    ],\n",
      "    [\n",
      "      2,\n",
      "      32\n",
      "    ],\n",
      "    [\n",
      "      3,\n",
      "      64\n",
      "    ],\n",
      "    [\n",
      "      4,\n",
      "      128\n",
      "    ],\n",
      "    [\n",
      "      5,\n",
      "      256\n",
      "    ],\n",
      "    [\n",
      "      6,\n",
      "      512\n",
      "    ],\n",
      "    [\n",
      "      7,\n",
      "      1024\n",
      "    ]\n",
      "  ],\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_word_length\": 50,\n",
      "  \"mlm_vocab_size\": 30522,\n",
      "  \"model_type\": \"character_bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_highway_layers\": 2,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true\n",
      "}\n",
      "\n",
      "loading weights file E:\\Documents\\Character Bert\\Hate Speech\\character-bert-hindi\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at E:\\Documents\\Character Bert\\Hate Speech\\character-bert-hindi were not used when initializing CharacterBertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing CharacterBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CharacterBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of CharacterBertModel were initialized from the model checkpoint at E:\\Documents\\Character Bert\\Hate Speech\\character-bert-hindi.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use CharacterBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#### REPLACING BERT WITH CHARACTER_BERT ####\n",
    "\n",
    "character_bert_model = CharacterBertModel.from_pretrained(\n",
    "    \"E:\\Documents\\Character Bert\\Hate Speech\\character-bert-hindi\")\n",
    "model.bert = character_bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c8361a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharacterCnn(\n",
       "  (char_conv_0): Conv1d(16, 32, kernel_size=(1,), stride=(1,))\n",
       "  (char_conv_1): Conv1d(16, 32, kernel_size=(2,), stride=(1,))\n",
       "  (char_conv_2): Conv1d(16, 64, kernel_size=(3,), stride=(1,))\n",
       "  (char_conv_3): Conv1d(16, 128, kernel_size=(4,), stride=(1,))\n",
       "  (char_conv_4): Conv1d(16, 256, kernel_size=(5,), stride=(1,))\n",
       "  (char_conv_5): Conv1d(16, 512, kernel_size=(6,), stride=(1,))\n",
       "  (char_conv_6): Conv1d(16, 1024, kernel_size=(7,), stride=(1,))\n",
       "  (_highways): Highway(\n",
       "    (_layers): ModuleList(\n",
       "      (0-1): 2 x Linear(in_features=2048, out_features=4096, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (_projection): Linear(in_features=2048, out_features=768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bert.embeddings.word_embeddings  # wordpieces are replaced with a CharacterCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "cb0137c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  disable weights and biases logging\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "056872a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "batch_size=32\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"character_bert_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=1e-5,\n",
    "    #learning_rate=3e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    #num_train_epochs=2,\n",
    "    num_train_epochs=1,\n",
    "    #warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    #weight_decay=0.09,\n",
    "    #weight_decay=0.04,\n",
    "    #fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    #eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "71865add",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "trainer.remove_callback(transformers.integrations.TensorBoardCallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "88e79967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7115965485572815,\n",
       " 'eval_accuracy': 0.5454545454545454,\n",
       " 'eval_runtime': 0.3175,\n",
       " 'eval_samples_per_second': 277.184,\n",
       " 'eval_steps_per_second': 9.449}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6bbbae53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 362\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 12\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.703593</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 32\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12, training_loss=0.7022349834442139, metrics={'train_runtime': 3.4666, 'train_samples_per_second': 104.426, 'train_steps_per_second': 3.462, 'total_flos': 537601364704800.0, 'train_loss': 0.7022349834442139, 'epoch': 1.0})"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "41bd5bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "eabd6736",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 449\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6950348019599915,\n",
       " 'eval_accuracy': 0.534521158129176,\n",
       " 'eval_runtime': 1.5865,\n",
       " 'eval_samples_per_second': 283.008,\n",
       " 'eval_steps_per_second': 9.455,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(tokenized_datasets[\"test\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
