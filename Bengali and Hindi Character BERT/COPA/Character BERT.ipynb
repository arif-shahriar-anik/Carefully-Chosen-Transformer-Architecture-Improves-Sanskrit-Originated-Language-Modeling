{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd998685",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "import torch\n",
    "\n",
    "# random_seed = 42\n",
    "random_seed = 80\n",
    "\n",
    "set_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fa122b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CharacterBertModel\n",
    "import torch.nn as nn\n",
    "\n",
    "class CharacterBERTForMultipleChoice(nn.Module):\n",
    "    def __init__(self):\n",
    "          super(CharacterBERTForMultipleChoice, self).__init__()\n",
    "          self.bert = CharacterBertModel.from_pretrained(\"E:\\Documents\\Character Bert\\Hate Speech\\character-bert-hindi\")\n",
    "          ### New layers:\n",
    "          self.linear1 = nn.Linear(768, 1) ## 1 is the number of classes in this example\n",
    "\n",
    "    def forward(self, input_ids,attention_mask,token_type_ids,position_ids,head_mask,\\\n",
    "                inputs_embeds,output_attentions,output_hidden_states,return_dict):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask,\n",
    "            token_type_ids,\n",
    "            position_ids,\n",
    "            head_mask,\n",
    "            inputs_embeds,\n",
    "            output_attentions,\n",
    "            output_hidden_states,\n",
    "            return_dict)\n",
    "        \n",
    "        pooled_output = outputs[1]\n",
    "        logits = self.linear1(pooled_output)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209e3f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"json\", data_files=\"..\\datasets\\copa-translated\\\\hi\\\\train.jsonl\", \\\n",
    "                             split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cb0550",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = load_dataset(\"json\", data_files=\"..\\datasets\\copa-translated\\\\hi\\\\val.jsonl\", \\\n",
    "                            split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25a14c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = load_dataset(\"json\", data_files=\"..\\datasets\\copa-translated\\\\hi\\\\test.jsonl\", \\\n",
    "                             split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99241519",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "datasets = DatasetDict()\n",
    "datasets['train'] = train_dataset\n",
    "datasets['validation'] = val_dataset\n",
    "datasets['test'] = test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4ecbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# datasets = load_dataset(\"indic_glue\",\"copa.hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b417a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac25dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"train\"][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a882b123",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"test\"][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40831c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.set_format(\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce2da35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get label counts for both classes\n",
    "label_counts = datasets[\"train\"][\"label\"].value_counts()\n",
    "num_labels = (len(label_counts.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afd418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33520fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.reset_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0161e6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertConfig, CharacterBertModel, CharacterBertTokenizer\n",
    "\n",
    "tokenizer = CharacterBertTokenizer(strip_accents=None, do_lower_case=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fc03b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "choice_names = ['choice1', 'choice2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6010dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    premise = [[context] * 2 for context in examples[\"premise\"]]\n",
    "    cause = [[f\"{examples[choice][i]}\" for choice in choice_names] for i,_ in enumerate(premise)]\n",
    "\n",
    "    premise = sum(premise, [])\n",
    "    cause = sum(cause, [])\n",
    "    \n",
    "#     print(premise)\n",
    "#     print(cause)\n",
    "    \n",
    "\n",
    "    tokenized_examples = tokenizer(premise, cause, truncation=True, max_length=128, padding='max_length')\n",
    "#     print(len(tokenized_examples))\n",
    "    return {k: [v[i : i + 2] for i in range(0, len(v), 2)] for k, v in tokenized_examples.items()}\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d4330d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = preprocess_function(datasets[\"train\"][:1])\n",
    "len(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3957f337",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a51ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, CharacterBertTokenizer, BertModel,\\\n",
    "                        TrainingArguments, Trainer, CharacterBertConfig, CharacterBertModel\n",
    "\n",
    "model = CharacterBERTForMultipleChoice()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cde03ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c411e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets = tokenized_datasets.remove_columns(['premise', 'choice1', 'choice2', 'question', 'idx'])\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['premise', 'choice1', 'choice2', 'question'])\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146876d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets[\"train\"][0]['input_ids'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52e209d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets[\"train\"][0]['attention_mask'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6840fbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets[\"train\"][0]['token_type_ids'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e12f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aecc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "# optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d352b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "# num_epochs = 3\n",
    "num_epochs = 2\n",
    "num_training_steps = num_epochs * len(tokenized_datasets[\"train\"])\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    #num_warmup_steps=0.05 * num_training_steps,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a6239f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cc17e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# classifier = nn.Linear(768, 1).to(device)\n",
    "loss_fct = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7252f332",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in tokenized_datasets[\"train\"]:\n",
    "    item = {k: v.to(device) for k, v in item.items()}\n",
    "    position_ids=None\n",
    "    head_mask=None\n",
    "    inputs_embeds=None\n",
    "    output_attentions=None\n",
    "    output_hidden_states=None\n",
    "    return_dict=None\n",
    "    logits = model(\n",
    "            input_ids=item[\"input_ids\"],\n",
    "            attention_mask=item[\"attention_mask\"],\n",
    "            token_type_ids=item[\"token_type_ids\"],\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "    reshaped_logits = logits.view(-1, 2)\n",
    "    print(reshaped_logits)\n",
    "    print(item[\"label\"].unsqueeze(0))\n",
    "    loss = loss_fct(reshaped_logits, item[\"label\"].unsqueeze(0))\n",
    "    print(loss)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee067651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748b1f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "accumulation_steps = 32\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    training_losses = []\n",
    "    for i, item in enumerate(tokenized_datasets[\"train\"]):\n",
    "        item = {k: v.to(device) for k, v in item.items()}\n",
    "        logits = model(\n",
    "                input_ids=item[\"input_ids\"],\n",
    "                attention_mask=item[\"attention_mask\"],\n",
    "                token_type_ids=item[\"token_type_ids\"],\n",
    "                position_ids=None,\n",
    "                head_mask=None,\n",
    "                inputs_embeds=None,\n",
    "                output_attentions=None,\n",
    "                output_hidden_states=None,\n",
    "                return_dict=None\n",
    "            )\n",
    "        reshaped_logits = logits.view(-1, 2)\n",
    "        loss = loss_fct(reshaped_logits, item[\"label\"].unsqueeze(0))\n",
    "        loss.backward()\n",
    "        \n",
    "        if (i+1) % accumulation_steps == 0:             # Wait for several backward steps\n",
    "            optimizer.step()                            # Now we can do an optimizer step\n",
    "            optimizer.zero_grad()                       # Reset gradients tensors\n",
    "    \n",
    "        lr_scheduler.step()\n",
    "        progress_bar.update(1)\n",
    "        training_losses.append(loss.repeat(1))\n",
    "    \n",
    "    training_losses = torch.cat(training_losses)\n",
    "    training_losses = training_losses[: len(tokenized_datasets[\"train\"])]\n",
    "    \n",
    "    print(f\">>> Epoch {epoch}: Training Loss: {torch.mean(training_losses)}\")\n",
    "    \n",
    "    progress_bar2 = tqdm(range(len(tokenized_datasets[\"validation\"])))\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    training_losses = []\n",
    "    for item in tokenized_datasets[\"validation\"]:\n",
    "        item = {k: v.to(device) for k, v in item.items()}\n",
    "        logits = model(\n",
    "                input_ids=item[\"input_ids\"],\n",
    "                attention_mask=item[\"attention_mask\"],\n",
    "                token_type_ids=item[\"token_type_ids\"],\n",
    "                position_ids=None,\n",
    "                head_mask=None,\n",
    "                inputs_embeds=None,\n",
    "                output_attentions=None,\n",
    "                output_hidden_states=None,\n",
    "                return_dict=None\n",
    "            )\n",
    "        reshaped_logits = logits.view(-1, 2)\n",
    "        pred = torch.argmax(reshaped_logits)\n",
    "        metric.add_batch(predictions=pred.unsqueeze(0), references=item[\"label\"].unsqueeze(0))\n",
    "        progress_bar2.update(1)\n",
    "\n",
    "    acc = metric.compute()\n",
    "    print(\"accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2d2cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(len(tokenized_datasets[\"test\"])))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "training_losses = []\n",
    "for item in tokenized_datasets[\"test\"]:\n",
    "    item = {k: v.to(device) for k, v in item.items()}\n",
    "    logits = model(\n",
    "            input_ids=item[\"input_ids\"],\n",
    "            attention_mask=item[\"attention_mask\"],\n",
    "            token_type_ids=item[\"token_type_ids\"],\n",
    "            position_ids=None,\n",
    "            head_mask=None,\n",
    "            inputs_embeds=None,\n",
    "            output_attentions=None,\n",
    "            output_hidden_states=None,\n",
    "            return_dict=None\n",
    "        )\n",
    "    reshaped_logits = logits.view(-1, 2)\n",
    "    pred = torch.argmax(reshaped_logits)\n",
    "    predictions.append(pred.tolist())\n",
    "    metric.add_batch(predictions=pred.unsqueeze(0), references=item[\"label\"].unsqueeze(0))\n",
    "    progress_bar.update(1)\n",
    "        \n",
    "acc = metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af7a96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916d81ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tokenized_datasets['test']['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb63cb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d88e755",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['choice1', 'choice2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08a234e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from seaborn import heatmap\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#plot heatmap of confusion matrix\n",
    "mat = confusion_matrix(y_true, y_preds)\n",
    "heatmap(mat, cmap=\"Pastel1_r\", fmt=\"d\", xticklabels=target_names, yticklabels=target_names, annot=True)\n",
    "\n",
    "#add overall title to plot\n",
    "plt.title('Confusion matrix for COPA', fontsize = 12) # title with fontsize 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a00dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
