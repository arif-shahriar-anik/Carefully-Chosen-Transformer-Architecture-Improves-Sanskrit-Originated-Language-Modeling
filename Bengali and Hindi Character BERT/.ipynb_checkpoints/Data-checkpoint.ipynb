{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08066abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "import os\n",
    "import random\n",
    "import logging\n",
    "from collections import OrderedDict\n",
    "import h5py\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, CharacterBertTokenizer\n",
    "\n",
    "from utils.text import convert_to_unicode, truncate_seq_pair\n",
    "\n",
    "WORKDIR = os.environ['WORKDIR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e70a669",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreTrainingExample:\n",
    "    r\"\"\"\n",
    "    A single pre-training example: sentence pair with targets for NSP and MLM.\n",
    "    Args:\n",
    "    tokens (:obj:`list(str)`):\n",
    "        Input tokens: [CLS] A A A [SEP] B B B [SEP]\n",
    "    segment_ids (:obj:`list(int)`):\n",
    "        Segment ids corresponding to the input tokens: 0 0 0 0 0 1 1 1 1\n",
    "    mlm_positions (:obj:`list(int)`):\n",
    "        Indices of masked (altered) tokens.\n",
    "    mlm_labels (:obj:`list(str)`):\n",
    "        Original tokens for masked (altered) positions.\n",
    "    is_random_next (:obj:`bool`):\n",
    "        Whether text span B is random (NSP target is False when this is True).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "        tokens: List[str],\n",
    "        segment_ids: List[int],\n",
    "        mlm_positions: List[int],\n",
    "        mlm_labels: List[str],\n",
    "        is_random_next: bool\n",
    "    ):\n",
    "        self.tokens = tokens\n",
    "        self.segment_ids = segment_ids\n",
    "        self.is_random_next = is_random_next\n",
    "        self.mlm_positions = mlm_positions\n",
    "        self.mlm_labels = mlm_labels\n",
    "\n",
    "    def __str__(self):\n",
    "        string = \"\"\n",
    "        string += \"tokens: %s\\n\" % \\\n",
    "            (\" \".join([convert_to_unicode(x) for x in self.tokens]))\n",
    "        string += \"segment_ids: %s\\n\" % \\\n",
    "            (\" \".join([str(x) for x in self.segment_ids]))\n",
    "        string += \"is_random_next: %s\\n\" % \\\n",
    "            self.is_random_next\n",
    "        string += \"mlm_positions: %s\\n\" % \\\n",
    "            (\" \".join([str(x) for x in self.mlm_positions]))\n",
    "        string += \"mlm_labels: %s\\n\" % \\\n",
    "            (\" \".join([convert_to_unicode(x) for x in self.mlm_labels]))\n",
    "        string += \"\\n\"\n",
    "        return string\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd0553e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreTrainingDataGenerator:\n",
    "    r\"\"\"\n",
    "    Generates MLM/NSP examples for BERT/CharacterBERT from a single shard.\n",
    "    Args:\n",
    "    shard_fpath (:obj:`str`):\n",
    "        Path to a corpus shard contraining one sentence per line and a blank\n",
    "        line between sentences from different documents.\n",
    "    output_directory (:obj:`str`)\n",
    "        'Path to a directory for saving the output .hdf5 file.'\n",
    "    is_character_bert (:obj:`bool`):\n",
    "        Whether to create pre-training for CharacterBERT of BERT. When this is\n",
    "        True, data is generated for CharacterBERT.\n",
    "    duplication_factor (:obj:`int`):\n",
    "        How many iterations over the shard contents. The higher this value the\n",
    "        more we randomly generate examples from the same sentences/documents.\n",
    "    max_input_length (:obj:`int`):\n",
    "        Maximum sequence length for the model input. Usually this is set to 512.\n",
    "    short_input_probability (:obj:`float`):\n",
    "        Probability of generating an exemple with a shorter input. When this\n",
    "        happens, a random number between 2 and `max_input_length` is picked and\n",
    "        a short input of that size is generated.\n",
    "    max_masked_tokens_per_input (:obj:`int`):\n",
    "        Hard limit on the number of tokens that can be masked.\n",
    "    masked_tokens_ratio (:obj:`float`):\n",
    "        Proportion of input tokens that we attempt to mask. If this is higher than\n",
    "        `max_masked_tokens_per_input` then we only mask the allowed maximum number.\n",
    "    random_seed (:obj:`float`):\n",
    "        A random seed to set before randomly generating pre-training examples.\n",
    "    verbose (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
    "        Whether to print logs and progress bars. It is useful to turn this off\n",
    "        if running multiple processes (too many logs)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "        shard_fpath: str,\n",
    "        output_directory: str,\n",
    "        is_character_bert: bool,\n",
    "        duplication_factor: int,\n",
    "        max_input_length: int,\n",
    "        short_input_probability: float,\n",
    "        max_masked_tokens_per_input: int,\n",
    "        masked_tokens_ratio: float,\n",
    "        random_seed: int,\n",
    "        verbose: Optional[bool] = True,\n",
    "    ):\n",
    "        self.verbose = verbose\n",
    "        if not self.verbose:\n",
    "            logging.disable(logging.WARNING)\n",
    "\n",
    "        self.random_seed = random_seed\n",
    "        self.set_all_random_seeds()\n",
    "\n",
    "        self.shard_fpath = shard_fpath\n",
    "        self.output_directory = output_directory\n",
    "        self.is_character_bert = is_character_bert\n",
    "        self.duplication_factor = duplication_factor\n",
    "        self.max_input_length = max_input_length\n",
    "        self.short_input_probability = short_input_probability\n",
    "        self.max_masked_tokens_per_input = max_masked_tokens_per_input\n",
    "        self.masked_tokens_ratio = masked_tokens_ratio\n",
    "\n",
    "        self.pretraining_examples = []\n",
    "\n",
    "        # Load tokenizer / masked language modeling vocabulary:\n",
    "        if self.is_character_bert:\n",
    "            # Here, MLM vocabulary is a list of most frequent tokens in the corpus\n",
    "            logging.info('Using a \"word\" vocabulary for MLM.')\n",
    "            self.tokenizer = CharacterBertTokenizer()\n",
    "            corpus_name = os.path.basename(os.path.dirname(self.shard_fpath))\n",
    "            self.mlm_vocabulary = [\n",
    "                line.strip().split()[-1]\n",
    "                for line in open(\n",
    "                    os.path.join(\n",
    "                        WORKDIR, 'data', 'mlm_vocabularies',\n",
    "                        corpus_name, 'mlm_vocab.txt'),\n",
    "                    'r', encoding='utf-8')\n",
    "            ]\n",
    "        else:\n",
    "            # Here, MLM vocabulary is the same as the wordpiece vocabulary\n",
    "            logging.info('Using a WordPiece vocabulary for MLM.')\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(\n",
    "                os.path.join(WORKDIR, 'data', 'bert-base-uncased'))\n",
    "            self.mlm_vocabulary = list(self.tokenizer.vocab)\n",
    "        logging.info('Example tokens: %s', random.sample(self.mlm_vocabulary, 10))\n",
    "\n",
    "\n",
    "    def set_all_random_seeds(self):\n",
    "        r\"\"\"Sets all random seeds to `self.random_seed`\"\"\"\n",
    "        random.seed(self.random_seed)\n",
    "        np.random.seed(self.random_seed)\n",
    "        torch.manual_seed(self.random_seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(self.random_seed)\n",
    "        logging.info(\"Random seed set to: %d\", self.random_seed)\n",
    "\n",
    "\n",
    "    def generate_pretraining_examples(self):\n",
    "        r\"\"\"Generates pre-training examples (MLM/NSP) from the input shard.\"\"\"\n",
    "        logging.info('BEGIN: Generate Pre-training Examples')\n",
    "\n",
    "        # Gather token sequences by document\n",
    "        documents = self.read_documents_from_shard()\n",
    "\n",
    "        # We run through the corpus as many times as `self.duplication_factor`\n",
    "        iterator = range(self.duplication_factor)\n",
    "        if self.verbose:\n",
    "            iterator = tqdm(iterator, desc='Iterations over all documents')\n",
    "        for _ in iterator:\n",
    "            for doc_id in range(len(documents)):\n",
    "                new_pretraining_examples = \\\n",
    "                    self.generate_examples_from_document(\n",
    "                        document_id=doc_id,\n",
    "                        all_documents=documents\n",
    "                    )\n",
    "                self.pretraining_examples.extend(new_pretraining_examples)\n",
    "\n",
    "        # Shuffle the examples within the shard's pre-training data\n",
    "        logging.info(\"Shuffling examples...\")\n",
    "        random.shuffle(self.pretraining_examples)\n",
    "\n",
    "        logging.info('END: Generate Pre-training Examples')\n",
    "\n",
    "\n",
    "    def read_documents_from_shard(self):\n",
    "        r\"\"\"\n",
    "        Groups token sequences by document for the input shard then returns\n",
    "        a list of documents where each document is a set of token sequences.\n",
    "        \"\"\"\n",
    "        # Input file format:\n",
    "        # (1) One sentence per line. These should ideally be actual sentences, not\n",
    "        # entire paragraphs or arbitrary spans of text. (Because we use the\n",
    "        # sentence boundaries for the \"next sentence prediction\" task).\n",
    "        # (2) Blank lines between documents. Document boundaries are needed so\n",
    "        # that the \"next sentence prediction\" task doesn't span between documents.\n",
    "        counter = 0\n",
    "        documents = [[]]\n",
    "        logging.info(\"Grouping sentences by document from: %s\", self.shard_fpath)\n",
    "        with open(self.shard_fpath, \"r\", encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = convert_to_unicode(line)\n",
    "                line = line.strip()\n",
    "                if line == '':\n",
    "                    documents.append([])  # New document\n",
    "                else:\n",
    "                    # BERT: basic tokenization + wordpiece tokenization\n",
    "                    # CharacterBERT: basic tokenization only\n",
    "                    tokens = self.tokenizer.tokenize(line)\n",
    "                    if tokens:\n",
    "                        # Add tokens to last document\n",
    "                        documents[-1].append(tokens)\n",
    "                        if counter < 2:\n",
    "                            logging.info(\"Example of a sentence: %s\", tokens)\n",
    "                    counter += 1\n",
    "\n",
    "        logging.info(\"Removing empty documents (if any)...\")\n",
    "        # There shouldn't be any empty documents, but just to be extra safe\n",
    "        documents = [x for x in documents if x]\n",
    "\n",
    "        logging.info(\"Shuffling documents...\")\n",
    "        random.shuffle(documents)\n",
    "        return documents\n",
    "\n",
    "\n",
    "    def generate_examples_from_document(self, document_id, all_documents):\n",
    "        \"\"\"Returns a number of `PreTrainingExample` objects from a single document.\"\"\"\n",
    "        document = all_documents[document_id]\n",
    "\n",
    "        # Account for [CLS], [SEP], [SEP]\n",
    "        max_num_tokens = self.max_input_length - 3\n",
    "\n",
    "        # We *usually* want to fill up the entire input since we are padding\n",
    "        # to `max_input_length` anyways, so short input sequences are generally wasted\n",
    "        # computation. However, we *sometimes*\n",
    "        # (i.e., short_input_probability = 0.1 = 10% of the time) want to use shorter\n",
    "        # input sequences to minimize the mismatch between pre-training and fine-tuning.\n",
    "        # The `target_input_length` is just a rough target however, whereas `max_input_length`\n",
    "        # is a hard limit.\n",
    "        target_input_length = max_num_tokens\n",
    "        if random.random() < self.short_input_probability:\n",
    "            target_input_length = random.randint(2, max_num_tokens)\n",
    "\n",
    "        # We DON'T just concatenate all of the tokens from a document into a long\n",
    "        # sequence and choose an arbitrary split point because this would make the\n",
    "        # next sentence prediction task too easy. Instead, we split the input into\n",
    "        # segments \"A\" and \"B\" based on the actual \"sentences\" provided by the user input.\n",
    "        examples = []\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "        i = 0\n",
    "        while i < len(document):\n",
    "            sentence = document[i]\n",
    "            current_chunk.append(sentence)\n",
    "            current_length += len(sentence)\n",
    "\n",
    "            no_more_sentences_in_document = (i == (len(document) - 1))\n",
    "            exceeded_target_length = (current_length >= target_input_length)\n",
    "            if no_more_sentences_in_document or exceeded_target_length:\n",
    "                if current_chunk:\n",
    "\n",
    "                    # Building the segment `A`:\n",
    "\n",
    "                    tokens_a = []\n",
    "                    # `a_end` is how many sentences from `current_chunk` go into `A`.\n",
    "                    if len(current_chunk) > 1:\n",
    "                        a_end = random.randint(1, len(current_chunk) - 1)\n",
    "                    else:\n",
    "                        a_end = 1\n",
    "                    for j in range(a_end):\n",
    "                        tokens_a.extend(current_chunk[j])\n",
    "\n",
    "                    # Building the segment `B`:\n",
    "                    tokens_b = []\n",
    "                    is_random_next = False  # By default, `B` follows `A`\n",
    "\n",
    "                    # In 50% of the cases,\n",
    "                    # or if the chunk only has one sentence,\n",
    "                    # segment `B` is from another random document\n",
    "                    # (does not follow `A`, i.e. NSP target is False)\n",
    "                    if len(current_chunk) == 1 or random.random() < 0.5:\n",
    "                        is_random_next = True\n",
    "                        target_b_length = target_input_length - len(tokens_a)\n",
    "\n",
    "                        # Since we didn't actually use these sentences, we\n",
    "                        # \"put them back\" so they don't go to waste.\n",
    "                        num_unused_sentences = len(current_chunk) - a_end\n",
    "                        i -= num_unused_sentences\n",
    "\n",
    "                        # This should rarely go for more than one iteration for large\n",
    "                        # corpora. However, just to be safe, we try to make sure that\n",
    "                        # the random document is not the same as the document\n",
    "                        # we're processing.\n",
    "                        # We don't use a while loop to avoid looping indefinitely\n",
    "                        # if there is only one document (shouldn't happen)\n",
    "                        for _ in range(10):\n",
    "                            random_document_id = random.randint(0, len(all_documents) - 1)\n",
    "                            if random_document_id != document_id:\n",
    "                                break\n",
    "\n",
    "                        # Add tokens using sentences from a random document\n",
    "                        random_document = all_documents[random_document_id]\n",
    "                        random_start = random.randint(0, len(random_document) - 1)\n",
    "                        for j in range(random_start, len(random_document)):\n",
    "                            tokens_b.extend(random_document[j])\n",
    "                            if len(tokens_b) >= target_b_length:\n",
    "                                break\n",
    "\n",
    "                    # In all other cases, use the actual next sentences\n",
    "                    # (j >= a_end) to build segment `B`\n",
    "                    else:\n",
    "                        is_random_next = False\n",
    "                        for j in range(a_end, len(current_chunk)):\n",
    "                            tokens_b.extend(current_chunk[j])\n",
    "\n",
    "                    # Truncate sentence pair to the target length\n",
    "                    truncate_seq_pair(tokens_a, tokens_b, target_input_length)\n",
    "                    assert (len(tokens_a) >= 1) and (len(tokens_b) >= 1)\n",
    "\n",
    "                    # Building the actual input for the model:\n",
    "                    # 1 - Adding CLS/SEP to [A, B]\n",
    "                    # 2 - Building segment ids\n",
    "                    # 3 - Masking tokens\n",
    "                    tokens = []\n",
    "                    segment_ids = []\n",
    "\n",
    "                    tokens.append(\"[CLS]\")\n",
    "                    segment_ids.append(0)\n",
    "\n",
    "                    for token in tokens_a:\n",
    "                        tokens.append(token)\n",
    "                        segment_ids.append(0)\n",
    "\n",
    "                    tokens.append(\"[SEP]\")\n",
    "                    segment_ids.append(0)\n",
    "\n",
    "                    for token in tokens_b:\n",
    "                        tokens.append(token)\n",
    "                        segment_ids.append(1)\n",
    "\n",
    "                    tokens.append(\"[SEP]\")\n",
    "                    segment_ids.append(1)\n",
    "\n",
    "                    (transformed_tokens, mlm_positions, mlm_labels) = \\\n",
    "                        self.generate_mlm_instances_from_tokens(\n",
    "                            input_tokens=tokens)\n",
    "\n",
    "                    examples.append(\n",
    "                        PreTrainingExample(\n",
    "                            tokens=transformed_tokens,  # Original tokens with random tokens changed\n",
    "                            segment_ids=segment_ids,  # The input's segment ids\n",
    "                            is_random_next=is_random_next,  # For Next Sentence Prediction\n",
    "                            mlm_positions=mlm_positions,  # For Masked Language Modeling\n",
    "                            mlm_labels=mlm_labels  # For Masked Language Modeling\n",
    "                        )\n",
    "                    )\n",
    "                current_chunk = []\n",
    "                current_length = 0\n",
    "            i += 1\n",
    "\n",
    "        return examples\n",
    "\n",
    "\n",
    "    def generate_mlm_instances_from_tokens(self, input_tokens):\n",
    "        \"\"\"Generates instances for the Masked Language Modelling objective.\"\"\"\n",
    "        candidate_tokens = []\n",
    "        for (i, token) in enumerate(input_tokens):\n",
    "            if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "                continue\n",
    "            candidate_tokens.append((i, token))\n",
    "        random.shuffle(candidate_tokens)\n",
    "\n",
    "        # Make a copy of the original input.\n",
    "        # This will be transformed by randomly altering its tokens\n",
    "        # and will be the actual input for the model\n",
    "        output_tokens = input_tokens.copy()\n",
    "\n",
    "        num_to_predict = max(\n",
    "            1, int(round(len(input_tokens) * self.masked_tokens_ratio)))\n",
    "        num_to_predict = min(\n",
    "            num_to_predict, self.max_masked_tokens_per_input)\n",
    "\n",
    "        # Create MLM instances\n",
    "        mlm_instances = []\n",
    "        covered_indices = set()\n",
    "        for index, token in candidate_tokens:\n",
    "            if len(mlm_instances) >= num_to_predict:\n",
    "                break\n",
    "            if index in covered_indices:\n",
    "                continue\n",
    "            if self.is_character_bert:\n",
    "                # CharacterBERT: only mask tokens that are in the\n",
    "                # MLM vocabulary (i.e. most frequent tokens in the corpus)\n",
    "                if token not in self.mlm_vocabulary:\n",
    "                    continue\n",
    "            else:\n",
    "                # BERT: all tokens are in the MLM vocabulary anyway\n",
    "                # as MLM vocabulary == WordPiece vocabulary and all tokens\n",
    "                # are WordPieces\n",
    "                pass\n",
    "            covered_indices.add(index)\n",
    "\n",
    "            # Compute the token that will be placed at `index`\n",
    "            masked_token = None\n",
    "            # 80% of the time, replace with [MASK]\n",
    "            if random.random() < 0.8:\n",
    "                masked_token = \"[MASK]\"\n",
    "            else:\n",
    "                # 10% of the time, keep original\n",
    "                if random.random() < 0.5:\n",
    "                    masked_token = token\n",
    "                # 10% of the time, replace with random word\n",
    "                else:\n",
    "                    masked_token = random.choice(self.mlm_vocabulary)\n",
    "\n",
    "            # Replace token at `index`\n",
    "            output_tokens[index] = masked_token\n",
    "\n",
    "            # Add MLM instance\n",
    "            original_token = input_tokens[index]\n",
    "            mlm_instances.append((index, original_token))\n",
    "\n",
    "        #  Sort instances according to the masked tokens indices\n",
    "        mlm_instances = sorted(mlm_instances, key=lambda x: x[0])\n",
    "\n",
    "        mlm_positions, mlm_labels = [], []\n",
    "        for (index, original_token) in mlm_instances:\n",
    "            mlm_positions.append(index)\n",
    "            mlm_labels.append(original_token)\n",
    "\n",
    "        # Returns:\n",
    "        #     - output_tokens: transformed tokens\n",
    "        #     - mlm_positions: indices of masked tokens\n",
    "        #     - mlm_labels: original token for each masked token\n",
    "        return (output_tokens, mlm_positions, mlm_labels)\n",
    "\n",
    "\n",
    "    def write_examples_to_hdf5(self):\n",
    "        \"\"\"Converts examples to tensors and saves it in hdf5 format.\"\"\"\n",
    "\n",
    "        logging.info('BEGIN: Write Pre-Training Examples to hdf5 File')\n",
    "\n",
    "        total_written = 0\n",
    "        features = OrderedDict()\n",
    "        token_to_id = {w: i for i, w in enumerate(self.mlm_vocabulary)}\n",
    "\n",
    "        num_instances = len(self.pretraining_examples)\n",
    "\n",
    "        #### Initializing empty tensors ####\n",
    "\n",
    "        # NOTE: (!important) here we assume that padding index is 0\n",
    "        # for both BERT's and CharacterBERT's input ids, which is the default.\n",
    "        if self.is_character_bert:\n",
    "            features[\"input_ids\"] = np.zeros(\n",
    "                [\n",
    "                    num_instances,\n",
    "                    self.max_input_length,\n",
    "                    self.tokenizer._mapper.max_word_length\n",
    "                ],\n",
    "                dtype=\"int32\"\n",
    "            )\n",
    "        else:\n",
    "            features[\"input_ids\"] = np.zeros(\n",
    "                [\n",
    "                    num_instances,\n",
    "                    self.max_input_length,\n",
    "                ],\n",
    "                dtype=\"int32\")\n",
    "        features[\"input_mask\"] = np.zeros(\n",
    "            [num_instances, self.max_input_length], dtype=\"int32\")\n",
    "        features[\"segment_ids\"] = np.zeros(\n",
    "            [num_instances, self.max_input_length], dtype=\"int32\")\n",
    "        features[\"masked_lm_positions\"] = np.zeros(\n",
    "            [num_instances, self.max_masked_tokens_per_input], dtype=\"int32\")\n",
    "        features[\"masked_lm_ids\"] = np.zeros(\n",
    "            [num_instances, self.max_masked_tokens_per_input], dtype=\"int32\")\n",
    "        features[\"next_sentence_labels\"] = np.zeros(\n",
    "            num_instances, dtype=\"int32\")\n",
    "\n",
    "        #### Filling the tensors ####\n",
    "\n",
    "        iterator = self.pretraining_examples\n",
    "        if self.verbose:\n",
    "            iterator = tqdm(\n",
    "                iterator, total=num_instances,\n",
    "                desc='Converting pre-training examples to tensors'\n",
    "            )\n",
    "        for instance_id, instance in enumerate(iterator):\n",
    "            input_ids = self.tokenizer.convert_tokens_to_ids(instance.tokens)\n",
    "            segment_ids = list(instance.segment_ids)\n",
    "            input_mask = [1] * len(segment_ids)\n",
    "            assert len(input_ids) <= self.max_input_length\n",
    "            assert len(segment_ids) <= self.max_input_length\n",
    "\n",
    "            # Padding:\n",
    "            while len(segment_ids) < self.max_input_length:\n",
    "                if self.is_character_bert:\n",
    "                    input_ids.append([0] * self.tokenizer._mapper.max_word_length)\n",
    "                else:\n",
    "                    input_ids.append(0)\n",
    "                input_mask.append(0)\n",
    "                segment_ids.append(0)\n",
    "\n",
    "            assert len(input_ids) == self.max_input_length\n",
    "            assert len(input_mask) == self.max_input_length\n",
    "            assert len(segment_ids) == self.max_input_length\n",
    "\n",
    "\n",
    "            mlm_positions = list(instance.mlm_positions)\n",
    "            if self.is_character_bert:\n",
    "                masked_lm_ids = [\n",
    "                    token_to_id[label]\n",
    "                    for label in instance.mlm_labels]\n",
    "            else:\n",
    "                # Since the MLM output layer is a copy of the input Embedding matrix\n",
    "                masked_lm_ids = \\\n",
    "                    self.tokenizer.convert_tokens_to_ids(\n",
    "                        instance.mlm_labels)\n",
    "\n",
    "            # NOTE: I'm not very sure about this part...\n",
    "            # it seems that we fill up the mlm positions with the CLS position\n",
    "            while len(mlm_positions) < self.max_masked_tokens_per_input:\n",
    "                mlm_positions.append(0)\n",
    "                masked_lm_ids.append(0)\n",
    "\n",
    "            # NOTE: in the original code `is_random_next` is 1, we change it here\n",
    "            # so that NSP target is True when not `is_random_next`\n",
    "            next_sentence_label = 1 if (not instance.is_random_next) else 0\n",
    "\n",
    "            features[\"input_ids\"][instance_id] = input_ids\n",
    "            features[\"input_mask\"][instance_id] = input_mask\n",
    "            features[\"segment_ids\"][instance_id] = segment_ids\n",
    "            features[\"masked_lm_positions\"][instance_id] = mlm_positions\n",
    "            features[\"masked_lm_ids\"][instance_id] = masked_lm_ids\n",
    "            features[\"next_sentence_labels\"][instance_id] = next_sentence_label\n",
    "\n",
    "            total_written += 1\n",
    "\n",
    "        # Saving pretraining data as an .hdf5 file\n",
    "        os.makedirs(self.output_directory, exist_ok=True)\n",
    "        basename = os.path.basename(self.shard_fpath)\n",
    "        output_hdf5_fpath = os.path.join(\n",
    "            self.output_directory, basename.replace('.txt', '.hdf5'))\n",
    "\n",
    "        logging.info(\"Saving data...\")\n",
    "        f = h5py.File(output_hdf5_fpath, 'w')\n",
    "        f.create_dataset(\n",
    "            \"input_ids\",\n",
    "            data=features[\"input_ids\"],\n",
    "            dtype='i4', compression='gzip'\n",
    "        )\n",
    "        f.create_dataset(\n",
    "            \"input_mask\",\n",
    "            data=features[\"input_mask\"],\n",
    "            dtype='i1', compression='gzip'\n",
    "        )\n",
    "        f.create_dataset(\n",
    "            \"segment_ids\",\n",
    "            data=features[\"segment_ids\"],\n",
    "            dtype='i1', compression='gzip'\n",
    "        )\n",
    "        f.create_dataset(\n",
    "            \"masked_lm_positions\",\n",
    "            data=features[\"masked_lm_positions\"],\n",
    "            dtype='i4', compression='gzip'\n",
    "        )\n",
    "        f.create_dataset(\n",
    "            \"masked_lm_ids\",\n",
    "            data=features[\"masked_lm_ids\"],\n",
    "            dtype='i4', compression='gzip'\n",
    "        )\n",
    "        f.create_dataset(\n",
    "            \"next_sentence_labels\",\n",
    "            data=features[\"next_sentence_labels\"],\n",
    "            dtype='i1', compression='gzip'\n",
    "        )\n",
    "        f.flush()\n",
    "        f.close()\n",
    "\n",
    "        logging.info('END: Write Pre-Training Examples to hdf5 File')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb28826",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = \"data/hdf5/80_12/\"\n",
    "# max_input_length = 80\n",
    "# max_input_length = 20\n",
    "max_input_length = 120\n",
    "# max_masked_tokens_per_input = 12\n",
    "# max_masked_tokens_per_input = 3\n",
    "max_masked_tokens_per_input = 18\n",
    "is_character_bert = True\n",
    "short_input_probability = 0.10\n",
    "masked_tokens_ratio = 0.15\n",
    "random_seed = 42\n",
    "duplication_factor = 1\n",
    "shard_fpath = 'data\\\\shrads\\\\long_text.formatted.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2030ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True\n",
    "if not verbose:\n",
    "    logging.disable(logging.WARNING)\n",
    "\n",
    "random_seed = random_seed\n",
    "\n",
    "# set_all_random_seeds()\n",
    "r\"\"\"Sets all random seeds to `random_seed`\"\"\"\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "logging.info(\"Random seed set to: %d\", random_seed)\n",
    "\n",
    "pretraining_examples = []\n",
    "\n",
    "# Load tokenizer / masked language modeling vocabulary:\n",
    "if is_character_bert:\n",
    "    # Here, MLM vocabulary is a list of most frequent tokens in the corpus\n",
    "    logging.info('Using a \"word\" vocabulary for MLM.')\n",
    "    tokenizer = CharacterBertTokenizer(strip_accents=None, do_lower_case=None)\n",
    "    corpus_name = os.path.basename(os.path.dirname(shard_fpath))\n",
    "    mlm_vocabulary = [\n",
    "        line.strip().split()[-1]\n",
    "        for line in open(\n",
    "            os.path.join(\n",
    "                WORKDIR, 'data', 'mlm_vocabularies',\n",
    "                corpus_name, 'mlm_vocab.txt'),\n",
    "            'r', encoding='utf-8')\n",
    "    ]\n",
    "else:\n",
    "    # Here, MLM vocabulary is the same as the wordpiece vocabulary\n",
    "    logging.info('Using a WordPiece vocabulary for MLM.')\n",
    "    tokenizer = BertTokenizer.from_pretrained(\n",
    "        os.path.join(WORKDIR, 'data', 'bert-base-uncased'))\n",
    "    mlm_vocabulary = list(tokenizer.vocab)\n",
    "logging.info('Example tokens: %s', random.sample(mlm_vocabulary, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcab3d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input file format:\n",
    "# (1) One sentence per line. These should ideally be actual sentences, not\n",
    "# entire paragraphs or arbitrary spans of text. (Because we use the\n",
    "# sentence boundaries for the \"next sentence prediction\" task).\n",
    "# (2) Blank lines between documents. Document boundaries are needed so\n",
    "# that the \"next sentence prediction\" task doesn't span between documents.\n",
    "counter = 0\n",
    "documents = [[]]\n",
    "logging.info(\"Grouping sentences by document from: %s\", shard_fpath)\n",
    "with open(shard_fpath, \"r\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = convert_to_unicode(line)\n",
    "        line = line.strip()\n",
    "        if line == '':\n",
    "            documents.append([])  # New document\n",
    "        else:\n",
    "            # BERT: basic tokenization + wordpiece tokenization\n",
    "            # CharacterBERT: basic tokenization only\n",
    "            tokens = tokenizer.tokenize(line)\n",
    "            if tokens:\n",
    "                # Add tokens to last document\n",
    "                documents[-1].append(tokens)\n",
    "                if counter < 2:\n",
    "                    logging.info(\"Example of a sentence: %s\", tokens)\n",
    "            counter += 1\n",
    "\n",
    "logging.info(\"Removing empty documents (if any)...\")\n",
    "# There shouldn't be any empty documents, but just to be extra safe\n",
    "documents = [x for x in documents if x]\n",
    "\n",
    "logging.info(\"Shuffling documents...\")\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133a300f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf0215c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(documents[0], test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cda1cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0] = train\n",
    "# documents[0] = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e73dec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ce0b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d51838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_len = int(0.8 * len(documents[0]))\n",
    "# documents[0] = documents[0][:train_len]\n",
    "# documents[0] = documents[0][train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57db067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_examples_from_document(document_id, all_documents):\n",
    "    \"\"\"Returns a number of `PreTrainingExample` objects from a single document.\"\"\"\n",
    "    document = all_documents[document_id]\n",
    "\n",
    "    # Account for [CLS], [SEP], [SEP]\n",
    "    max_num_tokens = max_input_length - 3\n",
    "\n",
    "    # We *usually* want to fill up the entire input since we are padding\n",
    "    # to `max_input_length` anyways, so short input sequences are generally wasted\n",
    "    # computation. However, we *sometimes*\n",
    "    # (i.e., short_input_probability = 0.1 = 10% of the time) want to use shorter\n",
    "    # input sequences to minimize the mismatch between pre-training and fine-tuning.\n",
    "    # The `target_input_length` is just a rough target however, whereas `max_input_length`\n",
    "    # is a hard limit.\n",
    "    target_input_length = max_num_tokens\n",
    "\n",
    "    # We DON'T just concatenate all of the tokens from a document into a long\n",
    "    # sequence and choose an arbitrary split point because this would make the\n",
    "    # next sentence prediction task too easy. Instead, we split the input into\n",
    "    # segments \"A\" and \"B\" based on the actual \"sentences\" provided by the user input.\n",
    "    examples = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    #train_len = int(0.8 * len(document))\n",
    "    i = 0\n",
    "#     i = train_len\n",
    "#     random.shuffle(document)\n",
    "    \n",
    "    while i  < len(document):\n",
    "#     while i < train_len:\n",
    "        sentence = document[i]\n",
    "        current_chunk.append(sentence)\n",
    "        current_length = len(sentence)\n",
    "\n",
    "        exceeded_target_length = (current_length >= target_input_length)\n",
    "        if not exceeded_target_length:\n",
    "            if current_chunk:\n",
    "                # Building the segment `A`:\n",
    "\n",
    "                tokens_a = []\n",
    "                # `a_end` is how many sentences from `current_chunk` go into `A`.\n",
    "                if len(current_chunk) > 1:\n",
    "                    a_end = random.randint(1, len(current_chunk) - 1)\n",
    "                else:\n",
    "                    a_end = 1\n",
    "                for j in range(a_end):\n",
    "                    tokens_a.extend(current_chunk[j])\n",
    "\n",
    "                # Truncate sentence pair to the target length\n",
    "                tokens_b = []\n",
    "                truncate_seq_pair(tokens_a, tokens_b, target_input_length)\n",
    "                assert (len(tokens_a) >= 1)\n",
    "\n",
    "                # Building the actual input for the model:\n",
    "                # 1 - Adding CLS/SEP to [A, B]\n",
    "                # 2 - Building segment ids\n",
    "                # 3 - Masking tokens\n",
    "                tokens = []\n",
    "                segment_ids = []\n",
    "\n",
    "                tokens.append(\"[CLS]\")\n",
    "                segment_ids.append(0)\n",
    "\n",
    "                for token in tokens_a:\n",
    "                    tokens.append(token)\n",
    "                    segment_ids.append(0)\n",
    "\n",
    "                tokens.append(\"[SEP]\")\n",
    "                segment_ids.append(0)\n",
    "\n",
    "                (transformed_tokens, mlm_positions, mlm_labels) = \\\n",
    "                    generate_mlm_instances_from_tokens(\n",
    "                        input_tokens=tokens)\n",
    "\n",
    "                examples.append(\n",
    "                    PreTrainingExample(\n",
    "                        tokens=transformed_tokens,  # Original tokens with random tokens changed\n",
    "                        segment_ids=segment_ids,  # The input's segment ids\n",
    "                        is_random_next=False,  # For Next Sentence Prediction\n",
    "                        mlm_positions=mlm_positions,  # For Masked Language Modeling\n",
    "                        mlm_labels=mlm_labels  # For Masked Language Modeling\n",
    "                    )\n",
    "                )\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "        i += 1\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14a3d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mlm_instances_from_tokens(input_tokens):\n",
    "    \"\"\"Generates instances for the Masked Language Modelling objective.\"\"\"\n",
    "    candidate_tokens = []\n",
    "    for (i, token) in enumerate(input_tokens):\n",
    "        if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "            continue\n",
    "        candidate_tokens.append((i, token))\n",
    "    random.shuffle(candidate_tokens)\n",
    "\n",
    "    # Make a copy of the original input.\n",
    "    # This will be transformed by randomly altering its tokens\n",
    "    # and will be the actual input for the model\n",
    "    output_tokens = input_tokens.copy()\n",
    "\n",
    "    num_to_predict = max(\n",
    "        1, int(round(len(input_tokens) * masked_tokens_ratio)))\n",
    "    num_to_predict = min(\n",
    "        num_to_predict, max_masked_tokens_per_input)\n",
    "\n",
    "    # Create MLM instances\n",
    "    mlm_instances = []\n",
    "    covered_indices = set()\n",
    "    for index, token in candidate_tokens:\n",
    "        if len(mlm_instances) >= num_to_predict:\n",
    "            break\n",
    "        if index in covered_indices:\n",
    "            continue\n",
    "        if is_character_bert:\n",
    "            # CharacterBERT: only mask tokens that are in the\n",
    "            # MLM vocabulary (i.e. most frequent tokens in the corpus)\n",
    "            if token not in mlm_vocabulary:\n",
    "                continue\n",
    "        else:\n",
    "            # BERT: all tokens are in the MLM vocabulary anyway\n",
    "            # as MLM vocabulary == WordPiece vocabulary and all tokens\n",
    "            # are WordPieces\n",
    "            pass\n",
    "        covered_indices.add(index)\n",
    "\n",
    "        # Compute the token that will be placed at `index`\n",
    "        masked_token = None\n",
    "        # 80% of the time, replace with [MASK]\n",
    "        if random.random() < 0.8:\n",
    "            masked_token = \"[MASK]\"\n",
    "        else:\n",
    "            # 10% of the time, keep original\n",
    "            if random.random() < 0.5:\n",
    "                masked_token = token\n",
    "            # 10% of the time, replace with random word\n",
    "            else:\n",
    "                masked_token = random.choice(mlm_vocabulary)\n",
    "\n",
    "        # Replace token at `index`\n",
    "        output_tokens[index] = masked_token\n",
    "\n",
    "        # Add MLM instance\n",
    "        original_token = input_tokens[index]\n",
    "        mlm_instances.append((index, original_token))\n",
    "\n",
    "    #  Sort instances according to the masked tokens indices\n",
    "    mlm_instances = sorted(mlm_instances, key=lambda x: x[0])\n",
    "\n",
    "    mlm_positions, mlm_labels = [], []\n",
    "    for (index, original_token) in mlm_instances:\n",
    "        mlm_positions.append(index)\n",
    "        mlm_labels.append(original_token)\n",
    "\n",
    "    # Returns:\n",
    "    #     - output_tokens: transformed tokens\n",
    "    #     - mlm_positions: indices of masked tokens\n",
    "    #     - mlm_labels: original token for each masked token\n",
    "    return (output_tokens, mlm_positions, mlm_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50657294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = [[0]]\n",
    "# temp[0] = documents[0][:5]\n",
    "# temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6156e949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_examples_from_document(\n",
    "#                 document_id=0,\n",
    "#                 all_documents=temp\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718194de",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretraining_examples = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93749d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We run through the corpus as many times as `duplication_factor`\n",
    "iterator = range(duplication_factor)\n",
    "if verbose:\n",
    "    iterator = tqdm(iterator, desc='Iterations over all documents')\n",
    "for _ in iterator:\n",
    "    for doc_id in range(len(documents)):\n",
    "        new_pretraining_examples = \\\n",
    "            generate_examples_from_document(\n",
    "                document_id=doc_id,\n",
    "                all_documents=documents\n",
    "            )\n",
    "        pretraining_examples.extend(new_pretraining_examples)\n",
    "        \n",
    "logging.info('END: Generate Pre-training Examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955a1642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_examples_to_hdf5():\n",
    "    \"\"\"Converts examples to tensors and saves it in hdf5 format.\"\"\"\n",
    "\n",
    "    logging.info('BEGIN: Write Pre-Training Examples to hdf5 File')\n",
    "\n",
    "    total_written = 0\n",
    "    features = OrderedDict()\n",
    "    token_to_id = {w: i for i, w in enumerate(mlm_vocabulary)}\n",
    "\n",
    "    num_instances = len(pretraining_examples)\n",
    "\n",
    "    #### Initializing empty tensors ####\n",
    "\n",
    "    # NOTE: (!important) here we assume that padding index is 0\n",
    "    # for both BERT's and CharacterBERT's input ids, which is the default.\n",
    "    if is_character_bert:\n",
    "        features[\"input_ids\"] = np.zeros(\n",
    "            [\n",
    "                num_instances,\n",
    "                max_input_length,\n",
    "                tokenizer._mapper.max_word_length\n",
    "            ],\n",
    "            dtype=\"int32\"\n",
    "        )\n",
    "    else:\n",
    "        features[\"input_ids\"] = np.zeros(\n",
    "            [\n",
    "                num_instances,\n",
    "                max_input_length,\n",
    "            ],\n",
    "            dtype=\"int32\")\n",
    "    features[\"input_mask\"] = np.zeros(\n",
    "        [num_instances, max_input_length], dtype=\"int32\")\n",
    "    features[\"segment_ids\"] = np.zeros(\n",
    "        [num_instances, max_input_length], dtype=\"int32\")\n",
    "    features[\"masked_lm_positions\"] = np.zeros(\n",
    "        [num_instances, max_masked_tokens_per_input], dtype=\"int32\")\n",
    "    features[\"masked_lm_ids\"] = np.zeros(\n",
    "        [num_instances, max_masked_tokens_per_input], dtype=\"int32\")\n",
    "    features[\"next_sentence_labels\"] = np.zeros(\n",
    "        num_instances, dtype=\"int32\")\n",
    "\n",
    "    #### Filling the tensors ####\n",
    "\n",
    "    iterator = pretraining_examples\n",
    "    if verbose:\n",
    "        iterator = tqdm(\n",
    "            iterator, total=num_instances,\n",
    "            desc='Converting pre-training examples to tensors'\n",
    "        )\n",
    "    for instance_id, instance in enumerate(iterator):\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(instance.tokens)\n",
    "        segment_ids = list(instance.segment_ids)\n",
    "        input_mask = [1] * len(segment_ids)\n",
    "        assert len(input_ids) <= max_input_length\n",
    "        assert len(segment_ids) <= max_input_length\n",
    "\n",
    "        # Padding:\n",
    "        while len(segment_ids) < max_input_length:\n",
    "            if is_character_bert:\n",
    "                input_ids.append([0] * tokenizer._mapper.max_word_length)\n",
    "            else:\n",
    "                input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "            segment_ids.append(0)\n",
    "\n",
    "        assert len(input_ids) == max_input_length\n",
    "        assert len(input_mask) == max_input_length\n",
    "        assert len(segment_ids) == max_input_length\n",
    "\n",
    "\n",
    "        mlm_positions = list(instance.mlm_positions)\n",
    "        if is_character_bert:\n",
    "            masked_lm_ids = [\n",
    "                token_to_id[label]\n",
    "                for label in instance.mlm_labels]\n",
    "        else:\n",
    "            # Since the MLM output layer is a copy of the input Embedding matrix\n",
    "            masked_lm_ids = \\\n",
    "                tokenizer.convert_tokens_to_ids(\n",
    "                    instance.mlm_labels)\n",
    "\n",
    "        # NOTE: I'm not very sure about this part...\n",
    "        # it seems that we fill up the mlm positions with the CLS position\n",
    "        while len(mlm_positions) < max_masked_tokens_per_input:\n",
    "            mlm_positions.append(0)\n",
    "            masked_lm_ids.append(0)\n",
    "\n",
    "        # NOTE: in the original code `is_random_next` is 1, we change it here\n",
    "        # so that NSP target is True when not `is_random_next`\n",
    "        next_sentence_label = 1 if (not instance.is_random_next) else 0\n",
    "\n",
    "        features[\"input_ids\"][instance_id] = input_ids\n",
    "        features[\"input_mask\"][instance_id] = input_mask\n",
    "        features[\"segment_ids\"][instance_id] = segment_ids\n",
    "        features[\"masked_lm_positions\"][instance_id] = mlm_positions\n",
    "        features[\"masked_lm_ids\"][instance_id] = masked_lm_ids\n",
    "#         features[\"next_sentence_labels\"][instance_id] = next_sentence_label\n",
    "\n",
    "        total_written += 1\n",
    "\n",
    "    # Saving pretraining data as an .hdf5 file\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    basename = os.path.basename(shard_fpath)\n",
    "    output_hdf5_fpath = os.path.join(\n",
    "        output_directory, basename.replace('.txt', '.hdf5'))\n",
    "\n",
    "    logging.info(\"Saving data...\")\n",
    "    f = h5py.File(output_hdf5_fpath, 'w')\n",
    "    f.create_dataset(\n",
    "        \"input_ids\",\n",
    "        data=features[\"input_ids\"],\n",
    "        dtype='i4', compression='gzip'\n",
    "    )\n",
    "    f.create_dataset(\n",
    "        \"input_mask\",\n",
    "        data=features[\"input_mask\"],\n",
    "        dtype='i1', compression='gzip'\n",
    "    )\n",
    "    f.create_dataset(\n",
    "        \"segment_ids\",\n",
    "        data=features[\"segment_ids\"],\n",
    "        dtype='i1', compression='gzip'\n",
    "    )\n",
    "    f.create_dataset(\n",
    "        \"masked_lm_positions\",\n",
    "        data=features[\"masked_lm_positions\"],\n",
    "        dtype='i4', compression='gzip'\n",
    "    )\n",
    "    f.create_dataset(\n",
    "        \"masked_lm_ids\",\n",
    "        data=features[\"masked_lm_ids\"],\n",
    "        dtype='i4', compression='gzip'\n",
    "    )\n",
    "    f.create_dataset(\n",
    "        \"next_sentence_labels\",\n",
    "        data=features[\"next_sentence_labels\"],\n",
    "        dtype='i1', compression='gzip'\n",
    "    )\n",
    "    f.flush()\n",
    "    f.close()\n",
    "\n",
    "    logging.info('END: Write Pre-Training Examples to hdf5 File')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b37629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the examples within the shard's pre-training data\n",
    "logging.info(\"Shuffling examples...\")\n",
    "random.shuffle(pretraining_examples)\n",
    "\n",
    "write_examples_to_hdf5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd504c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pretraining_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0bd700",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pretraining_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67249e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in pretraining_examples:\n",
    "    if \"\" in each.tokens:\n",
    "        print(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b6fd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '    '\n",
    "encoded = tokenizer(text)\n",
    "# encoded[\"input_ids\"].shape\n",
    "\n",
    "tokenizer.decode(encoded[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83de58b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "character_bert",
   "language": "python",
   "name": "character_bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
