# Improving Bengali and Hindi Large Language Models
 
Developing language models for popular yet low-resource languages like Bengali and Hindi remains understudied. State-of-the-art monolingual and multilingual BERT trained wordpiece tokenization system for modeling Bengali and Hindi. However, we observed that the wordpiece tokenizer splits words into less meaningful tokens and fails to separate root, suffix, prefix, and affix. We find that modeling fine-grained character-level information and interactions between roots and suffixes, prefixes, and affixes is essential for such highly inflected and morphologically complex languages. Therefore, we modified the BERT architecture with two different tokenizers - Bengali and Hindi unigram tokenizer and character-level tokenizer, which showed better performance empirically for these languages. We pretrained two language models based on these tokenizers. Finally, we evaluated the pretrained models for masked token detection in correct and erroneous settings and fundamental NLU tasks. We provide experimental evidence that unigram and character-level tokenizers lead to better-pretrained models for Bengali and Hindi and outperform BERT with wordpiece vocabulary. We conduct the first study investigating the efficacy of different tokenization methods for modeling Sanskrit-originated languages and make our code and models available for further research.
