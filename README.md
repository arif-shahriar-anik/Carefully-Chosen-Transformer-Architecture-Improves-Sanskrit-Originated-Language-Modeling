# Improving Bengali and Hindi Large Language Models
 
Bengali and Hind are two widely spoken yet low-resource languages. The state-of-the-art in modeling such languages uses BERT and the Wordpiece tokenizer. We observed that the Wordpiece tokenizer often breaks words into meaningless tokens, failing to separate roots from affixes. Moreover, Wordpiece does not take into account fine-grained character-level information. We hypothesize that modeling fine-grained character-level information or interactions between roots and affixes is essential for modeling highly inflected and morphologically complex Bengali and Hindi. Therefore, we modified the BERT architecture with two different tokenizers - Bengali and Hindi Unigram tokenizer and character-level tokenizer and observed better performance empirically. Then, we pretrained two language models based on these tokenizers. Finally, we evaluated our pretrained models for masked token detection in correct and erroneous settings and fundamental NLU tasks. We provide experimental evidence that Unigram and character-level tokenizers lead to better-pretrained models for Bengali and Hindi and outperform BERT with Wordpiece vocabulary. We conduct the first study investigating the efficacy of different tokenization methods for modeling Bengali and Hindi. We also make our code and models available for further research.
